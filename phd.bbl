% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.0 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nyt/global//global/global}
  \entry{anstattTypenSemantischerRelationen2009}{incollection}{}
    \name{author}{1}{}{%
      {{hash=AT}{%
         family={Anstatt},
         familyi={A\bibinitperiod},
         given={Tanja},
         giveni={T\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=BT}{%
         family={Berger},
         familyi={B\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=GK}{%
         family={Gutschmidt},
         familyi={G\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={Kempgen},
         familyi={K\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=KP}{%
         family={Kosta},
         familyi={K\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{AT1}
    \strng{fullhash}{AT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2009}
    \field{labeldatesource}{year}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{booktitle}{Die Slavischen {{Sprachen}}. {{Ein}} Internationales
  {{Handbuch}} Zu Ihrer {{Geschichte}}, Ihrer {{Struktur}} Und Ihrer
  {{Erforschung}}}
    \field{pages}{906\bibrangedash 915}
    \field{title}{Typen Semantischer {{Relationen}}}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/UJJVJSM9/Tanja_Anstatt_2009_Typen_se
    \verb mantischer_Relationen.html
    \endverb
    \field{annotation}{%
    913: Suffixe zur Bildung von Hyponymen in slawischen Sprachen: "[...] zur
  Bildung von Hyponymen kann z.B. das Suffix -ovye/-evye (seld' ,Hering`
  \textendash{} sel'devye ,Heringsartige`) verwendet werden (s. Ginzburg 1985,
  9)."%
    }
    \field{year}{2009}
  \endentry

  \entry{ayseExtractionSemanticWord2011}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=AS}{%
         family={Ay{\c s}e},
         familyi={A\bibinitperiod},
         given={{\c S}erbet{\c c}i},
         giveni={{\c S}\bibinitperiod},
      }}%
      {{hash=ZO}{%
         family={Zeynep},
         familyi={Z\bibinitperiod},
         given={Orhan},
         giveni={O\bibinitperiod},
      }}%
      {{hash=IP}{%
         family={{\.I}lknur},
         familyi={{\\bibinitperiodI}\bibinitperiod},
         given={Pehlivan},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Association for Computational Linguistics}}%
    }
    \strng{namehash}{ASZOIP1}
    \strng{fullhash}{ASZOIP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2011}
    \field{labeldatesource}{year}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{booktitle}{Proceedings of the {{ACL}} 2011 {{Workshop}} on
  {{Relational Models}} of {{Semantics}}}
    \field{pages}{11\bibrangedash 18}
    \field{title}{Extraction of {{Semantic Word Relations}} in {{Turkish}} from
  {{Dictionary Definitions}}}
    \list{location}{1}{%
      {{Portland, Oregon, USA}}%
    }
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/HKR3KEGH/Ay≈üe et al. - 2011 - Extra
    \verb ction of Semantic Word Relations in Turkish f.pdf
    \endverb
    \field{annotation}{%
    12: semantische Relationen m{\"u}ssen immer je nach Wortbedeutung
  differenziert werden: "For more accurate semantic analysis, the connection
  between words should be established between appropriate senses of the words.
  To be more concrete, an example can be given on the semantically ambiguous
  word as; y{\"u}z `face' or `hundred'. When a has-a relation is detected
  between the words v{\"u}cut `body' and y{\"u}z, the appropriate sense for
  y{\"u}z should be selected as `face', instead of `hundred'."%
    }
    \field{month}{06}
    \field{year}{2011}
  \endentry

  \entry{bartunovBreakingSticksAmbiguities2016}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=BS}{%
         family={Bartunov},
         familyi={B\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
      {{hash=KD}{%
         family={Kondrashkin},
         familyi={K\bibinitperiod},
         given={Dmitry},
         giveni={D\bibinitperiod},
      }}%
      {{hash=OA}{%
         family={Osokin},
         familyi={O\bibinitperiod},
         given={Anton},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VD}{%
         family={Vetrov},
         familyi={V\bibinitperiod},
         given={Dmitry},
         giveni={D\bibinitperiod},
      }}%
    }
    \strng{namehash}{BS+1}
    \strng{fullhash}{BSKDOAVD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{labeldatesource}{year}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{booktitle}{Artificial {{Intelligence}} and {{Statistics}}}
    \field{pages}{130\bibrangedash 138}
    \field{title}{Breaking Sticks and Ambiguities with Adaptive Skip-Gram}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/K7AHQAHW/Bartunov et al. - 2016 - Br
    \verb eaking sticks and ambiguities with adaptive skip.pdf
    \endverb
    \field{annotation}{%
    131: AdaGram erlaubt mehrere Wortbedeutungen f{\"u}r jedes Wort: "It
  retains all noticeable properties of [Skip-gram] such as fast online learning
  and high quality of representations while allowing to automatically learn the
  necessary number of prototypes per word at desired semantic resolution." 137:
  Testset f{\"u}r Word Sense Induction aus den Disambiguierungsseiten von
  Wikipedia gewonnen: "Since the problem of learning multi-prototype word
  representation is closely related to word-sense induction, we evaluated
  AdaGram on several WSI datasets and contributed a new large one obtained
  automatically from Wikipedia disambiguation pages."%
    }
    \field{year}{2016}
  \endentry

  \entry{boledaIntensionalityWasOnly2013}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=BG}{%
         family={Boleda},
         familyi={B\bibinitperiod},
         given={Gemma},
         giveni={G\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Baroni},
         familyi={B\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
      {{hash=ML}{%
         family={McNally},
         familyi={M\bibinitperiod},
         given={Louise},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{ACL (Association for Computational Linguistics)}}%
    }
    \strng{namehash}{BGBMML1}
    \strng{fullhash}{BGBMML1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2013}
    \field{labeldatesource}{year}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{booktitle}{Proceedings of the 10th {{International Conference}} on
  {{Computational Semantics}} ({{IWCS}} 2013): Long Papers; 2013 {{Mar}} 20-22;
  {{Postdam}}, {{Germany}}. {{Stroudsburg}} ({{USA}}): {{Association}} for
  {{Computational Linguistics}} ({{ACL}})}
    \field{pages}{35\bibrangedash 46}
    \field{shorttitle}{Intensionality Was Only Alleged}
    \field{title}{Intensionality Was Only Alleged: {{On}} Adjective-Noun
  Composition in Distributional Semantics}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/TZL9TG8J/Boleda et al. - 2013 - Inte
    \verb nsionality was only alleged On adjective-noun.pdf;/Users/Konstantin/Z
    \verb otero/storage/NX339DFP/37126.html
    \endverb
    \field{annotation}{%
    42: Modifizierende Adjektive k{\"o}nnen die Polysemie von Substantiven
  deutlich eingrenzen und dadurch zur Disambiguierung der Wortbedeutung
  wesentlich beitragen: "We find that, the more polysemous a noun is, the less
  similar its vector is to the corresponding phrase vector. It is plausible
  that modifying a noun has a larger impact when the noun is polysemous, as the
  adjective narrows down the meaning of the noun; indeed, adjectives have been
  independently shown to be powerful word sense disambiguators of nouns [...].
  In distributional terms, the adjective notably ``shifts'' the vector of
  polysemous nouns, but for monosemous nouns there is just not much shifting
  room."%
    }
    \field{year}{2013}
  \endentry

  \entry{carlsonArchitectureNeverendingLanguage2010}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=CA}{%
         family={Carlson},
         familyi={C\bibinitperiod},
         given={Andrew},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BJ}{%
         family={Betteridge},
         familyi={B\bibinitperiod},
         given={Justin},
         giveni={J\bibinitperiod},
      }}%
      {{hash=KB}{%
         family={Kisiel},
         familyi={K\bibinitperiod},
         given={Bryan},
         giveni={B\bibinitperiod},
      }}%
      {{hash=SB}{%
         family={Settles},
         familyi={S\bibinitperiod},
         given={Burr},
         giveni={B\bibinitperiod},
      }}%
      {{hash=HER}{%
         family={Hruschka},
         familyi={H\bibinitperiod},
         given={Estevam\bibnamedelima R.},
         giveni={E\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=MTM}{%
         family={Mitchell},
         familyi={M\bibinitperiod},
         given={Tom\bibnamedelima M.},
         giveni={T\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \strng{namehash}{CA+1}
    \strng{fullhash}{CABJKBSBHERMTM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2010}
    \field{labeldatesource}{year}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{booktitle}{Twenty-{{Fourth AAAI Conference}} on {{Artificial
  Intelligence}}}
    \field{pages}{1306\bibrangedash 1313}
    \field{title}{Toward an Architecture for Never-Ending Language Learning}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/CMRWR4KL/Carlson et al. - 2010 - Tow
    \verb ard an architecture for never-ending language l.pdf;/Users/Konstantin
    \verb /Zotero/storage/PXP2U9WG/1879.html
    \endverb
    \field{annotation}{%
    1306: semantische Relationen sind auch im Bereich der k{\"u}nstlichen
  Intelligenz f{\"u}r den Spracherwerb unerl{\"a}sslich: "At present,
  [Never-Ending Language Learner] acquires two types of knowledge: (1)
  knowledge about which noun phrases refer to which specified semantic
  categories, such as cities, companies, and sports teams, and (2) knowledge
  about which pairs of noun phrases satisfy which specified semantic relations,
  such as hasOfficesIn(organization, location)."%
    }
    \field{year}{2010}
  \endentry

  \entry{coenenAnalogieUndMetapher2013}{book}{}
    \name{author}{1}{}{%
      {{hash=CHG}{%
         family={Coenen},
         familyi={C\bibinitperiod},
         given={Hans\bibnamedelima Georg},
         giveni={H\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {de}%
    }
    \list{publisher}{1}{%
      {{Walter de Gruyter}}%
    }
    \keyw{Literary Criticism / General,Language Arts \& Disciplines /
  Linguistics / Historical \& Comparative,Literary Criticism / European /
  German,Literary Criticism / Semiotics \& Theory}
    \strng{namehash}{CHG1}
    \strng{fullhash}{CHG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2013}
    \field{labeldatesource}{year}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Von zahllosen anderen Ver{\"o}ffentlichungen zur bildlichen Rede
  unterscheidet sich das Studienbuch "Analogie und Metapher" in zweifacher
  Hinsicht: 1. Es verbindet Anregungen der antiken Rhetorik mit
  Errungenschaften der modernen Sprachwissenschaft und -philosophie, so da{\ss}
  die Entw{\"u}rfe der Klassiker in ein tragf{\"a}higeres theoretisches
  Ger{\"u}st eingef{\"u}gt werden k{\"o}nnen. 2. Es f{\"u}hrt die verschiedenen
  Erscheinungsformen der bildlichen Rede auf eine gemeinsame Tiefenstruktur
  zur{\"u}ck: die Analogie, verstanden als Geltung eines gemeinsamen
  Beschreibungsinhalts f{\"u}r zwei verschiedene Gegenst{\"a}nde. Die
  Differenzierung des Analogiebegriffs - z.B. in ein- und mehrstellige sowie
  triviale und nicht-triviale Analogien - erweist sich als Schl{\"u}ssel sowohl
  zum linguistischen Verst{\"a}ndnis des Ph{\"a}nomens der Bildlichkeit wie
  auch zur Interpretation einzelner sprachlicher Bilder etwa in der Literatur.
  Insofern ist dieses Studienbuch sowohl f{\"u}r Linguisten als auch f{\"u}r
  Literaturwissenschaftler von Interesse.%
    }
    \field{isbn}{978-3-11-089463-9}
    \field{shorttitle}{{Analogie und Metapher}}
    \field{title}{{Analogie und Metapher: Grundlegung einer Theorie der
  bildlichen Rede}}
    \field{annotation}{%
    46: Unterscheidung von theoretischer und empirischer Bedeutung bzw.
  Anwendungsbereich eines Wortes: "Theoretischer Anwendungsbereich eines
  Wortes: die Klasse der Gegenst{\"a}nde, auf die ein Wort kraft seiner
  Bedeutung beschreibend angewandt werden kann (TA). Empirischer
  Anwendungsbereich eines Wortes: die Menge der Gegenst{\"a}nde, zu deren
  Beschreibung ein Wort - bis zu einem gewissen Zeitpunkt - in der Erfahrung
  eines bestimmten SprachteiInehmers bereits angewandt wurde (EA)." 58:
  Hyponymie l{\"a}sst sich hierarchisch {\"u}bertragen, ist also transitiv:
  "Die Relationen der Hyper- und Hyponymie sind transitiv. Wenn A Hyponym in
  Bezug auf B ist, dann auch in Bezug auf die Hyperonyme von B, und wenn B
  Hyperonym in Bezug auf A ist, dann auch in Bezug auf die Hyponyme von A."%
    }
    \field{month}{02}
    \field{year}{2013}
  \endentry

  \entry{crossleyDevelopmentSemanticRelations2010}{article}{}
    \name{author}{3}{}{%
      {{hash=CSA}{%
         family={Crossley},
         familyi={C\bibinitperiod},
         given={Scott\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=ST}{%
         family={Salsbury},
         familyi={S\bibinitperiod},
         given={Tom},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MDS}{%
         family={McNamara},
         familyi={M\bibinitperiod},
         given={Danielle\bibnamedelima S.},
         giveni={D\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \strng{namehash}{CSASTMDS1}
    \strng{fullhash}{CSASTMDS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2010}
    \field{labeldatesource}{year}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{pages}{55\bibrangedash 74}
    \field{shorttitle}{The Development of Semantic Relations in Second Language
  Speakers}
    \field{title}{The Development of Semantic Relations in Second Language
  Speakers: {{A}} Case for Latent Semantic Analysis.}
    \field{volume}{7}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/37ZW5ST9/Crossley et al. - 2010 - Th
    \verb e development of semantic relations in second la.pdf
    \endverb
    \field{journaltitle}{Vigo International Journal of Applied Linguistics}
    \field{annotation}{%
    56: Auswendiglernen hilft nur bei hochfrequenten Vokabeln im
  anf{\"a}nglichen Spracherwerb, f{\"u}r Fortgeschrittene sind semantische
  Relationen wichtiger: "While it may be true that explicit vocabulary
  instruction concentrating on the first 2,000 to 3,000 words is valuable for
  the beginning learner [...], it is generally agreed that subsequent
  vocabulary acquisition results from inference strategies and the development
  of word connections [...]." 70: Fortgeschrittene Sprachlernende entwickeln
  immer engere semantische Verkn{\"u}pfungen zwischen einzelnen
  Sprachsegmenten: "[...] L2 learners begin to develop closer semantic
  similarities between speech segments as they progress in acquiring a second
  language." 71: Wortschatzerwerb ist zu komplex, um ohne maschinelle Hilfe
  erforscht zu werden: "[...] since lexical acquisition is a phenomenon that is
  generally too complex to be analyzed based on human intuition, computational
  approaches are beneficial."%
    }
    \field{year}{2010}
  \endentry

  \entry{devlinBertPretrainingDeep2018}{article}{}
    \name{author}{4}{}{%
      {{hash=DJ}{%
         family={Devlin},
         familyi={D\bibinitperiod},
         given={Jacob},
         giveni={J\bibinitperiod},
      }}%
      {{hash=CMW}{%
         family={Chang},
         familyi={C\bibinitperiod},
         given={Ming-Wei},
         giveni={M\bibinithyphendelim W\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={Kenton},
         giveni={K\bibinitperiod},
      }}%
      {{hash=TK}{%
         family={Toutanova},
         familyi={T\bibinitperiod},
         given={Kristina},
         giveni={K\bibinitperiod},
      }}%
    }
    \strng{namehash}{DJ+1}
    \strng{fullhash}{DJCMWLKTK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2018}
    \field{labeldatesource}{year}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{shorttitle}{Bert}
    \field{title}{Bert: {{Pre}}-Training of Deep Bidirectional Transformers for
  Language Understanding}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/QIH93SR6/Devlin et al. - 2018 - Bert
    \verb  Pre-training of deep bidirectional transform.pdf;/Users/Konstantin/Z
    \verb otero/storage/Q5R6ARG5/1810.html
    \endverb
    \field{journaltitle}{arXiv preprint arXiv:1810.04805}
    \field{annotation}{%
    8: Gr{\"u}nde f{\"u}r die {\"U}berlegenheit von tief bidirektionalen
  Modellen wie BERT gegen{\"u}ber einfachen LSTMs oder losen Verkettungen
  zweier LSTMs im Sinne eines oberfl{\"a}chlich bidirektionalen Modells (z.B:
  ELMo): "(b) [Concatenating separate Left-To-Right and Right-To-Left models]
  is non-intuitive for tasks like QA, since the RTL model would not be able to
  condition the answer on the question; (c) [this] is strictly less powerful
  than a deep bidirectional model, since it can use both left and right context
  at every layer."%
    }
    \field{year}{2018}
  \endentry

  \entry{divjakCorpusbasedCognitiveSemantics2009}{article}{}
    \name{author}{2}{}{%
      {{hash=DD}{%
         family={Divjak},
         familyi={D\bibinitperiod},
         given={Dagmar},
         giveni={D\bibinitperiod},
      }}%
      {{hash=GST}{%
         family={Gries},
         familyi={G\bibinitperiod},
         given={Stefan\bibnamedelima Th},
         giveni={S\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
    }
    \strng{namehash}{DDGST1}
    \strng{fullhash}{DDGST1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2009}
    \field{labeldatesource}{year}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{pages}{273\bibrangedash 296}
    \field{shorttitle}{Corpus-Based Cognitive Semantics}
    \field{title}{Corpus-Based Cognitive Semantics: {{A}} Contrastive Study of
  Phasal Verbs in {{English}} and {{Russian}}}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/574YI3WY/Divjak and Gries - 2009 - C
    \verb orpus-based cognitive semantics A contrastive st.pdf
    \endverb
    \field{journaltitle}{Studies in cognitive corpus linguistics}
    \field{annotation}{%
    274: Probleme bei der Erforschung von Synonymie: "Polysemy requires the
  researcher to determine whether two usage events are identical or
  sufficiently similar to be considered a single sense, what the degree of
  similarity is between different senses, where to connect a sense to others in
  the network, and which sense(s) to recognize as prototypical one(s). [...] in
  addition, [linguists] have to decide what the differences are between the
  near-synonyms as well as what the relation is between semantically similar
  words in a domain."%
    }
    \field{year}{2009}
  \endentry

  \entry{ellisUsagebasedLanguageInvestigating2013}{article}{}
    \name{author}{3}{}{%
      {{hash=ENC}{%
         family={Ellis},
         familyi={E\bibinitperiod},
         given={Nick\bibnamedelima C.},
         giveni={N\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=OMB}{%
         family={O'Donnell},
         familyi={O\bibinitperiod},
         given={Matthew\bibnamedelima Brook},
         giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=RU}{%
         family={R{\"o}mer},
         familyi={R\bibinitperiod},
         given={Ute},
         giveni={U\bibinitperiod},
      }}%
    }
    \strng{namehash}{ENCOMBRU1}
    \strng{fullhash}{ENCOMBRU1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2013}
    \field{labeldatesource}{year}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \field{pages}{25\bibrangedash 51}
    \field{shorttitle}{Usage-Based Language}
    \field{title}{Usage-Based Language: {{Investigating}} the Latent Structures
  That Underpin Acquisition}
    \field{volume}{63}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/XJ25GGBE/Ellis et al. - 2013 - Usage
    \verb -based language Investigating the latent str.pdf
    \endverb
    \field{journaltitle}{Language Learning}
    \field{annotation}{%
    27: Basiskategorien / Prototypen werden fr{\"u}her erworben als {\"u}ber-
  oder untergeordnete W{\"o}rter: "Basic categories are also those which are
  the most codable (naming is faster), most coded, and most necessary in
  language (being highly frequent in usage). Children acquire basic-category
  terms like dog, bird, hammer, or apple earlier than they do their
  superordinates animal, tool, fruit, or subordinates poodle, wren, ball-peen
  hammer, or Granny Smith."%
    }
    \field{year}{2013}
  \endentry

  \entry{faruquiProblemsEvaluationWord2016}{article}{}
    \name{author}{4}{}{%
      {{hash=FM}{%
         family={Faruqui},
         familyi={F\bibinitperiod},
         given={Manaal},
         giveni={M\bibinitperiod},
      }}%
      {{hash=TY}{%
         family={Tsvetkov},
         familyi={T\bibinitperiod},
         given={Yulia},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=RP}{%
         family={Rastogi},
         familyi={R\bibinitperiod},
         given={Pushpendre},
         giveni={P\bibinitperiod},
      }}%
      {{hash=DC}{%
         family={Dyer},
         familyi={D\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
    }
    \strng{namehash}{FM+1}
    \strng{fullhash}{FMTYRPDC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{labeldatesource}{year}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{title}{Problems with Evaluation of Word Embeddings Using Word
  Similarity Tasks}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/KY2HU9J4/Faruqui et al. - 2016 - Pro
    \verb blems with evaluation of word embeddings using .pdf;/Users/Konstantin
    \verb /Zotero/storage/LNR5YHXT/1605.html
    \endverb
    \field{journaltitle}{arXiv preprint arXiv:1605.02276}
    \field{annotation}{%
    4: Evaluation von Embeddings beruht oft auf Reduzierung der Wortbedeutungen
  auf eine einzige zentrale Bedeutung pro Wort: "However in [the dataset]
  WS-353, bank is given a similarity score of 8.5/10 to money, signifying that
  bank is a financial institution. Such an assumption of one sense per word is
  prevalent in many of the existing word similarity tasks, and it can
  incorrectly penalize a word vector model for capturing a specific sense of
  the word absent in the word similarity task."%
    }
    \field{year}{2016}
  \endentry

  \entry{fellbaumChallengesMultilingualWordnet2012}{article}{}
    \name{author}{2}{}{%
      {{hash=FC}{%
         family={Fellbaum},
         familyi={F\bibinitperiod},
         given={Christiane},
         giveni={C\bibinitperiod},
      }}%
      {{hash=VP}{%
         family={Vossen},
         familyi={V\bibinitperiod},
         given={Piek},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {en}%
    }
    \keyw{Formal ontology,Information system,Multilingual wordnets}
    \strng{namehash}{FCVP1}
    \strng{fullhash}{FCVP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2012}
    \field{labeldatesource}{year}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Wordnets have been created in many languages, revealing both their lexical
  commonalities and diversity. The next challenge is to make multilingual
  wordnets fully interoperable. The EuroWordNet experience revealed the
  shortcomings of an interlingua based on a natural language. Instead, we
  propose a model based on the division of the lexicon and a
  language-independent, formal ontology that serves as the hub interlinking the
  language-specific lexicons. The ontology avoids the idiosyncracies of the
  lexicon and furthermore allows formal reasoning about the concepts it
  contains. We address the division of labor between ontology and lexicon.
  Finally, we illustrate our model in the context of a domain-specific
  multilingual information system based on a central ontology and
  interconnected wordnets in seven languages.%
    }
    \verb{doi}
    \verb 10.1007/s10579-012-9186-z
    \endverb
    \field{issn}{1574-0218}
    \field{number}{2}
    \field{pages}{313\bibrangedash 326}
    \field{title}{Challenges for a Multilingual Wordnet}
    \field{volume}{46}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/59NZN9HK/Fellbaum and Vossen - 2012
    \verb - Challenges for a multilingual wordnet.pdf
    \endverb
    \field{journaltitle}{Language Resources and Evaluation}
    \field{annotation}{%
    315: WordNet als Modell des menschlichen Ged{\"a}chtnisses, v.a. in Bezug
  auf Semantik: "WordNet's original motivation was to test the feasibility of a
  model of human semantic memory that sought to explain principles of storage
  and retrieval of words and concepts."%
    }
    \field{month}{06}
    \field{year}{2012}
  \endentry

  \entry{fowlerAgileManifesto2001}{article}{}
    \name{author}{2}{}{%
      {{hash=FM}{%
         family={Fowler},
         familyi={F\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
      {{hash=HJ}{%
         family={Highsmith},
         familyi={H\bibinitperiod},
         given={Jim},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{FMHJ1}
    \strng{fullhash}{FMHJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2001}
    \field{labeldatesource}{year}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{number}{8}
    \field{pages}{28\bibrangedash 35}
    \field{title}{The Agile Manifesto}
    \field{volume}{9}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/NE9U5PM5/Fowler and Highsmith - 2001
    \verb  - The agile manifesto.pdf
    \endverb
    \field{journaltitle}{Software Development}
    \field{annotation}{%
    32: st{\"a}ndige/iterative Anpassung der Anforderungen und des Designs:
  "[...] agile processes assume and encourage the alteration of requirements
  while the code is being written. As such, design cannot be a purely up-front
  activity to be completed before construction. Instead, design is a continuous
  activity that's performed throughout the project."%
    }
    \field{year}{2001}
  \endentry

  \entry{garcia-penalvoOpenKnowledgeManagement2010}{article}{}
    \name{author}{3}{}{%
      {{hash=GFJ}{%
         family={{Garc{\'i}a-Penalvo}},
         familyi={G\bibinitperiod},
         given={Francisco\bibnamedelima J.},
         giveni={F\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=GC}{%
         family={{Garc{\'i}a de Figuerola}},
         familyi={G\bibinitperiod},
         given={Carlos},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MJA}{%
         family={Merlo},
         familyi={M\bibinitperiod},
         given={Jose\bibnamedelima A.},
         giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \strng{namehash}{GFJGCMJA1}
    \strng{fullhash}{GFJGCMJA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2010}
    \field{labeldatesource}{year}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{number}{4}
    \field{pages}{517\bibrangedash 519}
    \field{title}{Open Knowledge Management in Higher Education}
    \field{volume}{34}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/TSKVH6Y2/Garc√≠a-Penalvo Carlos Garc
    \verb √≠a de Figuerola Jose A. Merlo - 2010 - Open knowledge management in
    \verb higher education.pdf;/Users/Konstantin/Zotero/storage/B46WJT8U/oir.20
    \verb 10.26434daa.html
    \endverb
    \field{journaltitle}{Online Information Review}
    \field{annotation}{%
    518: Offenes Wissen als gemeinsame Grundlage von offener Software und
  offener Wissenschaft etc.: "We think that Open Knowledge comprises Open
  Software, Open Content, Open Science and Open Innovation. [...] Open Software
  owes its deepest roots to Open Access; Open Contents are related to open
  access to the educative, cultural or divulgative contents that are published
  under a non restrictive license that allows copy and distribution, but also
  the right to modify works. Open Science is devoted to the open access to
  scientific contents, while Open Innovation transfers the Open Access
  principles to the enterprise production world, which is actually
  indispensable for the enhancement of University-Enterprises relationships."%
    }
    \field{year}{2010}
  \endentry

  \entry{gardnerValidatingConstructWord2007}{article}{}
    \name{author}{1}{}{%
      {{hash=GD}{%
         family={Gardner},
         familyi={G\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {en}%
    }
    \strng{namehash}{GD1}
    \strng{fullhash}{GD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2007}
    \field{labeldatesource}{year}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \verb{doi}
    \verb 10.1093/applin/amm010
    \endverb
    \field{issn}{0142-6001, 1477-450X}
    \field{number}{2}
    \field{pages}{241\bibrangedash 265}
    \field{shorttitle}{Validating the {{Construct}} of {{Word}} in {{Applied
  Corpus}}-Based {{Vocabulary Research}}}
    \field{title}{Validating the {{Construct}} of {{Word}} in {{Applied
  Corpus}}-Based {{Vocabulary Research}}: {{A Critical Survey}}}
    \field{volume}{28}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/HYDZU5BG/612101f506d0270b002e8a051d5
    \verb 424233478.pdf
    \endverb
    \field{journaltitle}{Applied Linguistics}
    \field{annotation}{%
    248: Wortfamilien sollten von der Wurzel her gelernt werden: "Of particular
  concern here is the question of whether learners of various skill levels and
  language backgrounds are as capable of recognizing and utilizing the common
  morphemic stem of a Word Family when their initial and extensive exposure to
  that stem may be through inflected and derived forms, rather than base forms.
  Extant research in this regard suggests that such a non-linear process may be
  more difficult for learners. [...] Furthermore, while instruction may tend to
  favor the presentation of root forms before their affixed relatives (cf.
  Jiang 2000), there is no way of controlling for such exposure in `authentic'
  texts and during `natural' reading and conversational experiences. This would
  only seem possible through materials and communicative contexts that have
  been linguistically engineered to control for vocabulary presentation." 251:
  Homonymie und Polysemie werden erst im Kontext wirklich erfassbar und
  analysierbar: "This potential for meaning variation (both homonymy and
  polysemy) becomes even more convoluted when the morphological word family is
  considered. For instance, forms that appear to be related through affixation
  may actually be homographs in context (e.g. \emph{bear}, the animal, and
  \emph{bears}/\emph{bearing}, the verb meaning \emph{to carry}) [...]."%
    }
    \field{month}{04}
    \field{year}{2007}
  \endentry

  \entry{gevaudanSemantischeRelationenNominalen1999}{article}{}
    \name{author}{1}{}{%
      {{hash=GP}{%
         family={G{\'e}vaudan},
         familyi={G\bibinitperiod},
         given={Paul},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{GP1}
    \strng{fullhash}{GP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1999}
    \field{labeldatesource}{year}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{pages}{11\bibrangedash 34}
    \field{title}{Semantische {{Relationen}} in Nominalen Und Adjektivischen
  {{Kompositionen}} Und {{Syntagmen}}}
    \field{volume}{9}
    \field{journaltitle}{PhiN. Philologie im Netz}
    \field{annotation}{%
    16: Definition Hyponymie: "Auf der konzeptuellen Ebene, auf der auch
  Similarit{\"a}t und Kontiguit{\"a}t angesiedelt sind, beruht Hyponymie auf
  der~\emph{taxonomischen Inklusion}~von Konzepten. Die taxonomische Inklusion
  ist nichts anderes als eine Ober-/Unterbegriffsbeziehung [...]. Diese
  Konzeptpaare sind teilidentisch, weil die Extension (die Menge der mit dem
  Begriff gemeinten Ph{\"a}nomene) des Unterbegriffs in der des Oberbegriffs
  enthalten ist und gleichzeitig die Intension (die Menge der dem Begriff
  zugeordneten Eigenschaften) des Oberbegriffs in der des Unterbegriffs
  enthalten ist [...]." 17: Zusammenspiel von Hyponymie mit anderen
  semantischen Ph{\"a}nomenen: "(4) Hyponymische Relationen (taxonomische
  Inklusion) a. Polysemie: fr.~\emph{homme}~'Mann' / 'Mensch' b.
  Bedeutungswandel: lt.~\emph{passer}~'Sperling' {$>$}
  sp.~\emph{p{\'a}jaro}~'(kleiner) Vogel' c. Suffigierung:
  lt.~\emph{artus}~'Gelenk' {$>~$}\emph{articulus} 'kleines Gelenk'" 18:
  Hyponymie (taxonomische Inklusion) beruht auf Spezifizierung bzw.
  Generalisierung:%
    }
    \field{year}{1999}
  \endentry

  \entry{griesBehavioralProfilesCorpusbased2009}{article}{}
    \name{author}{2}{}{%
      {{hash=GST}{%
         family={Gries},
         familyi={G\bibinitperiod},
         given={Stefan\bibnamedelima Th},
         giveni={S\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=DD}{%
         family={Divjak},
         familyi={D\bibinitperiod},
         given={Dagmar},
         giveni={D\bibinitperiod},
      }}%
    }
    \strng{namehash}{GSTDD1}
    \strng{fullhash}{GSTDD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2009}
    \field{labeldatesource}{year}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{pages}{57\bibrangedash 75}
    \field{shorttitle}{Behavioral Profiles}
    \field{title}{Behavioral Profiles: A Corpus-Based Approach to Cognitive
  Semantic Analysis}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/JBN4ZAXB/books.html
    \endverb
    \field{journaltitle}{New directions in cognitive linguistics}
    \field{annotation}{%
    59: {\"a}hnliche Wortverteilung weist auf {\"a}hnliche Semantik hin: "[...]
  distributional similarity reflects, or is indicative of, functional
  similarity, the understanding of functional similarity being rather broad,
  i.e. encompassing semantic, discourse-pragmatic, and other functions a
  particular expression can take on." 60: Frequenz, Authentizit{\"a}t,
  Variation und systematische Induktion als Vorteile von Korpora gegen{\"u}ber
  Introspektion/Intuition: "[In contrast to introspection,] corpora a. provide
  many instances rather than a few isolated judgments; b. provide data from
  natural settings rather than 'armchair' judgments or responses that
  potentially reflect experimentally-induced biases; c. provide co-occurrence
  data of many different kinds, i.e. not just those a particular researcher may
  consider important; d. and thus, allow for bottom-up identification of
  relevant distinctions as well as for a more comprehensive description than is
  typically provided."%
    }
    \field{year}{2009}
  \endentry

  \entry{gyllenstenRgramsUnsupervisedLearning2019}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=GAC}{%
         family={Gyllensten},
         familyi={G\bibinitperiod},
         given={Amaru\bibnamedelima Cuba},
         giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=EA}{%
         family={Ekgren},
         familyi={E\bibinitperiod},
         given={Ariel},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sahlgren},
         familyi={S\bibinitperiod},
         given={Magnus},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {en-us}%
    }
    \strng{namehash}{GACEASM1}
    \strng{fullhash}{GACEASM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2019}
    \field{labeldatesource}{year}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{booktitle}{Proceedings of the 13th {{International Conference}} on
  {{Computational Semantics}} - {{Student Papers}}}
    \field{pages}{52\bibrangedash 62}
    \field{shorttitle}{R-Grams}
    \field{title}{R-Grams: {{Unsupervised Learning}} of {{Semantic Units}} in
  {{Natural Language}}}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/HMWS97VS/Gyllensten et al. - 2019 -
    \verb R-grams Unsupervised Learning of Semantic Units i.pdf;/Users/Konstant
    \verb in/Zotero/storage/Z9EN8SK6/W19-0607.html
    \endverb
    \field{annotation}{%
    56f.: word2vec nutzt preprocessing, um die Wortverteilungen f{\"u}r die
  Analyse zu gl{\"a}tten: "It is worth noting that the skipgram model uses
  subsampling of common words, which is an optimization introduced to
  compensate for the power law distribution in common vocabularies. Also, the
  skipgram model controls for collocations by dampening the impact of frequent
  collocations."%
    }
    \field{year}{2019}
  \endentry

  \entry{hagiwaraSupervisedSynonymAcquisition2009}{article}{}
    \name{author}{3}{}{%
      {{hash=HM}{%
         family={Hagiwara},
         familyi={H\bibinitperiod},
         given={Masato},
         giveni={M\bibinitperiod},
      }}%
      {{hash=OY}{%
         family={Ogawa},
         familyi={O\bibinitperiod},
         given={Yasuhiro},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=TK}{%
         family={Toyama},
         familyi={T\bibinitperiod},
         given={Katsuhiko},
         giveni={K\bibinitperiod},
      }}%
    }
    \strng{namehash}{HMOYTK1}
    \strng{fullhash}{HMOYTK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2009}
    \field{labeldatesource}{year}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{number}{2}
    \field{pages}{558\bibrangedash 582}
    \field{title}{Supervised Synonym Acquisition Using Distributional Features
  and Syntactic Patterns}
    \field{volume}{4}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/EAC7LMV9/Hagiwara et al. - 2009 - Su
    \verb pervised synonym acquisition using distributiona.pdf;/Users/Konstanti
    \verb n/Zotero/storage/B2DHR6NQ/ja.html
    \endverb
    \field{journaltitle}{Information and Media Technologies}
    \field{annotation}{%
    566: Modellierung von Synonymie als Pointwise Mutual Information der beiden
  Wortkandidaten in Bezug auf den gemeinsamen Kontext: "The value of
  distributional features f\textsubscript{j}\textsuperscript{D} (x, z) is
  determined so that it represents the degree of commonality of context
  c\textsubscript{j} shared by the word pair (x, z). [...] The advantage of
  this feature construction is that, given the independence assumption between
  word x and z , the feature value is easily calculated as the simple sum of
  two corresponding pointwise mutual information weights as:
  f\textsubscript{j}\textsuperscript{D} (x, z) = PMI(x, c\textsubscript{j}) +
  PMI(z, c\textsubscript{j}) [...]"%
    }
    \field{year}{2009}
  \endentry

  \entry{hamiltonDiachronicWordEmbeddings2016}{article}{}
    \name{author}{3}{}{%
      {{hash=HWL}{%
         family={Hamilton},
         familyi={H\bibinitperiod},
         given={William\bibnamedelima L.},
         giveni={W\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Leskovec},
         familyi={L\bibinitperiod},
         given={Jure},
         giveni={J\bibinitperiod},
      }}%
      {{hash=JD}{%
         family={Jurafsky},
         familyi={J\bibinitperiod},
         given={Dan},
         giveni={D\bibinitperiod},
      }}%
    }
    \strng{namehash}{HWLLJJD1}
    \strng{fullhash}{HWLLJJD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2016}
    \field{labeldatesource}{year}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{title}{Diachronic Word Embeddings Reveal Statistical Laws of
  Semantic Change}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/HKULL62R/Hamilton et al. - 2016 - Di
    \verb achronic word embeddings reveal statistical laws.pdf;/Users/Konstanti
    \verb n/Zotero/storage/Z5L9AGG8/1605.html
    \endverb
    \field{journaltitle}{arXiv preprint arXiv:1605.09096}
    \field{annotation}{%
    8: Kollokationsnetzwerke als n{\"a}herungsweise Quantifizierung von
  Polysemie: "We construct empirical co-occurrence networks for the top-10,000
  non-stop words of each language using the PPMI measure [...]. In these
  networks words are connected to each other if they co-occur more than one
  would expect by chance (after smoothing). The polysemy of a word is then
  measured as its local clustering coefficient within this network [...]."%
    }
    \field{year}{2016}
  \endentry

  \entry{haugCreatingParallelTreebank2008}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=HDT}{%
         family={Haug},
         familyi={H\bibinitperiod},
         given={Dag\bibnamedelima TT},
         giveni={D\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=JM}{%
         family={J{\o}hndal},
         familyi={J\bibinitperiod},
         given={Marius},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{HDTJM1}
    \strng{fullhash}{HDTJM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2008}
    \field{labeldatesource}{year}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Proceedings of the {{Second Workshop}} on {{Language
  Technology}} for {{Cultural Heritage Data}} ({{LaTeCH}} 2008)}
    \field{pages}{27\bibrangedash 34}
    \field{title}{Creating a Parallel Treebank of the Old {{Indo}}-{{European
  Bible}} Translations [{{PROIEL}}]}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/G88CBH7B/Haug and J√∏hndal - 2008 -
    \verb Creating a parallel treebank of the old Indo-Europ.pdf;/Users/Konstan
    \verb tin/Zotero/storage/RSZ8U3HE/Haug and J√∏hndal - 2008 - Creating a par
    \verb allel treebank of the old Indo-Europ.pdf
    \endverb
    \field{annotation}{%
    29: Objekt eines Verbs sagt viel {\"u}ber Wortstellung aus: "To study the
  interaction between syntax/argument structure and pragmatics in determining
  word order, we need to be able to separate objects (OBJ) from other arguments
  of the verb (OBL)"%
    }
    \field{year}{2008}
  \endentry

  \entry{herbelotMeasuringSemanticContent2013}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=HA}{%
         family={Herbelot},
         familyi={H\bibinitperiod},
         given={Aur{\'e}lie},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GM}{%
         family={Ganesalingam},
         familyi={G\bibinitperiod},
         given={Mohan},
         giveni={M\bibinitperiod},
      }}%
    }
    \strng{namehash}{HAGM1}
    \strng{fullhash}{HAGM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2013}
    \field{labeldatesource}{year}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Proceedings of the 51st {{Annual Meeting}} of the
  {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short
  Papers}})}
    \field{pages}{440\bibrangedash 445}
    \field{title}{Measuring Semantic Content in Distributional Vectors}
    \field{volume}{2}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/X7YJ3IHE/Herbelot and Ganesalingam -
    \verb  2013 - Measuring semantic content in distributional vecto.pdf
    \endverb
    \field{annotation}{%
    443: Hypernyme sind nicht notwendigerweise allgemeinere W{\"o}rter im
  distributionellen Sinne, sie k{\"o}nnen trotzdem sehr spezifische Kontexte
  aufweisen (gro{\ss}e Extension, aber kleine Intension): "Although beverage is
  an umbrella word for many various types of drinks, speakers of English use it
  in very particular contexts. So, distributionally, it is not a `general
  word'." 444: bei semantischen Analysen sollte immer auf verschiedene
  Wortbedeutungen geachtet werden: "Some of the errors we observe may also be
  related to word senses. For instance, the word medium, to be found in the
  pair magazine \textendash{} medium, can be synonymous with middle,
  clairvoyant or again mode of communication. In the sense of clairvoyant, it
  is clearly more specific than in the sense intended in the test pair. As
  distributions do not distinguish between senses, this will have an effect on
  our results." 444: Informationsma{\ss}e werden durch starke Kollokationen
  gest{\"o}rt: "[...] strong collocation effects can influence the measurement
  of information negatively: it is an open question which phrases should be
  considered `words-with-spaces' when building distributions."%
    }
    \field{year}{2013}
  \endentry

  \entry{herbelotBuildingSharedWorld2015}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=HA}{%
         family={Herbelot},
         familyi={H\bibinitperiod},
         given={Aur{\'e}lie},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VEM}{%
         family={Vecchi},
         familyi={V\bibinitperiod},
         given={Eva\bibnamedelima Maria},
         giveni={E\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \strng{namehash}{HAVEM1}
    \strng{fullhash}{HAVEM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2015}
    \field{labeldatesource}{year}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Proceedings of the 2015 {{Conference}} on {{Empirical
  Methods}} in {{Natural Language Processing}}}
    \field{pages}{22\bibrangedash 32}
    \field{shorttitle}{Building a Shared World}
    \field{title}{Building a Shared World: {{Mapping}} Distributional to
  Model-Theoretic Semantic Spaces}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/I2QCLQF4/Herbelot and Vecchi - 2015
    \verb - Building a shared world Mapping distributional to.pdf
    \endverb
    \field{annotation}{%
    23: mengentheoretische Zuordnungen (z.B. "Alle/Einige/Keine X sind auch Y")
  k{\"o}nnen mittels distributioneller Semantik induziert werden: "[...]
  human-like intuitions about the quantification of simple subject/predicate
  pairs can be induced from standard distributional data." 27: bei
  ausreichenden, spezialisierten Trainingsdaten (z.B. annotierte Texte {\"u}ber
  Tiere) k{\"o}nnen modelltheoretische Repr{\"a}sentationen f{\"u}r
  Konzept-Eigenschaft-Paare (z.B. bear -{$>$} is\_brown) distributionell
  gelernt werden: "[...] given a reasonable amount of training data for a
  category, we can proficiently generate modeltheoretic representations for
  concept-feature pairs from a distributional space."%
    }
    \field{year}{2015}
  \endentry

  \entry{hestnessDeepLearningScaling2017}{article}{}
    \name{author}{9}{}{%
      {{hash=HJ}{%
         family={Hestness},
         familyi={H\bibinitperiod},
         given={Joel},
         giveni={J\bibinitperiod},
      }}%
      {{hash=NS}{%
         family={Narang},
         familyi={N\bibinitperiod},
         given={Sharan},
         giveni={S\bibinitperiod},
      }}%
      {{hash=AN}{%
         family={Ardalani},
         familyi={A\bibinitperiod},
         given={Newsha},
         giveni={N\bibinitperiod},
      }}%
      {{hash=DG}{%
         family={Diamos},
         familyi={D\bibinitperiod},
         given={Gregory},
         giveni={G\bibinitperiod},
      }}%
      {{hash=JH}{%
         family={Jun},
         familyi={J\bibinitperiod},
         given={Heewoo},
         giveni={H\bibinitperiod},
      }}%
      {{hash=KH}{%
         family={Kianinejad},
         familyi={K\bibinitperiod},
         given={Hassan},
         giveni={H\bibinitperiod},
      }}%
      {{hash=PMMA}{%
         family={Patwary},
         familyi={P\bibinitperiod},
         given={Md\bibnamedelima Mostofa\bibnamedelima Ali},
         giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim
  A\bibinitperiod},
      }}%
      {{hash=YY}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Yang},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Yanqi},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \strng{namehash}{HJ+1}
    \strng{fullhash}{HJNSANDGJHKHPMMAYYZY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2017}
    \field{labeldatesource}{year}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Deep learning (DL) creates impactful advances following a virtuous recipe:
  model architecture search, creating large training data sets, and scaling
  computation. It is widely believed that growing training sets and models
  should improve accuracy and result in better products. As DL application
  domains grow, we would like a deeper understanding of the relationships
  between training set size, computational scale, and model accuracy
  improvements to advance the state-of-the-art. This paper presents a large
  scale empirical characterization of generalization error and model size
  growth as training sets grow. We introduce a methodology for this measurement
  and test four machine learning domains: machine translation, language
  modeling, image processing, and speech recognition. Our empirical results
  show power-law generalization error scaling across a breadth of factors,
  resulting in power-law exponents---the "steepness" of the learning
  curve---yet to be explained by theoretical work. Further, model improvements
  only shift the error but do not appear to affect the power-law exponent. We
  also show that model size scales sublinearly with data size. These scaling
  relationships have significant implications on deep learning research,
  practice, and systems. They can assist model debugging, setting accuracy
  targets, and decisions about data set growth. They can also guide computing
  system design and underscore the importance of continued computational
  scaling.%
    }
    \verb{eprint}
    \verb 1712.00409
    \endverb
    \field{title}{Deep {{Learning Scaling}} Is {{Predictable}},
  {{Empirically}}}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/JLUM4XBH/Hestness et al. - 2017 - De
    \verb ep Learning Scaling is Predictable, Empirically.pdf;/Users/Konstantin
    \verb /Zotero/storage/5DGISFJH/1712.html
    \endverb
    \field{journaltitle}{arXiv:1712.00409 [cs, stat]}
    \field{annotation}{%
    11: kleine Trainingsdaten verlangen kleinere Batch Size (\& Learning Rate?)
  "For large models with fixed hyperparameters, increasing the batch sizes and
  learning rates usually closed a significant portion of the gap to the
  power-law trend. Analogously, smaller training sets often require smaller
  batch sizes to ensure models behave well while fitting." 13: mehr
  Trainingsdaten sind IMMER besser: "We empirically validate that DL model
  accuracy improves as a power-law as we grow training sets for
  state-of-the-art (SOTA) model architectures in four machine learning domains:
  machine translation, language modeling, image processing, and speech
  recognition. These power-law learning curves [exist] across all tested
  domains, model architectures, optimizers, and loss functions." Comment: 19
  pages, 11 figures%
    }
    \field{eprinttype}{arxiv}
    \field{eprintclass}{cs, stat}
    \field{month}{12}
    \field{year}{2017}
    \warn{\item Can't use 'eprinttype' + 'archiveprefix'}
  \endentry

  \entry{karanDistributionalSemanticsApproach2012}{article}{}
    \name{author}{3}{}{%
      {{hash=KM}{%
         family={Karan},
         familyi={K\bibinitperiod},
         given={Mladen},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={{\v S}najder},
         familyi={{\v S}\bibinitperiod},
         given={Jan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BBD}{%
         family={Ba{\v s}ic},
         familyi={B\bibinitperiod},
         given={Bojana\bibnamedelima Dalbelo},
         giveni={B\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
    }
    \strng{namehash}{KMSJBBD1}
    \strng{fullhash}{KMSJBBD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2012}
    \field{labeldatesource}{year}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{pages}{111\bibrangedash 116}
    \field{title}{Distributional Semantics Approach to Detecting Synonyms in
  {{Croatian}} Language}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/FIGU2ATQ/Karan et al. - 2012 - Distr
    \verb ibutional semantics approach to detecting syn.pdf
    \endverb
    \field{journaltitle}{Information Society}
    \field{annotation}{%
    114: einheitliche Vektoren pro Lemma sind bei starker Polysemie
  problematisch: "Distributional representations of each sense of a polysemous
  word get merged into a single distributional representation \textendash{} a
  mixture of distributions. For questions with high polysemy level, the
  corresponding distributional vectors are blurred and the similarity
  comparisons between such vectors are less meaningful." 115: Unterscheidung
  zwischen Synonymie und semantischer Verwandtschaft ist f{\"u}r
  distributionelle Semantik problematisch: "However, manual inspection revealed
  that most false synonyms are syntagmatically related to the target word. To
  avoid this kind of error, we would need a method to distinguish between
  synonymy and general semantic relatedness."%
    }
    \field{year}{2012}
  \endentry

  \entry{minozziLatinWordNetProject2010}{incollection}{}
    \name{author}{1}{}{%
      {{hash=MS}{%
         family={Minozzi},
         familyi={M\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=AP}{%
         family={Anreiter},
         familyi={A\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=KM}{%
         family={Kienpointner},
         familyi={K\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Institut f{\"u}r Sprachen und Literaturen der Universit{\"a}t
  Innsbruck, Bereich Sprachwissenschaft}}%
    }
    \strng{namehash}{MS1}
    \strng{fullhash}{MS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2010}
    \field{labeldatesource}{year}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{booktitle}{Latin Linguistics Today: {{Akten}} Des 15.
  {{Internationalen Kolloquiums}} Zur {{Lateinischen Linguistik}},
  {{Innsbruck}}, 4.- 9. {{April}} 2009}
    \field{isbn}{978-3-85124-723-7}
    \field{pages}{707\bibrangedash 716}
    \field{series}{Innsbrucker {{Beitr{\"a}ge}} Zur {{Sprachwissenschaft}}}
    \field{title}{The {{Latin WordNet Project}}}
    \field{volume}{137}
    \list{location}{1}{%
      {{Innsbruck}}%
    }
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/M7U8QZFR/latin-linguistics-today-akt
    \verb en-des-15-internationalen-kolloquiums-zur-lateinischen-linguistik.htm
    \verb l
    \endverb
    \field{year}{2010}
  \endentry

  \entry{munznerNeueWegeIm2013}{article}{}
    \name{author}{1}{}{%
      {{hash=MI}{%
         family={M{\"u}nzner},
         familyi={M\bibinitperiod},
         given={Iris},
         giveni={I\bibinitperiod},
      }}%
    }
    \strng{namehash}{MI1}
    \strng{fullhash}{MI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2013}
    \field{labeldatesource}{year}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{pages}{218\bibrangedash 273}
    \field{shorttitle}{Neue {{Wege}} Im {{Lateinunterricht}}?}
    \field{title}{Neue {{Wege}} Im {{Lateinunterricht}}? {{Zur}} Praktischen
  {{Umsetzung}} Der {{Kompetenzorientierung}} Am {{Beispiel}} Der
  {{Metamorphosen Ovids}}}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/R6V4G366/M√ºnzner - 2013 - Neue Wege
    \verb  im Lateinunterricht Zur praktischen Ums.pdf
    \endverb
    \field{journaltitle}{Pegasus-Onlinezeitschrift}
    \field{annotation}{%
    243 Anm. 89: Diskussion {\"u}ber lautes Vorlesen im Lateinunterricht:
  "Gl{\"u}cklich / von Albrecht erg{\"a}nzen jedoch auch, dass das Lesen
  eigentlich erst nach dem {\"U}bersetzen und Interpretieren des lateinischen
  Textes sinnvoll sei, da erst dann Rhythmus, Musikalit{\"a}t und die
  inhaltliche Sinnhaftigkeit des Textes ber{\"u}cksichtigt werden k{\"o}nnten
  [...]. Diese Warnung wird nicht v{\"o}llig zu Unrecht ausgesprochen.
  Allerdings kann die Skandierung der Metrik auch als sinnvolle
  Textvorerschlie{\ss}ung gew{\"a}hlt werden und auf bestimmte Ph{\"a}nomene
  des Textes vor der {\"U}bersetzung hinweisen."%
    }
    \field{year}{2013}
  \endentry

  \entry{nerlichPolysemyFlexibilityIntroduction2003}{incollection}{}
    \name{author}{2}{}{%
      {{hash=NB}{%
         family={Nerlich},
         familyi={N\bibinitperiod},
         given={Brigitte},
         giveni={B\bibinitperiod},
      }}%
      {{hash=CDD}{%
         family={Clarke},
         familyi={C\bibinitperiod},
         given={David\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=NB}{%
         family={Nerlich},
         familyi={N\bibinitperiod},
         given={Brigitte},
         giveni={B\bibinitperiod},
      }}%
      {{hash=TZ}{%
         family={Todd},
         familyi={T\bibinitperiod},
         given={Zazie},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=HV}{%
         family={Herman},
         familyi={H\bibinitperiod},
         given={Vimala},
         giveni={V\bibinitperiod},
      }}%
      {{hash=CDD}{%
         family={Clarke},
         familyi={C\bibinitperiod},
         given={David\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {{Mouton de Gruyter}}%
    }
    \strng{namehash}{NBCDD1}
    \strng{fullhash}{NBCDD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2003}
    \field{labeldatesource}{year}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{booktitle}{Polysemy. {{Flexible Patterns}} of {{Meaning}} in
  {{Mind}} and {{Language}}}
    \field{pages}{3\bibrangedash 30}
    \field{shorttitle}{Polysemy and Flexibility}
    \field{title}{Polysemy and Flexibility: Introduction and Overview}
    \list{location}{1}{%
      {{Berlin}}%
    }
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/FU82SJXC/books.html
    \endverb
    \field{annotation}{%
    4: Polysemie vs. Homonymie vs. Ambiguit{\"a}t vs. Vagheit: "To give a
  relatively clear-cut example, the homonyms (river) bank and (financial) bank
  would\\ be accommodated in two entries, the meanings of the polysemous word
  nose ('facial organ', 'sense of smell' and 'attribute of a wine') would be
  accommodated\\ in one. But not all cases are so clear-cut. Another problem
  arising from polysemy and homonymy is lexical ambiguity, and the precise
  relationship between polysemy, homonymy, ambiguity and vagueness is still an
  unresolved issue in lexical semantics." 5: polysemes Wort als Kategorie mit
  netzwerkartig verbundenen Mitgliedern: "In cognitive linguistics, the word
  itself with its network of polysemous senses came to be regarded as a
  category in which the senses of the word (i.e. the members of the category)
  are related to each other by means of general cognitive principles such as
  metaphor, metonymy, generalization, specialization, and image-schema
  transformations."%
    }
    \field{year}{2003}
  \endentry

  \entry{onoWordEmbeddingbasedAntonym2015}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=OM}{%
         family={Ono},
         familyi={O\bibinitperiod},
         given={Masataka},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Miwa},
         familyi={M\bibinitperiod},
         given={Makoto},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SY}{%
         family={Sasaki},
         familyi={S\bibinitperiod},
         given={Yutaka},
         giveni={Y\bibinitperiod},
      }}%
    }
    \strng{namehash}{OMMMSY1}
    \strng{fullhash}{OMMMSY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2015}
    \field{labeldatesource}{year}
    \field{sortinit}{O}
    \field{sortinithash}{O}
    \field{booktitle}{Proceedings of the 2015 {{Conference}} of the {{North
  American Chapter}} of the {{Association}} for {{Computational Linguistics}}:
  {{Human Language Technologies}}}
    \field{pages}{984\bibrangedash 989}
    \field{title}{Word Embedding-Based Antonym Detection Using Thesauri and
  Distributional Information}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/UE83AWER/Ono et al. - 2015 - Word em
    \verb bedding-based antonym detection using thesa.pdf
    \endverb
    \field{annotation}{%
    984-988: Distributionelle Semantik alleine reicht zur Unterscheidung von
  Synonymie und Antonymie nicht aus, man braucht auch Thesauri / Goldstandard:
  "The WE-T model receives supervised information from synonym and antonym
  pairs in thesauri and infers the relations of the other word pairs in the
  thesauri from the supervised information. The WE-TD model incorporates
  corpus-based contextual information (distributional information) into the
  WE-T model, which enables the calculation of the similarities among
  in-vocabulary and out-of-vocabulary words. [...] Our WE-TD model achieved the
  highest score among the models that use both thesauri and distributional
  information."%
    }
    \field{year}{2015}
  \endentry

  \entry{petersDeepContextualizedWord2018}{article}{}
    \name{author}{7}{}{%
      {{hash=PME}{%
         family={Peters},
         familyi={P\bibinitperiod},
         given={Matthew\bibnamedelima E.},
         giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
      {{hash=NM}{%
         family={Neumann},
         familyi={N\bibinitperiod},
         given={Mark},
         giveni={M\bibinitperiod},
      }}%
      {{hash=IM}{%
         family={Iyyer},
         familyi={I\bibinitperiod},
         given={Mohit},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GM}{%
         family={Gardner},
         familyi={G\bibinitperiod},
         given={Matt},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CC}{%
         family={Clark},
         familyi={C\bibinitperiod},
         given={Christopher},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={Kenton},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZL}{%
         family={Zettlemoyer},
         familyi={Z\bibinitperiod},
         given={Luke},
         giveni={L\bibinitperiod},
      }}%
    }
    \strng{namehash}{PME+1}
    \strng{fullhash}{PMENMIMGMCCLKZL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2018}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{title}{Deep Contextualized Word Representations}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/AL3ZNP2P/Peters et al. - 2018 - Deep
    \verb  contextualized word representations.pdf;/Users/Konstantin/Zotero/sto
    \verb rage/UL78NLC2/1802.html
    \endverb
    \field{journaltitle}{arXiv preprint arXiv:1802.05365}
    \field{annotation}{%
    2f.: ELMo ber{\"u}cksichtigt Wortkontext f{\"u}r die Berechnung der
  Vektoren, indem es die Token-Abfolge vorw{\"a}rts und r{\"u}ckw{\"a}rts
  modelliert: "Given a sequence of N tokens, (t1, t2, ..., tN), a forward
  language model computes the probability of the sequence by modeling the
  probability of token tk given the history (t1, ..., tk-1) [...]. A backward
  LM is similar to a forward LM, except it runs over the sequence in reverse,
  predicting the previous token given the future context [...]. A biLM combines
  both a forward and backward LM. Our formulation jointly maximizes the log
  likelihood of the forward and backward directions [...]."%
    }
    \field{year}{2018}
  \endentry

  \entry{pinkalSemantik1993}{incollection}{}
    \name{author}{1}{}{%
      {{hash=PM}{%
         family={Pinkal},
         familyi={P\bibinitperiod},
         given={Manfred},
         giveni={M\bibinitperiod},
      }}%
    }
    \name{editor}{1}{}{%
      {{hash=GG}{%
         family={G{\"o}rz},
         familyi={G\bibinitperiod},
         given={G{\"u}nther},
         giveni={G\bibinitperiod},
      }}%
    }
    \strng{namehash}{PM1}
    \strng{fullhash}{PM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{1993}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{booktitle}{Einf{\"u}hrung in Die K{\"u}nstliche {{Intelligenz}}}
    \field{edition}{1}
    \field{pages}{425\bibrangedash 498}
    \field{title}{Semantik}
    \field{annotation}{%
    427f.: semantische Konstruktion vs. Resolution vs. Auswertung einer
  Aussage: "[...] die Semantikkonstruktion [...], die das semantische Potential
  auf der Grundlage der lexikalischen und syntaktischen Information ermittelt,
  die in der Eingabekette enthalten ist; [...] die semantische Resolution, die
  den aktuellen semantischen Wert bestimmt, unter anderem durch die
  Aufl{\"o}sung von Mehrdeutigkeiten (Disambiguierung) und [...] die
  semantische Auswertung, die durch die Anwendung von Deduktions- und
  Inferenzmechanismen auf den semantischen Wert einer {\"A}u{\ss}erung die
  relevante {\"A}u{\ss}erungsinformation extrahiert und dabei unter anderem
  Weltwissen (episodisches Wissen und Regelwissen) einbezieht."%
    }
    \field{year}{1993}
  \endentry

  \entry{ponsborderiaPathsGrammaticalizationSpanish2014}{incollection}{}
    \name{author}{1}{}{%
      {{hash=PBS}{%
         family={Pons\bibnamedelima Border{\'i}a},
         familyi={P\bibinitperiod\bibinitdelim B\bibinitperiod},
         given={Salvador},
         giveni={S\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=GC}{%
         family={Ghezzi},
         familyi={G\bibinitperiod},
         given={Chiara},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MP}{%
         family={Molinelli},
         familyi={M\bibinitperiod},
         given={Piera},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{PBS1}
    \strng{fullhash}{PBS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2014}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{booktitle}{Discourse and {{Pragmatic Markers}} from {{Latin}} to the
  {{Romance Languages}}}
    \field{pages}{109\bibrangedash 136}
    \field{title}{Paths of Grammaticalization in {{Spanish}} o Sea}
    \field{annotation}{%
    126: eindeutige Unterscheidung zwischen Hyponymie und Kohyponymie nicht
  immer m{\"o}glich: "(29) Y en ese convento nos regalaron diversas veces con
  tortillas de huevos [ ... ]. Un d{\'i}a, o sea una tarde, salimos de dicho
  convento de San Diego, adonde hab{\'i}amos merendado muy bien de dichas
  tortillas (1705, Raimundo de Lantery, Memorias)\\ 'In that convent we were
  given egg omelettes several times [ ... ] One day, or be-SBJV one afternoon,
  we went out of this convent of San Diego, where we\\ had a tasty snack of
  such omelettes'\\ In (29), \emph{o sea} links two words, which can be
  interpreted as either hyperonym {$>$} hyponym (day {$>$} afternoon), or
  exclusive co-hyponyms (morning V afternoon)."%
    }
    \field{year}{2014}
  \endentry

  \entry{punuruLearningNontaxonomicalSemantic2012}{article}{}
    \name{author}{2}{}{%
      {{hash=PJ}{%
         family={Punuru},
         familyi={P\bibinitperiod},
         given={Janardhana},
         giveni={J\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Jianhua},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {en}%
    }
    \keyw{Ontology learning,Semantical relation,Text mining}
    \strng{namehash}{PJCJ1}
    \strng{fullhash}{PJCJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2012}
    \field{labeldatesource}{year}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Ontology of a domain mainly consists of concepts, taxonomical
  (hierarchical) relations and non-taxonomical relations. Automatic ontology
  construction requires methods for extracting both taxonomical and
  non-taxonomical relations. Compared to extensive works on concept extraction
  and taxonomical relation learning, little attention has been given on
  identification and labeling of non-taxonomical relations in text mining. In
  this paper, we propose an unsupervised technique for extracting
  non-taxonomical relations from domain texts. We propose the VF*ICF metric for
  measuring the importance of a verb as a representative relation label, in
  much the same spirit as the TF*IDF measure in information retrieval.
  Domain-relevant concepts (nouns) are extracted using techniques developed
  earlier. Candidate non-taxonomical relations are generated as (SVO) triples
  of the form (subject, verb, object) from domain texts. A statistical method
  with log-likelihood ratios is used to estimate the significance of
  relationships between concepts and to select suitable relation labels. Texts
  from two domains, the Electronic Voting (EV) domain texts and the Tenders and
  Mergers (TNM) domain texts are used to compare our method with one of the
  existing approaches. Experiments showed that our method achieved better
  performance in both domains.%
    }
    \verb{doi}
    \verb 10.1007/s10844-011-0149-4
    \endverb
    \field{issn}{1573-7675}
    \field{number}{1}
    \field{pages}{191\bibrangedash 207}
    \field{title}{Learning Non-Taxonomical Semantic Relations from Domain
  Texts}
    \field{volume}{38}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/RUML34M4/Punuru and Chen - 2012 - Le
    \verb arning non-taxonomical semantic relations from d.pdf
    \endverb
    \field{journaltitle}{Journal of Intelligent Information Systems}
    \field{annotation}{%
    192f.: automatischer Aufbau von Ontologien f{\"u}r das Semantic Web hilft
  bei der Erschlie{\ss}ung dom{\"a}nenspezifischer Texte und erh{\"o}ht die
  Qualit{\"a}t maschineller Suchverfahren: "Semantic Web aims at representing
  the textual content of Web pages by ontologies, which allow machines to
  comprehend the semantics of documents and return more accurate answers to
  user queries. As of now, ontologies for domains of interests are developed
  manually in spite of the wide spread use of ontologies. This situation
  clearly indicates the urgent need for techniques that can automatically learn
  (construct) ontology from domain texts."%
    }
    \field{month}{02}
    \field{year}{2012}
  \endentry

  \entry{roelliCorpusCorporumNew2014}{article}{}
    \name{author}{1}{}{%
      {{hash=RP}{%
         family={Roelli},
         familyi={R\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
    }
    \strng{namehash}{RP1}
    \strng{fullhash}{RP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2014}
    \field{labeldatesource}{year}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{title}{The {{Corpus Corporum}}, a New Open {{Latin}} Text Repository
  and Tool}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/8QP8LVWQ/Roelli - 2014 - The Corpus
    \verb Corporum, a new open Latin text reposit.pdf;/Users/Konstantin/Zotero/
    \verb storage/TMFM3VHH/69525.html
    \endverb
    \field{journaltitle}{Archivum Latinitatis Medii Aevi-Bulletin du Cange
  (ALMA)}
    \field{year}{2014}
  \endentry

  \entry{rollerInclusiveSelectiveSupervised2014}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=RS}{%
         family={Roller},
         familyi={R\bibinitperiod},
         given={Stephen},
         giveni={S\bibinitperiod},
      }}%
      {{hash=EK}{%
         family={Erk},
         familyi={E\bibinitperiod},
         given={Katrin},
         giveni={K\bibinitperiod},
      }}%
      {{hash=BG}{%
         family={Boleda},
         familyi={B\bibinitperiod},
         given={Gemma},
         giveni={G\bibinitperiod},
      }}%
    }
    \strng{namehash}{RSEKBG1}
    \strng{fullhash}{RSEKBG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{2014}
    \field{labeldatesource}{year}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{booktitle}{Proceedings of {{COLING}} 2014, the 25th {{International
  Conference}} on {{Computational Linguistics}}: {{Technical Papers}}}
    \field{pages}{1025\bibrangedash 1036}
    \field{shorttitle}{Inclusive yet Selective}
    \field{title}{Inclusive yet Selective: {{Supervised}} Distributional
  Hypernymy Detection}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/T8VLJX3A/Roller et al. - 2014 - Incl
    \verb usive yet selective Supervised distributional.pdf
    \endverb
    \field{annotation}{%
    1025: Definition Distributional Inclusion Hypothesis: "[The Distributional
  Inclusion Hypothesis] states that more specific terms appear in a subset of
  the distributional contexts in which more general terms appear. So, animal
  can occur in all the contexts in which dog can occur, plus some contexts in
  which dog cannot \textendash{} for instance, rights can be a typical
  cooccurrence for animal (e.g. ``animal rights''), but not so much for dog
  (e.g. \#``dog rights'')."%
    }
    \field{year}{2014}
  \endentry

  \entry{schibelZurAneignungLateinischer2013}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=SW}{%
         family={Schibel},
         familyi={S\bibinitperiod},
         given={Wolfgang},
         giveni={W\bibinitperiod},
      }}%
    }
    \strng{namehash}{SW1}
    \strng{fullhash}{SW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2013}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{booktitle}{Forum {{Classicum}}}
    \field{pages}{113\bibrangedash 124}
    \field{title}{Zur {{Aneignung}} Lateinischer {{Literatur}} Und {{Sprache}}}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/C4K8VU6U/Schibel - 2013 - Zur Aneign
    \verb ung lateinischer Literatur und Sprache.pdf
    \endverb
    \field{annotation}{%
    115: Lekt{\"u}reschock ist bedingt durch schlechtes Design der
  Lehrb{\"u}cher, z.B. zu wenig Progression: "Die Antwort gibt die von
  Lateinlehrern oft zu h{\"o}rende Klage {\"u}ber einen ,Lekt{\"u}reschock`,
  den Sch{\"u}ler beim {\"U}bergang vom Lehrbuch zu Originaltexten erleiden.
  Die wohlmeinende Sorge der Verfasser lateinischer Lehrwerke f{\"u}r einen
  allm{\"a}hlichen, von Lektion zu Lektion sparsam dosierten Anstieg der
  sprachlichen Anforderungen zeitigt einen langen Aufenthalt der Lerner in
  einer fabrizierten Sprachwelt, die nicht zur {\"U}berwindung sprachlicher
  H{\"u}rden anspornt."%
    }
    \field{year}{2013}
  \endentry

  \entry{souille-rigautSemanticAccountQuasiLexemes2010}{thesis}{}
    \name{author}{1}{}{%
      {{hash=SC}{%
         family={{Souill{\'e}-Rigaut}},
         familyi={S\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
    }
    \strng{namehash}{SC1}
    \strng{fullhash}{SC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2010}
    \field{labeldatesource}{year}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{title}{A {{Semantic Account}} of {{Quasi}}-{{Lexemes}} in {{Modern
  English}}-{{Processing Semiotic Units}} of {{Greek}} or {{Latin Origin}} into
  {{Lexical Units}}}
    \list{institution}{1}{%
      {University of Kansas}%
    }
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/WQSJT2H9/Souill√©-Rigaut - 2010 - A
    \verb Semantic Account of Quasi-Lexemes in Modern Engl.pdf;/Users/Konstanti
    \verb n/Zotero/storage/WJSA5ZGD/7406.html
    \endverb
    \field{type}{{{PhD Thesis}}}
    \field{annotation}{%
    24-33: semantische Relationen sind aus der Morphologie herleitbar, selbst
  wenn ein Wort als Ganzes nicht paraphrasiert werden kann: "Because
  quasi-lexemes are protolexical elements, rather than independent lexemes, I
  would argue that they are never paraphrasable. I may say that a toothache is
  when a tooth aches, but I may not say that an odontalgia is when an -odont-
  -alg-. [...] The head constituent in both compounds is on the right, the
  modifier constituent on the left, and, although `cardiopathy' is not
  paraphrasable, we can still deduce from its semiotic units that the compound
  as a whole is a hyponym of the meaning carried by the semiotic unit -path-."
  43: Komposition erm{\"o}glicht Hyponymie bzw. macht sie morphologisch
  sichtbar: "`Dontopedalogy is the science of opening your mouth and putting
  your foot in it, a science which I have practiced for a good many years'.
  There is evidence that we are dealing with a secondary compound of the type W
  + X + Y, that is to say, with the semiotic units -dont- + -ped- + -log- being
  concatenated synchronically. [...] secondary compounds W + XY are always
  expanded primary compounds in which WXY is a hyponym of XY. The compound XY
  itself may be a primary or a synthetic compound, but stands necessarily as a
  nominal constituent."%
    }
    \field{year}{2010}
  \endentry

  \entry{taslimExperimentalStudyTeaching2014}{article}{}
    \name{author}{1}{}{%
      {{hash=TF}{%
         family={Taslim},
         familyi={T\bibinitperiod},
         given={Fadilla},
         giveni={F\bibinitperiod},
      }}%
    }
    \strng{namehash}{TF1}
    \strng{fullhash}{TF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{labelyear}{2014}
    \field{labeldatesource}{year}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{number}{3}
    \field{pages}{189\bibrangedash 197}
    \field{title}{An {{Experimental Study}} of {{Teaching Vocabulary}} by
  {{Using Hyponymy Games}} on the {{Seventh Grader F MTs Syech Ibrahim
  Payakumbuh}}}
    \field{volume}{21}
    \verb{file}
    \verb /Users/Konstantin/Zotero/storage/SHBTLPL7/Taslim - 2014 - An Experime
    \verb ntal Study of Teaching Vocabulary by Us.pdf
    \endverb
    \field{journaltitle}{Al-Ta lim Journal}
    \field{annotation}{%
    196: hyponymie-basierte Spiele tragen zum Wortschatzerwerb bei: "[...]
  Hyponymy games are highly effective in developing students' level of
  vocabulary [...]."%
    }
    \field{year}{2014}
  \endentry

  \entry{zipfPsychobiologyLanguageIntroduction1936}{book}{}
    \name{author}{1}{}{%
      {{hash=ZGK}{%
         family={Zipf},
         familyi={Z\bibinitperiod},
         given={George\bibnamedelima Kingsley},
         giveni={G\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
    }
    \strng{namehash}{ZGK1}
    \strng{fullhash}{ZGK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{shorttitle}
    \field{labelyear}{1936}
    \field{labeldatesource}{year}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{shorttitle}{The Psycho-Biology of Language}
    \field{title}{The Psycho-Biology of Language: {{An}} Introduction to
  Dynamic Philology}
    \field{annotation}{%
    VI: Definition Zipf'sches Gesetz: "If the number of different words
  occurring once in a given sample is taken as x, the number of different words
  occurring twice, three times, four times, n times, in the same sample, is
  respectively 1/2**2, 1/3**2, 1/4**2,.... 1/n**2 of x, up to, though not
  including, the few most frequently used words; that is, we find an
  unmistakable progression according to the inverse square, valid for well over
  95\% of all the different words used in the sample."%
    }
    \field{year}{1936}
  \endentry
\enddatalist
\endinput
