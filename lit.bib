
@article{adiFinegrainedAnalysisSentence2016,
  title = {Fine-Grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks},
  author = {Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
  year = {2016},
  journal = {arXiv preprint arXiv:1608.04207},
  note = {1: Addition oder Durchschnittswerte als einfache Operationalisierung von Satzvektoren (sentence vectors): "A simple and common approach is producing word-level vectors using, e.g., word2vec [...], and summing or averaging the vectors of the words participating in the sentence. This continuous-bag-of-words [...] approach disregards the word order in the sentence."}
}

@inproceedings{akbikContextualStringEmbeddings2018,
  title = {Contextual String Embeddings for Sequence Labeling},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  year = {2018},
  pages = {1638--1649},
  note = {1640: Definition contextualized string (bzw. character-level) embeddings, d.h. auf Zeichenebene: "By training a language model, we learn P (xt |x0, . . . , xt-1), an estimate of the predictive distribution over the next character given past characters."}
}

@article{amramiBetterSubstitutionbasedWord2019,
  title = {Towards Better Substitution-Based Word Sense Induction},
  author = {Amrami, Asaf and Goldberg, Yoav},
  year = {2019},
  month = may,
  abstract = {Word sense induction (WSI) is the task of unsupervised clustering of word usages within a sentence to distinguish senses. Recent work obtain strong results by clustering lexical substitutes derived from pre-trained RNN language models (ELMo). Adapting the method to BERT improves the scores even further. We extend the previous method to support a dynamic rather than a fixed number of clusters as supported by other prominent methods, and propose a method for interpreting the resulting clusters by associating them with their most informative substitutes. We then perform extensive error analysis revealing the remaining sources of errors in the WSI task. Our code is available at https://github.com/asafamr/bertwsi.},
  archivePrefix = {arXiv},
  eprint = {1905.12598},
  eprinttype = {arxiv},
  file = {/Users/Konstantin/Zotero/storage/VJPA6TZF/1905.html},
  journal = {arXiv:1905.12598 [cs]},
  keywords = {Computer Science - Computation and Language},
  note = {1: Clustering von Stellvertreter-Vektoren hilft, um Ambiguit\"at/Unsicherheit bei der Bestimmung von Wortbedeutungen zu formalisieren: "A competing approach relies on substitute vectors: each target instance is represented by a distribution over possible in-context probable substitutes for the word, and clustering is performed over these distributions."
\par
4: Clustering-basierte Ermittlung von Wortbedeutungen findet (auf empirischer Basis) semantische Unterscheidungen, die durch menschliche Intuition (z.B. Wissensdatenbanken) nicht gefunden wurden: "As an example, one induced sense for the target meet(VERB) is characterized by the words ``convene'', ``group'', ``crowd'', indicating the sense of a meeting that involves many participants. Interestingly, the WordNet meet(VERB) entry does not make such a distinction between meeting types by the number of their participants, highlighting a case were the unsupervised algorithm refined the human curated lexicon."},
  primaryClass = {cs}
}

@incollection{anstattTypenSemantischerRelationen2009,
  title = {Typen Semantischer {{Relationen}}},
  booktitle = {Die Slavischen {{Sprachen}}. {{Ein}} Internationales {{Handbuch}} Zu Ihrer {{Geschichte}}, Ihrer {{Struktur}} Und Ihrer {{Erforschung}}},
  author = {Anstatt, Tanja},
  editor = {Berger, T. and Gutschmidt, K. and Kempgen, S. and Kosta, P.},
  year = {2009},
  pages = {906--915},
  file = {/Users/Konstantin/Zotero/storage/UJJVJSM9/Tanja_Anstatt_2009_Typen_semantischer_Relationen.html},
  note = {913: Suffixe zur Bildung von Hyponymen in slawischen Sprachen: "[...] zur Bildung von Hyponymen kann z.B. das Suffix -ovye/-evye (seld' ,Hering` \textendash{} sel'devye ,Heringsartige`) verwendet werden [...]."}
}

@inproceedings{ayseExtractionSemanticWord2011,
  title = {Extraction of {{Semantic Word Relations}} in {{Turkish}} from {{Dictionary Definitions}}},
  booktitle = {Proceedings of the {{ACL}} 2011 {{Workshop}} on {{Relational Models}} of {{Semantics}}},
  author = {Ay{\c s}e, {\c S}erbet{\c c}i and Zeynep, Orhan and {\.I}lknur, Pehlivan},
  year = {2011},
  month = jun,
  pages = {11--18},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}},
  note = {12: semantische Relationen m\"ussen immer je nach Wortbedeutung differenziert werden: "For more accurate semantic analysis, the connection between words should be established between appropriate senses of the words. To be more concrete, an example can be given on the semantically ambiguous word as; y\"uz `face' or `hundred'. When a has-a relation is detected between the words v\"ucut `body' and y\"uz, the appropriate sense for y\"uz should be selected as `face', instead of `hundred'."}
}

@article{baidakKeyDataTeaching2017,
  title = {Key {{Data}} on {{Teaching Languages}} at {{School}} in {{Europe}}. 2017 {{Edition}}. {{Eurydice Report}}.},
  author = {Ba{\"i}dak, Nathalie and Balcon, Marie-Pascale and Motiejunaite, Akvile},
  year = {2017},
  publisher = {{ERIC}},
  issn = {9294924823},
  journal = {Education, Audiovisual and Culture Executive Agency, European Commission},
  note = {51: Latein als Pflichtsprache f\"ur (zumindest einige) Lernende des ISCED-2-Bereichs in 1/3 der europ\"aischen L\"ander: "LATIN IS COMPULSORY FOR AT LEAST SOME UPPER SECONDARY STUDENTS IN ONE THIRD OF EUROPEAN COUNTRIES"
\par
52: Verpflichtung zum Erlernen von Latein/Altgriechisch f\"ur das Abitur an Altsprachlichen Gymnasien: "Both classical Greek and Latin are compulsory for students in certain schools or educational pathways in Germany, Greece, Croatia, Italy, the Netherlands, Romania and Iceland. For example, in Germany, Latin and Classical Greek are compulsory for pupils who want to attain the \emph{Allgemeine Hochschulreife} in a \emph{Gymnasium} specialising in classical languages."}
}

@inproceedings{bakerWordNetFrameNetComplementary2009,
  title = {{{WordNet}} and {{FrameNet}} as {{Complementary Resources}} for {{Annotation}}},
  booktitle = {Proceedings of the {{Third Linguistic Annotation Workshop}} ({{LAW III}})},
  author = {Baker, Collin F. and Fellbaum, Christiane},
  year = {2009},
  month = aug,
  pages = {125--129},
  publisher = {{Association for Computational Linguistics}},
  address = {{Suntec, Singapore}},
  note = {127: Nutzung von Korpora zur Zusammenf\"uhrung bzw. Harmonisierung von Wissensdatenbanken (Knowledge Bases): "[...] researchers are semi-automatically annotating selected lemmas in the American National Corpus with both [FrameNet] frames and [WordNet] senses. [...] A preliminary group of instances are manually tagged by trained annotators, and then the teams working on [WordNet] and [FrameNet] annotation discuss and resolve discrepancies among the taggers before the remaining tokens are annotated."}
}

@inproceedings{bartunovBreakingSticksAmbiguities2016,
  title = {Breaking Sticks and Ambiguities with Adaptive Skip-Gram},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  year = {2016},
  pages = {130--138},
  keywords = {reference},
  note = {131: AdaGram erlaubt mehrere Wortbedeutungen f\"ur jedes Wort: "It retains all noticeable properties of [Skip-gram] such as fast online learning and high quality of representations while allowing to automatically learn the necessary number of prototypes per word at desired semantic resolution."
\par
137: Testset f\"ur Word Sense Induction aus den Disambiguierungsseiten von Wikipedia gewonnen: "Since the problem of learning multi-prototype word representation is closely related to word-sense induction, we evaluated AdaGram on several WSI datasets and contributed a new large one obtained automatically from Wikipedia disambiguation pages."}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  volume = {3},
  pages = {1137--1155},
  journal = {Journal of machine learning research},
  note = {1140: Kontinuierliche Vektorr\"aume (z.B. Word Embeddings) als statistische Modelle f\"ur Wortverteilungen: "A statistical model of language can be represented by the conditional probability of the next word given all the previous ones [...]. When building statistical models of natural language, one considerably reduces the difficulty of this modeling problem by taking advantage of word order, and the fact that temporally closer words in the word sequence are statistically more dependent. [...] In contrast, here we push this idea to a large scale, and concentrate on learning a statistical model of the distribution of word sequences, rather than learning the role of words in a sentence."
\par
1141: Definition kontinuierlicher Vektorr\"aume bzw. Word Embeddings: "In the model proposed here, instead of characterizing the similarity with a discrete random or deterministic variable (which corresponds to a soft or hard partition of the set of words), we use a continuous real-vector for each word, i.e. a learned distributed feature vector, to represent similarity between words. [...] The idea of using a vector-space representation for words has been well exploited in the area of information retrieval [...], where feature vectors for words are learned on the basis of their probability of co-occurring in the same documents (Latent Semantic Indexing [...]). An important difference is that here we look for a representation for words that is helpful in representing compactly the probability distribution of word sequences from natural language text."},
  number = {Feb}
}

@inproceedings{boledaIntensionalityWasOnly2013,
  title = {Intensionality Was Only Alleged: {{On}} Adjective-Noun Composition in Distributional Semantics},
  shorttitle = {Intensionality Was Only Alleged},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Computational Semantics}} ({{IWCS}} 2013): Long Papers; 2013 {{Mar}} 20-22; {{Postdam}}, {{Germany}}. {{Stroudsburg}} ({{USA}}): {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Boleda, Gemma and Baroni, Marco and McNally, Louise},
  year = {2013},
  pages = {35--46},
  publisher = {{ACL (Association for Computational Linguistics)}},
  file = {/Users/Konstantin/Zotero/storage/NX339DFP/37126.html},
  note = {42: Modifizierende Adjektive k\"onnen die Polysemie von Substantiven deutlich eingrenzen und dadurch zur Disambiguierung der Wortbedeutung wesentlich beitragen: "We find that, the more polysemous a noun is, the less similar its vector is to the corresponding phrase vector. It is plausible that modifying a noun has a larger impact when the noun is polysemous, as the adjective narrows down the meaning of the noun; indeed, adjectives have been independently shown to be powerful word sense disambiguators of nouns [...]. In distributional terms, the adjective notably ``shifts'' the vector of polysemous nouns, but for monosemous nouns there is just not much shifting room."}
}

@article{boneApplyingMachineLearning2015,
  title = {Applying Machine Learning to Facilitate Autism Diagnostics: Pitfalls and Promises},
  author = {Bone, Daniel and Goodwin, Matthew S and Black, Matthew P and Lee, Chi-Chun and Audhkhasi, Kartik and Narayanan, Shrikanth},
  year = {2015},
  volume = {45},
  pages = {1121--1136},
  publisher = {{Springer}},
  issn = {0162-3257},
  journal = {Journal of autism and developmental disorders},
  note = {1127: Unausgewogene Trainingsdaten f\"uhren zu geringem Recall f\"ur Klassifikatoren mittels Machine Learning: "Since confidence in recall for a class with a small number of samples is low, regardless of the overall size of the data set, this more balanced dataset is preferable for machine learning experiments."},
  number = {5}
}

@inproceedings{borgesvolkerHDTUDVeryLarge2019,
  title = {{{HDT}}-{{UD}}: {{A}} Very Large {{Universal Dependencies Treebank}} for {{German}}},
  shorttitle = {{{HDT}}-{{UD}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Universal Dependencies}} ({{UDW}}, {{SyntaxFest}} 2019)},
  author = {Borges V{\"o}lker, Emanuel and Wendt, Maximilian and Hennig, Felix and K{\"o}hn, Arne},
  year = {2019},
  month = aug,
  pages = {46--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Paris, France}},
  doi = {10.18653/v1/W19-8006}
}

@inproceedings{carlsonArchitectureNeverendingLanguage2010,
  title = {Toward an Architecture for Never-Ending Language Learning},
  booktitle = {Twenty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R. and Mitchell, Tom M.},
  year = {2010},
  pages = {1306--1313},
  file = {/Users/Konstantin/Zotero/storage/PXP2U9WG/1879.html},
  note = {1306: semantische Relationen sind auch im Bereich der k\"unstlichen Intelligenz f\"ur den Spracherwerb unerl\"asslich: "At present, [Never-Ending Language Learner] acquires two types of knowledge: (1) knowledge about which noun phrases refer to which specified semantic categories, such as cities, companies, and sports teams, and (2) knowledge about which pairs of noun phrases satisfy which specified semantic relations, such as hasOfficesIn(organization, location)."}
}

@book{coenenAnalogieUndMetapher2013,
  title = {{Analogie und Metapher: Grundlegung einer Theorie der bildlichen Rede}},
  shorttitle = {{Analogie und Metapher}},
  author = {Coenen, Hans Georg},
  year = {2013},
  month = feb,
  publisher = {{Walter de Gruyter}},
  abstract = {Von zahllosen anderen Ver\"offentlichungen zur bildlichen Rede unterscheidet sich das Studienbuch "Analogie und Metapher" in zweifacher Hinsicht: 1. Es verbindet Anregungen der antiken Rhetorik mit Errungenschaften der modernen Sprachwissenschaft und -philosophie, so da\ss{} die Entw\"urfe der Klassiker in ein tragf\"ahigeres theoretisches Ger\"ust eingef\"ugt werden k\"onnen. 2. Es f\"uhrt die verschiedenen Erscheinungsformen der bildlichen Rede auf eine gemeinsame Tiefenstruktur zur\"uck: die Analogie, verstanden als Geltung eines gemeinsamen Beschreibungsinhalts f\"ur zwei verschiedene Gegenst\"ande. Die Differenzierung des Analogiebegriffs - z.B. in ein- und mehrstellige sowie triviale und nicht-triviale Analogien - erweist sich als Schl\"ussel sowohl zum linguistischen Verst\"andnis des Ph\"anomens der Bildlichkeit wie auch zur Interpretation einzelner sprachlicher Bilder etwa in der Literatur. Insofern ist dieses Studienbuch sowohl f\"ur Linguisten als auch f\"ur Literaturwissenschaftler von Interesse.},
  googlebooks = {HfUgAAAAQBAJ},
  isbn = {978-3-11-089463-9},
  keywords = {Language Arts \& Disciplines / Linguistics / Historical \& Comparative,Literary Criticism / European / German,Literary Criticism / General,Literary Criticism / Semiotics \& Theory},
  language = {de},
  note = {46: Unterscheidung von theoretischer und empirischer Bedeutung bzw. Anwendungsbereich eines Wortes: "Theoretischer Anwendungsbereich eines Wortes: die Klasse der Gegenst\"ande, auf die ein Wort kraft seiner Bedeutung beschreibend angewandt werden kann (TA). Empirischer Anwendungsbereich eines Wortes: die Menge der Gegenst\"ande, zu deren Beschreibung ein Wort - bis zu einem gewissen Zeitpunkt - in der Erfahrung eines bestimmten SprachteiInehmers bereits angewandt wurde (EA)."
\par
58: Hyponymie l\"asst sich hierarchisch \"ubertragen, ist also transitiv: "Die Relationen der Hyper- und Hyponymie sind transitiv. Wenn A Hyponym in Bezug auf B ist, dann auch in Bezug auf die Hyperonyme von B, und wenn B Hyperonym in Bezug auf A ist, dann auch in Bezug auf die Hyponyme von A."}
}

@article{crossleyDevelopmentSemanticRelations2010,
  title = {The Development of Semantic Relations in Second Language Speakers: {{A}} Case for Latent Semantic Analysis.},
  shorttitle = {The Development of Semantic Relations in Second Language Speakers},
  author = {Crossley, Scott A. and Salsbury, Tom and McNamara, Danielle S.},
  year = {2010},
  volume = {7},
  pages = {55--74},
  journal = {Vigo International Journal of Applied Linguistics},
  note = {56: Auswendiglernen hilft nur bei hochfrequenten Vokabeln im anf\"anglichen Spracherwerb, f\"ur Fortgeschrittene sind semantische Relationen wichtiger: "While it may be true that explicit vocabulary instruction concentrating on the first 2,000 to 3,000 words is valuable for the beginning learner [...], it is generally agreed that subsequent vocabulary acquisition results from inference strategies and the development of word connections [...]."
\par
70: Fortgeschrittene Sprachlernende entwickeln immer engere semantische Verkn\"upfungen zwischen einzelnen Sprachsegmenten: "[...] L2 learners begin to develop closer semantic similarities between speech segments as they progress in acquiring a second language."
\par
71: Wortschatzerwerb ist zu komplex, um ohne maschinelle Hilfe erforscht zu werden: "[...] since lexical acquisition is a phenomenon that is generally too complex to be analyzed based on human intuition, computational approaches are beneficial."}
}

@article{daiDocumentEmbeddingParagraph2015,
  title = {Document {{Embedding}} with {{Paragraph Vectors}}},
  author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
  year = {2015},
  month = jul,
  abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
  archivePrefix = {arXiv},
  eprint = {1507.07998},
  eprinttype = {arxiv},
  file = {/Users/Konstantin/Zotero/storage/F2THWCYK/1507.html},
  journal = {arXiv:1507.07998 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {4: Paragraph-Vektoren erlauben dieselben arithmetischen Verfahren wie Wort-Vektoren: "The second experiment is to find the Japanese equivalence of ``Lady Gaga.'' This can be achieved by vector operations: pv(``Lady Gaga'') - wv(``American'') + wv(``Japanese'') where pv is paragraph vectors and wv is word vectors. Both sets of results show that Paragraph Vectors can achieve the same kind of analogies like Word Vectors [...]."},
  primaryClass = {cs}
}

@book{devineLatinWordOrder2006,
  title = {Latin Word Order: {{Structured}} Meaning and Information},
  author = {Devine, Andrew M. and Stephens, Laurence D.},
  year = {2006},
  publisher = {{Oxford University Press}},
  isbn = {0-19-972050-9},
  note = {452: autorenspezifischer Einsatz von Syntax/Wortstellung/Wortreihenfolge zur Markierung semantischer Unterschiede: "It is clear from the table that in Cicero and Caesar the postmodifier predominates for the locative meaning but the premodifier for the temporal meaning. However in Livy the premodifier predominates for both meanings."}
}

@article{devlinBertPretrainingDeep2018,
  title = {Bert: {{Pre}}-Training of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  file = {/Users/Konstantin/Zotero/storage/Q5R6ARG5/1810.html},
  journal = {arXiv preprint arXiv:1810.04805},
  keywords = {reference},
  note = {8: Gr\"unde f\"ur die \"Uberlegenheit von tief bidirektionalen Modellen wie BERT gegen\"uber einfachen LSTMs oder losen Verkettungen zweier LSTMs im Sinne eines oberfl\"achlich bidirektionalen Modells (z.B: ELMo): "(b) [Concatenating separate Left-To-Right and Right-To-Left models] is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) [this] is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer."}
}

@article{divjakCorpusbasedCognitiveSemantics2009,
  title = {Corpus-Based Cognitive Semantics: {{A}} Contrastive Study of Phasal Verbs in {{English}} and {{Russian}}},
  shorttitle = {Corpus-Based Cognitive Semantics},
  author = {Divjak, Dagmar and Gries, Stefan Th},
  year = {2009},
  pages = {273--296},
  journal = {Studies in cognitive corpus linguistics},
  note = {274: Probleme bei der Erforschung von Synonymie: "Polysemy requires the researcher to determine whether two usage events are identical or sufficiently similar to be considered a single sense, what the degree of similarity is between different senses, where to connect a sense to others in the network, and which sense(s) to recognize as prototypical one(s). [...] in addition, [linguists] have to decide what the differences are between the near-synonyms as well as what the relation is between semantically similar words in a domain."}
}

@article{ellisUsagebasedLanguageInvestigating2013,
  title = {Usage-Based Language: {{Investigating}} the Latent Structures That Underpin Acquisition},
  shorttitle = {Usage-Based Language},
  author = {Ellis, Nick C. and O'Donnell, Matthew Brook and R{\"o}mer, Ute},
  year = {2013},
  volume = {63},
  pages = {25--51},
  journal = {Language Learning},
  note = {27: Basiskategorien / Prototypen werden fr\"uher erworben als \"uber- oder untergeordnete W\"orter: "Basic categories are also those which are the most codable (naming is faster), most coded, and most necessary in language (being highly frequent in usage). Children acquire basic-category terms like dog, bird, hammer, or apple earlier than they do their superordinates animal, tool, fruit, or subordinates poodle, wren, ball-peen hammer, or Granny Smith."}
}

@article{faruquiProblemsEvaluationWord2016,
  title = {Problems with Evaluation of Word Embeddings Using Word Similarity Tasks},
  author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
  year = {2016},
  file = {/Users/Konstantin/Zotero/storage/LNR5YHXT/1605.html},
  journal = {arXiv preprint arXiv:1605.02276},
  keywords = {reference},
  note = {4: Evaluation von Embeddings beruht oft auf Reduzierung der Wortbedeutungen auf eine einzige zentrale Bedeutung pro Wort: "However in [the dataset] WS-353, bank is given a similarity score of 8.5/10 to money, signifying that bank is a financial institution. Such an assumption of one sense per word is prevalent in many of the existing word similarity tasks, and it can incorrectly penalize a word vector model for capturing a specific sense of the word absent in the word similarity task."}
}

@article{fellbaumChallengesMultilingualWordnet2012,
  title = {Challenges for a Multilingual Wordnet},
  author = {Fellbaum, Christiane and Vossen, Piek},
  year = {2012},
  month = jun,
  volume = {46},
  pages = {313--326},
  issn = {1574-0218},
  doi = {10.1007/s10579-012-9186-z},
  abstract = {Wordnets have been created in many languages, revealing both their lexical commonalities and diversity. The next challenge is to make multilingual wordnets fully interoperable. The EuroWordNet experience revealed the shortcomings of an interlingua based on a natural language. Instead, we propose a model based on the division of the lexicon and a language-independent, formal ontology that serves as the hub interlinking the language-specific lexicons. The ontology avoids the idiosyncracies of the lexicon and furthermore allows formal reasoning about the concepts it contains. We address the division of labor between ontology and lexicon. Finally, we illustrate our model in the context of a domain-specific multilingual information system based on a central ontology and interconnected wordnets in seven languages.},
  journal = {Language Resources and Evaluation},
  keywords = {Formal ontology,Information system,Multilingual wordnets},
  language = {en},
  note = {315: WordNet als Modell des menschlichen Ged\"achtnisses, v.a. in Bezug auf Semantik: "WordNet's original motivation was to test the feasibility of a model of human semantic memory that sought to explain principles of storage and retrieval of words and concepts."},
  number = {2}
}

@incollection{firthSynopsisLinguisticTheory1957,
  title = {A Synopsis of Linguistic Theory 1930-55},
  booktitle = {Studies in {{Linguistic Analysis}}},
  author = {Firth, J. R.},
  editor = {Firth, J. R.},
  year = {1957},
  pages = {1--32},
  publisher = {{Blackwell}},
  address = {{Oxford}},
  abstract = {Reprinted in: Palmer, F. R. (ed.) (1968). Selected Papers of J. R. Firth 1952-59, pages 168-205. Longmans, London.},
  keywords = {classic linguistics meanign relatedness semantic,reference},
  note = {30: Kollokationen (Lexik) und Kolligationen (Syntax) als Ann\"aherung an die (distributionelle) Semantik eines Textes: "The meaning of texts is dealt with by a dispersal of analysis at mutually congruent series of levels, beginning with contexts of situation and proceeding through collocation, syntax (including colligation) to phonology and phonetics with or without the use of machines."}
}

@inproceedings{fischerGroundTruthCreation2010,
  title = {Ground Truth Creation for Handwriting Recognition in Historical Documents},
  booktitle = {Proceedings of the 9th {{IAPR International Workshop}} on {{Document Analysis Systems}}},
  author = {Fischer, Andreas and Inderm{\"u}hle, Emanuel and Bunke, Horst and Viehhauser, Gabriel and Stolz, Michael},
  year = {2010},
  pages = {3--10},
  doi = {https://dl.acm.org/doi/pdf/10.1145/1815330.1815331},
  note = {4: Goldstandard in Datens\"atzen sollte fehlerfrei und eindeutig sein, um eine ad\"aquate Evaluation von NLP-Systemen zu gew\"ahrleisten: "For the ground truth, however, errors are not tolerable, because ground truth data should provide clean learning examples for the recognition systems and serve as an error-free reference to evaluate the correctness of the systems."}
}

@article{fowlerAgileManifesto2001,
  title = {The Agile Manifesto},
  author = {Fowler, Martin and Highsmith, Jim},
  year = {2001},
  volume = {9},
  pages = {28--35},
  journal = {Software Development},
  note = {32: st\"andige/iterative Anpassung der Anforderungen und des Designs: "[...] agile processes assume and encourage the alteration of requirements while the code is being written. As such, design cannot be a purely up-front activity to be completed before construction. Instead, design is a continuous activity that's performed throughout the project."},
  number = {8}
}

@article{garcia-gamezLearningNounsVerbs2019,
  title = {Learning Nouns and Verbs in a Foreign Language: {{The}} Role of Gestures},
  author = {{Garcia-Gamez}, Ana B and Macizo, Pedro},
  year = {2019},
  volume = {40},
  pages = {473--507},
  publisher = {{Cambridge University Press}},
  issn = {0142-7164},
  journal = {Applied Psycholinguistics},
  note = {9: Substantive werden generell leichter erworben als Verben, aber nur in Kulturen, in denen auch viel Wert auf Substantive gelegt wird: "However, apart from gestures, it has been corroborated that nouns are easier to learn than verbs, at least for children. Different studies have shown that children are able to learn English nouns easier than verbs in their natural context [...]. It is probably due to the fact that English speakers place special emphasis on nouns when they interact with children while acquiring their L1. However, this advantage is not present in other cultures (see Gentner \& Boroditsky, 2001; Gopnik \& Choi, 1995; Tardif et al., 1997, for the absence of this advantage in Korean and Mandarin languages)."},
  number = {2}
}

@article{garcia-penalvoOpenKnowledgeManagement2010,
  title = {Open Knowledge Management in Higher Education},
  author = {{Garc{\'i}a-Penalvo}, Francisco J. and {Garc{\'i}a de Figuerola}, Carlos and Merlo, Jose A.},
  year = {2010},
  volume = {34},
  pages = {517--519},
  file = {/Users/Konstantin/Zotero/storage/B46WJT8U/oir.2010.26434daa.html},
  journal = {Online Information Review},
  note = {518: Offenes Wissen als gemeinsame Grundlage von offener Software und offener Wissenschaft etc.: "We think that Open Knowledge comprises Open Software, Open Content, Open Science and Open Innovation. [...] Open Software owes its deepest roots to Open Access; Open Contents are related to open access to the educative, cultural or divulgative contents that are published under a non restrictive license that allows copy and distribution, but also the right to modify works. Open Science is devoted to the open access to scientific contents, while Open Innovation transfers the Open Access principles to the enterprise production world, which is actually indispensable for the enhancement of University-Enterprises relationships."},
  number = {4}
}

@article{gardnerValidatingConstructWord2007,
  title = {Validating the {{Construct}} of {{Word}} in {{Applied Corpus}}-Based {{Vocabulary Research}}: {{A Critical Survey}}},
  shorttitle = {Validating the {{Construct}} of {{Word}} in {{Applied Corpus}}-Based {{Vocabulary Research}}},
  author = {Gardner, D.},
  year = {2007},
  month = apr,
  volume = {28},
  pages = {241--265},
  issn = {0142-6001, 1477-450X},
  doi = {10.1093/applin/amm010},
  journal = {Applied Linguistics},
  language = {en},
  note = {248: Wortfamilien sollten von der Wurzel her gelernt werden: "Of particular concern here is the question of whether learners of various skill levels and language backgrounds are as capable of recognizing and utilizing the common morphemic stem of a Word Family when their initial and extensive exposure to that stem may be through inflected and derived forms, rather than base forms. Extant research in this regard suggests that such a non-linear process may be more difficult for learners. [...] Furthermore, while instruction may tend to favor the presentation of root forms before their affixed relatives (cf. Jiang 2000), there is no way of controlling for such exposure in `authentic' texts and during `natural' reading and conversational experiences. This would only seem possible through materials and communicative contexts that have been linguistically engineered to control for vocabulary presentation."
\par
251: Homonymie und Polysemie werden erst im Kontext wirklich erfassbar und analysierbar: "This potential for meaning variation (both homonymy and polysemy) becomes even more convoluted when the morphological word family is considered. For instance, forms that appear to be related through affixation may actually be homographs in context (e.g. \emph{bear}, the animal, and \emph{bears}/\emph{bearing}, the verb meaning \emph{to carry}) [...]."},
  number = {2}
}

@book{georgesAusfuhrlichesLateinischDeutschesHandworterbuch1913,
  title = {Ausf\"uhrliches {{Lateinisch}}-{{Deutsches Handw\"orterbuch}}},
  author = {Georges, Karl Ernst},
  year = {1913}
}

@inproceedings{gladkovaAnalogybasedDetectionMorphological2016,
  title = {Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings: What Works and What Doesn't},
  shorttitle = {Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings},
  booktitle = {Proceedings of the {{NAACL Student Research Workshop}}},
  author = {Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi},
  year = {2016},
  pages = {8--15},
  keywords = {reference},
  note = {8: Analogien mittels Word Embeddings zeigen verschiedene semantische und morphologische Relationen: "Analogies have been successfully used for detecting different semantic relations, such as synonymy and antonymy (Turney, 2008), ConceptNet relations and selectional preferences (Herdadelen and Baroni, 2009), and also for inducing morphological categories from unparsed data (Soricut and Och, 2015)."}
}

@inproceedings{golcherStylometryInterplayTopic2011,
  title = {Stylometry and the Interplay of Topic and {{L1}} in the Different Annotation Layers in the {{FALKO}} Corpus},
  booktitle = {Proceedings of {{Quantitative Investigations}} in {{Theoretical Linguistics}} 4},
  author = {Golcher, Felix and Reznicek, Marc},
  editor = {Zeldes, Amir and L{\"u}deling, Anke},
  year = {2011},
  pages = {29--34},
  publisher = {{Humboldt-Universit\"at zu Berlin}},
  note = {31: Textl\"angeneffekt als Problem bei stilometrischen Untersuchungen: "As hinted above, this matrix is normalized by dividing each cell by the mean of the respective row and column (Golcher 2007). This removes all text specific contributions to S, most notably the impact of text length."
\par
33: Thema eines Textes hat gro\ss en Einfluss auf sprachliche Gestaltung, v.a. Syntax: "[...] the influence of text topic on forms of expression is much larger than has been considered so far, even with regard to syntactic structures. Future [stylometric] studies should therefore control the data not only for text type, register and genre but for topic as well."}
}

@article{grefenstetteExperimentalSupportCategorical2011,
  title = {Experimental Support for a Categorical Compositional Distributional Model of Meaning},
  author = {Grefenstette, Edward and Sadrzadeh, Mehrnoosh},
  year = {2011},
  journal = {arXiv preprint arXiv:1106.4058},
  note = {9: Repr\"asentation der Semantik transitiver Verben durch ihre Argumente (Subjekt, Objekt):
\par
9: unterschiedliche digitale Repr\"asentationen (Vektor, Matrix) f\"ur Substantive und Verben/Adjektive/Adverbien: "The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the [British National Corpus]."}
}

@article{griesBehavioralProfilesCorpusbased2009,
  title = {Behavioral Profiles: A Corpus-Based Approach to Cognitive Semantic Analysis},
  shorttitle = {Behavioral Profiles},
  author = {Gries, Stefan Th and Divjak, Dagmar},
  year = {2009},
  pages = {57--75},
  file = {/Users/Konstantin/Zotero/storage/JBN4ZAXB/books.html},
  journal = {New directions in cognitive linguistics},
  keywords = {reference},
  note = {59: \"ahnliche Wortverteilung weist auf \"ahnliche Semantik hin: "[...] distributional similarity reflects, or is indicative of, functional similarity, the understanding of functional similarity being rather broad, i.e. encompassing semantic, discourse-pragmatic, and other functions a particular expression can take on."
\par
60: Frequenz, Authentizit\"at, Variation und systematische Induktion als Vorteile von Korpora gegen\"uber Introspektion/Intuition: "[In contrast to introspection,] corpora a. provide many instances rather than a few isolated judgments; b. provide data from natural settings rather than 'armchair' judgments or responses that potentially reflect experimentally-induced biases; c. provide co-occurrence data of many different kinds, i.e. not just those a particular researcher may consider important; d. and thus, allow for bottom-up identification of relevant distinctions as well as for a more comprehensive description than is typically provided."}
}

@inproceedings{gyllenstenRgramsUnsupervisedLearning2019,
  title = {R-Grams: {{Unsupervised Learning}} of {{Semantic Units}} in {{Natural Language}}},
  shorttitle = {R-Grams},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Computational Semantics}} - {{Student Papers}}},
  author = {Gyllensten, Amaru Cuba and Ekgren, Ariel and Sahlgren, Magnus},
  year = {2019},
  pages = {52--62},
  file = {/Users/Konstantin/Zotero/storage/Z9EN8SK6/W19-0607.html},
  language = {en-us},
  note = {56f.: word2vec nutzt preprocessing, um die Wortverteilungen f\"ur die Analyse zu gl\"atten: "It is worth noting that the skipgram model uses subsampling of common words, which is an optimization introduced to compensate for the power law distribution in common vocabularies. Also, the skipgram model controls for collocations by dampening the impact of frequent collocations."}
}

@article{hagiwaraSupervisedSynonymAcquisition2009,
  title = {Supervised Synonym Acquisition Using Distributional Features and Syntactic Patterns},
  author = {Hagiwara, Masato and Ogawa, Yasuhiro and Toyama, Katsuhiko},
  year = {2009},
  volume = {4},
  pages = {558--582},
  file = {/Users/Konstantin/Zotero/storage/B2DHR6NQ/ja.html},
  journal = {Information and Media Technologies},
  note = {566: Modellierung von Synonymie als Pointwise Mutual Information der beiden Wortkandidaten in Bezug auf den gemeinsamen Kontext: "The value of distributional features f\textsubscript{j}\textsuperscript{D} (x, z) is determined so that it represents the degree of commonality of context c\textsubscript{j} shared by the word pair (x, z). [...] The advantage of this feature construction is that, given the independence assumption between word x and z , the feature value is easily calculated as the simple sum of two corresponding pointwise mutual information weights as: f\textsubscript{j}\textsuperscript{D} (x, z) = PMI(x, c\textsubscript{j}) + PMI(z, c\textsubscript{j}) [...]"},
  number = {2}
}

@article{hamiltonDiachronicWordEmbeddings2016,
  title = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  year = {2016},
  file = {/Users/Konstantin/Zotero/storage/Z5L9AGG8/1605.html},
  journal = {arXiv preprint arXiv:1605.09096},
  keywords = {reference},
  note = {8: Kollokationsnetzwerke als n\"aherungsweise Quantifizierung von Polysemie: "We construct empirical co-occurrence networks for the top-10,000 non-stop words of each language using the PPMI measure [...]. In these networks words are connected to each other if they co-occur more than one would expect by chance (after smoothing). The polysemy of a word is then measured as its local clustering coefficient within this network [...]."}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  volume = {10},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  journal = {Word},
  keywords = {reference},
  language = {en},
  note = {162: die Verbselektion von Pronomina richtet sich nach ihren Bezugsw\"ortern, deshalb haben Pronomina keine "eigene" Bedeutung: "[...] we will get after \emph{The pianist-} much the same verbs as we will get after \emph{The pianist who-}, and so for every noun. This means that the verb selection of \emph{who} is the same as the verb selection of the noun preceding \emph{who}. We have here a distributional characteristic that distinguishes such pronominal elements from ordinary nouns."},
  number = {2-3}
}

@inproceedings{haugCreatingParallelTreebank2008,
  title = {Creating a Parallel Treebank of the Old {{Indo}}-{{European Bible}} Translations},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Language Technology}} for {{Cultural Heritage Data}} ({{LaTeCH}} 2008)},
  author = {Haug, Dag TT and Johndal, Marius},
  year = {2008},
  pages = {27--34},
  annotation = {PROIEL},
  note = {29: Objekt eines Verbs sagt viel \"uber Wortstellung aus: "To study the interaction between syntax/argument structure and pragmatics in determining word order, we need to be able to separate objects (OBJ) from other arguments of the verb (OBL)"}
}

@inproceedings{herbelotBuildingSharedWorld2015,
  title = {Building a Shared World: {{Mapping}} Distributional to Model-Theoretic Semantic Spaces},
  shorttitle = {Building a Shared World},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Herbelot, Aur{\'e}lie and Vecchi, Eva Maria},
  year = {2015},
  pages = {22--32},
  note = {23: mengentheoretische Zuordnungen (z.B. "Alle/Einige/Keine X sind auch Y") k\"onnen mittels distributioneller Semantik induziert werden: "[...] human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data."
\par
27: bei ausreichenden, spezialisierten Trainingsdaten (z.B. annotierte Texte \"uber Tiere) k\"onnen modelltheoretische Repr\"asentationen f\"ur Konzept-Eigenschaft-Paare (z.B. bear -{$>$} is\_brown) distributionell gelernt werden: "[...] given a reasonable amount of training data for a category, we can proficiently generate modeltheoretic representations for concept-feature pairs from a distributional space."}
}

@inproceedings{herbelotMeasuringSemanticContent2013,
  title = {Measuring Semantic Content in Distributional Vectors},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Herbelot, Aur{\'e}lie and Ganesalingam, Mohan},
  year = {2013},
  volume = {2},
  pages = {440--445},
  note = {443: Hypernyme sind nicht notwendigerweise allgemeinere W\"orter im distributionellen Sinne, sie k\"onnen trotzdem sehr spezifische Kontexte aufweisen (gro\ss e Extension, aber kleine Intension): "Although beverage is an umbrella word for many various types of drinks, speakers of English use it in very particular contexts. So, distributionally, it is not a `general word'."
\par
444: bei semantischen Analysen sollte immer auf verschiedene Wortbedeutungen geachtet werden: "Some of the errors we observe may also be related to word senses. For instance, the word medium, to be found in the pair magazine \textendash{} medium, can be synonymous with middle, clairvoyant or again mode of communication. In the sense of clairvoyant, it is clearly more specific than in the sense intended in the test pair. As distributions do not distinguish between senses, this will have an effect on our results."
\par
444: Informationsma\ss e werden durch starke Kollokationen gest\"ort: "[...] strong collocation effects can influence the measurement of information negatively: it is an open question which phrases should be considered `words-with-spaces' when building distributions."}
}

@misc{hestnessDeepLearningScaling2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  file = {/Users/Konstantin/Zotero/storage/5DGISFJH/1712.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {11: kleine Trainingsdaten verlangen kleinere Batch Size (\& Learning Rate?) "For large models with fixed hyperparameters, increasing the batch sizes and learning rates usually closed a significant portion of the gap to the power-law trend. Analogously, smaller training sets often require smaller batch sizes to ensure models behave well while fitting."
\par
13: mehr Trainingsdaten sind IMMER besser: "We empirically validate that DL model accuracy improves as a power-law as we grow training sets for state-of-the-art (SOTA) model architectures in four machine learning domains: machine translation, language modeling, image processing, and speech recognition. These power-law learning curves [exist] across all tested domains, model architectures, optimizers, and loss functions."}
}

@inproceedings{huangImprovingWordRepresentations2012,
  title = {Improving {{Word Representations}} via {{Global Context}} and {{Multiple Word Prototypes}}},
  booktitle = {Proceedings of the 50th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Eric and Socher, Richard and Manning, Christopher and Ng, Andrew},
  year = {2012},
  month = jul,
  pages = {873--882},
  publisher = {{Association for Computational Linguistics}},
  address = {{Jeju Island, Korea}},
  note = {876: Word Sense Induction durch un\"uberwachtes Clustering aller Kontexte eines Worts: "In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is represented by a weighted average of the context words' vectors, where again, we use idf-weighting as the weighting function, similar to the document context representation described in Section 2.2. We then use spherical k-means to cluster these context representations, which has been shown to model semantic relations well [...]. Finally, each word occurrence in the corpus is re-labeled to its associated cluster and is used to train the word representation for that cluster."
\par
878: wichtige Kriterien f\"ur Datens\"atze zur Word Sense Induction: "The dataset has three interesting characteristics: 1) human judgments are on pairs of words presented in sentential context, 2) word pairs and their contexts are chosen to reflect interesting variations in meanings of homonymous and polysemous words, and 3) verbs and adjectives are present in addition to nouns."}
}

@article{jimeno-yepesExploitingMeSHIndexing2011,
  title = {Exploiting {{MeSH}} Indexing in {{MEDLINE}} to Generate a Data Set for Word Sense Disambiguation},
  author = {{Jimeno-Yepes}, Antonio J. and McInnes, Bridget T. and Aronson, Alan R.},
  year = {2011},
  month = jun,
  volume = {12},
  pages = {1--14},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-12-223},
  abstract = {Evaluation of Word Sense Disambiguation (WSD) methods in the biomedical domain is difficult because the available resources are either too small or too focused on specific types of entities (e.g. diseases or genes). We present a method that can be used to automatically develop a WSD test collection using the Unified Medical Language System (UMLS) Metathesaurus and the manual MeSH indexing of MEDLINE. We demonstrate the use of this method by developing such a data set, called MSH WSD.},
  file = {/Users/Konstantin/Zotero/storage/THBLQSX8/1471-2105-12-223.html},
  journal = {BMC Bioinformatics},
  note = {2: MSH-WSD-Korpus basiert auf Wortbedeutungen aus einem Thesaurus bzw. W\"orterbuch: "The resulting data set contains both biomedical terms and abbreviations and is automatically created using the [Unified Medical Language System] Metathesaurus and the manual [Medical Subject Headings] indexing of [Medical Literature Analysis and Retrieval System Online]."},
  number = {223}
}

@article{karakantaNeuralMachineTranslation2018,
  title = {Neural Machine Translation for Low-Resource Languages without Parallel Corpora},
  author = {Karakanta, Alina and Dehdari, Jon and {van Genabith}, Josef},
  year = {2018},
  volume = {32},
  pages = {167--189},
  journal = {Machine Translation},
  note = {168: Definition Low-Resource Language: "[...] they lack in linguistic resources, e.g. grammars, POS taggers, corpora."}
}

@article{karanDistributionalSemanticsApproach2012,
  title = {Distributional Semantics Approach to Detecting Synonyms in {{Croatian}} Language},
  author = {Karan, Mladen and {\v S}najder, Jan and Ba{\v s}ic, Bojana Dalbelo},
  year = {2012},
  pages = {111--116},
  journal = {Information Society},
  note = {114: einheitliche Vektoren pro Lemma sind bei starker Polysemie problematisch: "Distributional representations of each sense of a polysemous word get merged into a single distributional representation \textendash{} a mixture of distributions. For questions with high polysemy level, the corresponding distributional vectors are blurred and the similarity comparisons between such vectors are less meaningful."
\par
115: Unterscheidung zwischen Synonymie und semantischer Verwandtschaft ist f\"ur distributionelle Semantik problematisch: "However, manual inspection revealed that most false synonyms are syntagmatically related to the target word. To avoid this kind of error, we would need a method to distinguish between synonymy and general semantic relatedness."}
}

@book{kuhnerAusfuhrlicheGrammatikLateinischen1914,
  title = {Ausf\"uhrliche {{Grammatik}} Der Lateinischen {{Sprache}}, 2. {{Teil}}: {{Satzlehre}}},
  author = {K{\"u}hner, Raphael and Stegmann, Carl},
  year = {1914},
  volume = {1},
  publisher = {{Hanovre}}
}

@article{lampleCrosslingualLanguageModel2019,
  title = {Cross-Lingual {{Language Model Pretraining}}},
  author = {Lample, Guillaume and Conneau, Alexis},
  year = {2019},
  month = jan,
  abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
  archivePrefix = {arXiv},
  eprint = {1901.07291},
  eprinttype = {arxiv},
  file = {/Users/Konstantin/Zotero/storage/3JC5XWV3/Lample and Conneau - 2019 - Cross-lingual Language Model Pretraining.pdf},
  journal = {arXiv:1901.07291 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{leeGeneratingGroundTruth2012,
  title = {Generating Ground Truth for Music Mood Classification Using Mechanical Turk},
  booktitle = {Proceedings of the 12th {{ACM}}/{{IEEE}}-{{CS}} Joint Conference on {{Digital Libraries}}},
  author = {Lee, Jin Ha and Hu, Xiao},
  year = {2012},
  pages = {129--138},
  doi = {https://dl.acm.org/doi/pdf/10.1145/2232817.2232842},
  note = {129: Kritik am Begriff 'Ground Truth': "The authors recognize that the term ``gold standard'' may be more suitable for describing the data set generated by human judgments since ``ground truth'' is usually assumed to be the objective measure of reality."}
}

@inproceedings{lewisCompositionalHyponymyPositive2019,
  title = {Compositional {{Hyponymy}} with {{Positive Operators}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Recent Advances}} in {{Natural Language Processing}} ({{RANLP}} 2019)},
  author = {Lewis, Martha},
  year = {2019},
  month = sep,
  pages = {638--647},
  publisher = {{INCOMA Ltd.}},
  address = {{Varna, Bulgaria}},
  doi = {10.26615/978-954-452-056-4_075},
  abstract = {Language is used to describe concepts, and many of these concepts are hierarchical. Moreover, this hierarchy should be compatible with forming phrases and sentences. We use linear-algebraic methods that allow us to encode words as collections of vectors. The representations we use have an ordering, related to subspace inclusion, which we interpret as modelling hierarchical information. The word representations built can be understood within a compositional distributional semantic framework, providing methods for composing words to form phrase and sentence level representations. We show that the resulting representations give competitive results on both word-level hyponymy and sentence-level entailment datasets.},
  note = {640: semantische Modellierung von Substantiven durch die Menge ihrer Hyponyme: "We consider words to be modelled as collections of their instances. To model a noun, we can consider the collection of nouns that are hyponyms of that noun, and form the matrix representation corresponding to that collection."}
}

@article{liuCombiningIntegratedSampling2011,
  title = {Combining Integrated Sampling with {{SVM}} Ensembles for Learning from Imbalanced Datasets},
  author = {Liu, Yang and Yu, Xiaohui and Huang, Jimmy Xiangji and An, Aijun},
  year = {2011},
  volume = {47},
  pages = {617--631},
  publisher = {{Elsevier}},
  issn = {0306-4573},
  journal = {Information Processing \& Management},
  note = {620: stark unausgewogene Trainingsdaten f\"uhren zu schlechter Klassifikation durch Machine Learning: "From Fig. 2a, we find that if the imbalance ratio is moderate, the boundary will still be close to the ``ideal boundary''. This observation demonstrates that SVMs could be robust and self-adjusting; they are thus able to alleviate the problem arising from moderate imbalance. Nonetheless, as the imbalance ratio becomes larger and larger, as illustrated in Fig. 2b and c, the boundaries get evidently biased toward the minority class. As a consequence, making predictions with such an approach may lead to a high false negative rate."},
  number = {4}
}

@article{luoCosineNormalizationUsing2017,
  title = {Cosine {{Normalization}}: {{Using Cosine Similarity Instead}} of {{Dot Product}} in {{Neural Networks}}},
  shorttitle = {Cosine {{Normalization}}},
  author = {Luo, Chunjie and Zhan, Jianfeng and Wang, Lei and Yang, Qiang},
  year = {2017},
  month = oct,
  abstract = {Traditionally, multi-layer neural networks use dot product between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded, thus increases the risk of large variance. Large variance of neuron makes the model sensitive to the change of input distribution, thus results in poor generalization, and aggravates the internal covariate shift which slows down the training. To bound dot product and decrease the variance, we propose to use cosine similarity or centered cosine similarity (Pearson Correlation Coefficient) instead of dot product in neural networks, which we call cosine normalization. We compare cosine normalization with batch, weight and layer normalization in fully-connected neural networks as well as convolutional networks on the data sets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that cosine normalization achieves better performance than other normalization techniques.},
  archivePrefix = {arXiv},
  eprint = {1702.05870},
  eprinttype = {arxiv},
  file = {/Users/Konstantin/Zotero/storage/8XEEV8AH/1702.html},
  journal = {arXiv:1702.05870 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {3: Cosine Similarity basiert auf Dot Product, d.h. auch auf Element-Wise Multiplication: "Using cosine normalization, the output of hidden unit is computed by Equation 4. [...] where netnorm is the normalized pre-activation, \textasciitilde w is the incoming weight vector and \textasciitilde x is the input vector, ({$\cdot$}) indicates dot product, f is nonlinear activation function."},
  primaryClass = {cs, stat}
}

@article{luoIncorporatingGlossesNeural2018,
  title = {Incorporating {{Glosses}} into {{Neural Word Sense Disambiguation}}},
  author = {Luo, Fuli and Liu, Tianyu and Xia, Qiaolin and Chang, Baobao and Sui, Zhifang},
  year = {2018},
  month = jul,
  abstract = {Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets.},
  archivePrefix = {arXiv},
  eprint = {1805.08028},
  eprinttype = {arxiv},
  file = {/Users/Konstantin/Zotero/storage/DYM287G2/Luo et al. - 2018 - Incorporating Glosses into Neural Word Sense Disam.pdf;/Users/Konstantin/Zotero/storage/Y6DIAFR4/1805.html},
  journal = {arXiv:1805.08028 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{mcdevitteCaesarGallicWar1869,
  title = {Caesar's {{Gallic War}}, Translated from {{De Bello Gallico}}, {{Julius Caesar}}},
  author = {McDevitte, WA and Bohn, WS},
  year = {1869},
  publisher = {{New York: Harper \& Brothers. Provided by The Internet Classics Archive at~\ldots}}
}

@book{mcgillivrayMethodsLatinComputational2013,
  title = {Methods in {{Latin}} Computational Linguistics},
  author = {McGillivray, Barbara},
  year = {2013},
  publisher = {{Brill}},
  file = {/Users/Konstantin/Zotero/storage/97SCDDJN/books.html},
  keywords = {reference},
  note = {3: statistische Modelle schwer auf Latein / Low-Resource Languages anwendbar: "The lack of large computational resources for Latin has also the effect of making some of the existing computational models (which rely on large amounts of data to make statistical generalizations) difficult to apply to this language."
\par
4: wenig lateinische Korpora und Goldstandards wegen fehlender Muttersprachler: "Historical languages lack native speakers. This means that the\\
manual creation of so-called gold standard resources is more time-consuming and prone to errors than for modern languages. Gold standards for historical\\
languages are particularly important when testing and evaluating automatic models; for example, programs that automatically give the syntactic analysis of sentences can be trained on analysed data (gold standard) before being run on raw data. Therefore, the availability of large, high-quality gold standards is particularly expensive and slow for historical languages, and represents an area where considerable progress can be made."
\par
62: starke Diachronie in lateinischen Korpora macht es f\"ur Sprachmodelle schwer zu generalisieren: "[...] the typical situation in Computational Linguistics involves large, usually homogeneous, synchronic corpora created for modern languages, and these corpora are usually employed to train statistical computational models. When one single model or tool is used for a whole diachronic historical corpus, the potential linguistic differences across the parts of the corpus may lead to a poor performance."}
}

@book{mengeLehrbuchLateinischenSyntax2009,
  title = {Lehrbuch Der Lateinischen {{Syntax}} Und {{Semantik}}},
  author = {Menge, Hermann and Burkard, Thorsten and Schauer, Markus},
  year = {2009},
  edition = {Fourth},
  publisher = {{Wissenschaftliche Buchgesellschaft}},
  collaborator = {Maier, Friedrich},
  isbn = {3-534-13661-6},
  note = {41: verb-lastiger / verbalisierender Charakter des Lateinischen: "Vielen deutschen Substantiven entsprechen im Lateinischen verbale Ausclrucksweisen."}
}

@misc{mihalceaSemCorCorpus2008,
  title = {{{SemCor Corpus}}},
  author = {Mihalcea, Rada},
  year = {2008},
  month = jun,
  abstract = {Sense-tagged Semantic Corpus 3.0},
  file = {/Users/Konstantin/Zotero/storage/V7SKIEMC/metadata.html}
}

@inproceedings{mihalceaSenseval3EnglishLexical2004,
  title = {The {{Senseval}}-3 {{English}} Lexical Sample Task},
  booktitle = {Proceedings of {{SENSEVAL}}-3, the {{Third International Workshop}} on the {{Evaluation}} of {{Systems}} for the {{Semantic Analysis}} of {{Text}}},
  author = {Mihalcea, Rada and Chklovski, Timothy and Kilgarriff, Adam},
  year = {2004},
  month = jul,
  pages = {25--28},
  publisher = {{Association for Computational Linguistics}},
  address = {{Barcelona, Spain}},
  note = {25: Verben im Senseval-3-Korpus sind mit Wortbedeutungen aus einem Online-W\"orterbuch annotiert: "Verbs are instead annotated with senses from Wordsmyth (http://www.wordsmyth.net/)."}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  file = {/Users/Konstantin/Zotero/storage/2F4JX28B/5021-distributed-representations-of-words-andphrases.html},
  note = {6: Abbildung von Phrasen im Vektorraum, indem man sie als einzelne Tokens behandelt: "We decided to use a simple data-driven approach, where phrases are formed based on the unigram and bigram counts, using
\par
score(w\textsubscript{i},w\textsubscript{j}) = (count(w\textsubscript{i}w\textsubscript{j}) - {$\delta$}) / (count(w\textsubscript{i}) x count(w\textsubscript{j})) .
\par
The {$\delta$} is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed. The bigrams with score above the chosen threshold are then used as phrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allowing longer phrases that consists of several words to be formed."}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  shorttitle = {{{Word2Vec}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  file = {/Users/Konstantin/Zotero/storage/VK2LNR37/1301.html},
  journal = {arXiv preprint arXiv:1301.3781},
  keywords = {reference},
  note = {2: Ermittlung von sprachlichen \"Ahnlichkeiten durch Modellierung als Vektorraum: "We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity"
\par
5: Definition CBOW und Skip-gram: "The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word."
\par
5: Definition der Analogie-Schl\"usse: "[...] the relationship is defined by subtracting two word vectors, and the result is added to another word. Thus for example, Paris - France + Italy = Rome."
\par
9: Neuronale Netzwerke (z.B. word2vec) bilden Semantik (z.B. bei der Vervollst\"andigung von S\"atzen) besser ab als traditionelle Algorithmen wie Latent Semantic Analysis:}
}

@incollection{minozziLatinWordNetProject2010,
  title = {The {{Latin WordNet Project}}},
  booktitle = {Latin Linguistics Today: {{Akten}} Des 15. {{Internationalen Kolloquiums}} Zur {{Lateinischen Linguistik}}, {{Innsbruck}}, 4.- 9. {{April}} 2009},
  author = {Minozzi, S.},
  editor = {Anreiter, Peter and Kienpointner, M.},
  year = {2010},
  volume = {137},
  pages = {707--716},
  publisher = {{Institut f\"ur Sprachen und Literaturen der Universit\"at Innsbruck, Bereich Sprachwissenschaft}},
  address = {{Innsbruck}},
  file = {/Users/Konstantin/Zotero/storage/M7U8QZFR/latin-linguistics-today-akten-des-15-internationalen-kolloquiums-zur-lateinischen-linguistik.html},
  isbn = {978-3-85124-723-7},
  series = {Innsbrucker {{Beitr\"age}} Zur {{Sprachwissenschaft}}}
}

@inproceedings{moroAnnotatingMASCCorpus2014,
  title = {Annotating the {{MASC Corpus}} with {{BabelNet}}},
  booktitle = {{{LREC}}},
  author = {Moro, Andrea and Navigli, Roberto and Tucci, Francesco Maria and Passonneau, Rebecca J},
  year = {2014},
  pages = {4214--4219},
  note = {4218: MASC-News-Korpus ist automatisch nach Wortbedeutungen annotiert, u.a. mit Hilfe von WordNet bzw. BabelNet: "We performed the disambiguation of all the word senses and named entities as found in the MASC 3.0 corpus using BabelNet 2.0, a multilingual semantic network that integrates encyclopedic and lexicographic knowledge automatically extracted from WordNet, Wikipedia and OmegaWiki, as our word sense/named entity inventory."}
}

@inproceedings{navigliBabelNetBuildingVery2010,
  title = {{{BabelNet}}: {{Building}} a {{Very Large Multilingual Semantic Network}}},
  shorttitle = {{{BabelNet}}},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Navigli, Roberto and Ponzetto, Simone Paolo},
  year = {2010},
  month = jul,
  pages = {216--225},
  publisher = {{Association for Computational Linguistics}},
  address = {{Uppsala, Sweden}},
  note = {216: Kombination von WordNet und Wikipedia zur Erzeugung einer fl\"achendeckenden Wissensdatenbank: "In this paper, we make a major step towards the vision of a wide-coverage multilingual knowledge resource. We present a novel methodology that produces a very large multilingual semantic network: BabelNet. This resource is created by linking Wikipedia to WordNet via an automatic mapping [...]."}
}

@misc{niederauNavigiumLateinDeutschWorterbuch2012,
  title = {Navigium {{Latein}}-{{Deutsch}}-{{W\"orterbuch}}},
  author = {Niederau, Philipp},
  year = {2012}
}

@incollection{niemeyerZurStellungAttributiven1997,
  title = {Zur {{Stellung}} Des Attributiven {{Adjektivs}} Im {{Lateinischen}} Und in Den Romanischen {{Sprachen}}. {{Syntaktische Gemeinsamkeiten}} Und {{Unterschiede}}},
  booktitle = {Semiotische {{Prozesse}} Und Nat\"urliche {{Sprache}}: {{Festschrift}} F\"ur {{Udo L}}. {{Figge}} Zum 60. {{Geburtstag}}},
  author = {Niemeyer, J{\"u}rgen and Krenn, Herwig},
  editor = {Gather, Andreas and Werner, Heinz},
  year = {1997},
  pages = {411--423},
  publisher = {{Franz Steiner Verlag}},
  address = {{Stuttgart}},
  note = {421: unbegrenzte Freiheit der Wortstellung im Lateinischen, v.a. bei attributiven Adjektiven: "Jedes attributiv verwendbare Adjektiv des Lateinischen kann in Ante- oder Postposition vorkommen und besitzt damit die Eigenschaft der 'Bipositionalit\"at'. Restriktionen dieser Eigenschaft sind nicht bekannt."}
}

@inproceedings{nivreUniversalDependenciesV12016,
  title = {Universal Dependencies v1: {{A}} Multilingual Treebank Collection},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Nivre, Joakim and De Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajic, Jan and Manning, Christopher D and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia},
  year = {2016},
  pages = {1659--1666},
  note = {1660: Beispiel f\"ur Visualisierung von Dependenzb\"aumen / Dependenzgrammatik / Treebanks:}
}

@inproceedings{ochabStylometryLiteraryPapyri2019,
  title = {Stylometry of Literary Papyri},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Digital Access}} to {{Textual Cultural Heritage}}},
  author = {Ochab, Jeremi K. and Essler, Holger},
  year = {2019},
  pages = {139--142},
  file = {/Users/Konstantin/Zotero/storage/WQRFYIPA/3322905.html},
  note = {141: Textl\"angeneffekt sorgt bei Stilometrie/Clustering f\"ur verzerrte Graphen, weil l\"angere Texte gesondert clustern und eigene Gravitationszentren bilden:}
}

@inproceedings{onoWordEmbeddingbasedAntonym2015,
  title = {Word Embedding-Based Antonym Detection Using Thesauri and Distributional Information},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ono, Masataka and Miwa, Makoto and Sasaki, Yutaka},
  year = {2015},
  pages = {984--989},
  note = {984-988: Distributionelle Semantik alleine reicht zur Unterscheidung von Synonymie und Antonymie nicht aus, man braucht auch Thesauri / Goldstandard: "The WE-T model receives supervised information from synonym and antonym pairs in thesauri and infers the relations of the other word pairs in the thesauri from the supervised information. The WE-TD model incorporates corpus-based contextual information (distributional information) into the WE-T model, which enables the calculation of the similarities among in-vocabulary and out-of-vocabulary words. [...] Our WE-TD model achieved the highest score among the models that use both thesauri and distributional information."}
}

@article{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  file = {/Users/Konstantin/Zotero/storage/UL78NLC2/1802.html},
  journal = {arXiv preprint arXiv:1802.05365},
  note = {2f.: ELMo ber\"ucksichtigt Wortkontext f\"ur die Berechnung der Vektoren, indem es die Token-Abfolge vorw\"arts und r\"uckw\"arts modelliert: "Given a sequence of N tokens, (t1, t2, ..., tN), a forward language model computes the probability of the sequence by modeling the probability of token tk given the history (t1, ..., tk-1) [...]. A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context [...]. A biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions [...]."}
}

@incollection{pinkalSemantik1993,
  title = {Semantik},
  booktitle = {Einf\"uhrung in Die K\"unstliche {{Intelligenz}}},
  author = {Pinkal, Manfred},
  editor = {G{\"o}rz, G{\"u}nther},
  year = {1993},
  edition = {First},
  pages = {425--498},
  note = {427f.: semantische Konstruktion vs. Resolution vs. Auswertung einer Aussage:
\par
"[...] die Semantikkonstruktion [...], die das semantische Potential auf der Grundlage der lexikalischen und syntaktischen Information ermittelt, die in der Eingabekette enthalten ist; [...] die semantische Resolution, die den aktuellen semantischen Wert bestimmt, unter anderem durch die Aufl\"osung von Mehrdeutigkeiten (Disambiguierung) und [...] die semantische Auswertung, die durch die Anwendung von Deduktions- und Inferenzmechanismen auf den semantischen Wert einer \"Au\ss erung die relevante \"Au\ss erungsinformation extrahiert und dabei unter anderem Weltwissen (episodisches Wissen und Regelwissen) einbezieht."}
}

@incollection{ponsborderiaPathsGrammaticalizationSpanish2014,
  title = {Paths of Grammaticalization in {{Spanish}} o Sea},
  booktitle = {Discourse and {{Pragmatic Markers}} from {{Latin}} to the {{Romance Languages}}},
  author = {Pons Border{\'i}a, Salvador},
  editor = {Ghezzi, Chiara and Molinelli, Piera},
  year = {2014},
  pages = {109--136},
  note = {126: eindeutige Unterscheidung zwischen Hyponymie und Kohyponymie nicht immer m\"oglich: "(29) Y en ese convento nos regalaron diversas veces con tortillas de huevos [ ... ]. Un d\'ia, o sea una tarde, salimos de dicho convento de San Diego, adonde hab\'iamos merendado muy bien de dichas tortillas (1705, Raimundo de Lantery, Memorias)\\
'In that convent we were given egg omelettes several times [ ... ] One day, or be-SBJV one afternoon, we went out of this convent of San Diego, where we\\
had a tasty snack of such omelettes'\\
In (29), \emph{o sea} links two words, which can be interpreted as either hyperonym {$>$} hyponym (day {$>$} afternoon), or exclusive co-hyponyms (morning V afternoon)."}
}

@article{punuruLearningNontaxonomicalSemantic2012,
  title = {Learning Non-Taxonomical Semantic Relations from Domain Texts},
  author = {Punuru, Janardhana and Chen, Jianhua},
  year = {2012},
  month = feb,
  volume = {38},
  pages = {191--207},
  issn = {1573-7675},
  doi = {10.1007/s10844-011-0149-4},
  abstract = {Ontology of a domain mainly consists of concepts, taxonomical (hierarchical) relations and non-taxonomical relations. Automatic ontology construction requires methods for extracting both taxonomical and non-taxonomical relations. Compared to extensive works on concept extraction and taxonomical relation learning, little attention has been given on identification and labeling of non-taxonomical relations in text mining. In this paper, we propose an unsupervised technique for extracting non-taxonomical relations from domain texts. We propose the VF*ICF metric for measuring the importance of a verb as a representative relation label, in much the same spirit as the TF*IDF measure in information retrieval. Domain-relevant concepts (nouns) are extracted using techniques developed earlier. Candidate non-taxonomical relations are generated as (SVO) triples of the form (subject, verb, object) from domain texts. A statistical method with log-likelihood ratios is used to estimate the significance of relationships between concepts and to select suitable relation labels. Texts from two domains, the Electronic Voting (EV) domain texts and the Tenders and Mergers (TNM) domain texts are used to compare our method with one of the existing approaches. Experiments showed that our method achieved better performance in both domains.},
  journal = {Journal of Intelligent Information Systems},
  keywords = {Ontology learning,Semantical relation,Text mining},
  language = {en},
  note = {192f.: automatischer Aufbau von Ontologien f\"ur das Semantic Web hilft bei der Erschlie\ss ung dom\"anenspezifischer Texte und erh\"oht die Qualit\"at maschineller Suchverfahren: "Semantic Web aims at representing the textual content of Web pages by ontologies, which allow machines to comprehend the semantics of documents and return more accurate answers to user queries. As of now, ontologies for domains of interests are developed manually in spite of the wide spread use of ontologies. This situation clearly indicates the urgent need for techniques that can automatically learn (construct) ontology from domain texts."},
  number = {1}
}

@article{robillardMonolingualBilingualChildren2014,
  title = {Monolingual and Bilingual Children with and without Primary Language Impairment: Core Vocabulary Comparison},
  shorttitle = {Monolingual and Bilingual Children with and without Primary Language Impairment},
  author = {Robillard, Manon and {Mayer-Crittenden}, Chantal and {Minor-Corriveau}, Mich{\`e}le and B{\'e}langer, Roxanne},
  year = {2014},
  volume = {30},
  pages = {267--278},
  file = {/Users/Konstantin/Zotero/storage/QP773HXB/07434618.2014.html},
  journal = {Augmentative and alternative communication},
  note = {2: Definition core vocabulary / basic vocabulary: "The term core vocabulary is used to identify words that are characterized both by their high frequency of use and their high degree of commonality among users"},
  number = {3}
}

@article{roelliCorpusCorporumNew2014,
  title = {The {{Corpus Corporum}}, a New Open {{Latin}} Text Repository and Tool},
  author = {Roelli, Philipp},
  year = {2014},
  file = {/Users/Konstantin/Zotero/storage/TMFM3VHH/69525.html},
  journal = {Archivum Latinitatis Medii Aevi-Bulletin du Cange (ALMA)}
}

@inproceedings{rogersTooManyProblems2017,
  title = {The (Too Many) Problems of Analogical Reasoning with Word Vectors},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (* {{SEM}} 2017)},
  author = {Rogers, Anna and Drozd, Aleksandr and Li, Bofang},
  year = {2017},
  pages = {135--148},
  file = {/Users/Konstantin/Zotero/storage/K5QC7J34/S17-1017.html},
  keywords = {reference},
  note = {142: Analogieschl\"usse funktionieren nicht immer gleich, sondern sind sehr fallspezifisch: "Basically, the problem with analogy is that not all similarities warrant all conclusions, and establishing valid analogies requires much case-by-case consideration."
\par
143: Analogien funktionieren nur f\"ur koh\"arente Klassen von W\"ortern: "[...] it can only work for relations between coherent word classes. That excludes many lexicographic relations like synonyms (car is to automobile as snake is to serpent), frame-semantic or encyclopedic relations (white is to snow as red is to rose)."
\par
143: Vektorraummodelle zeigen morphologische Relationen meist relativ gut, semantische eher schlechter: "Most models score well on morphological inflections \textendash{} because morphological forms of the same word are highly distributionally similar and are likely to be close. But we do not see equal success for synonyms, suffixes, colors and other categories \textendash{} because it is hard to expect of any one model to ``guess'' which words should have synonyms as closest neighbors and which words should be close to their antonyms."}
}

@inproceedings{rollerInclusiveSelectiveSupervised2014,
  title = {Inclusive yet Selective: {{Supervised}} Distributional Hypernymy Detection},
  shorttitle = {Inclusive yet Selective},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Roller, Stephen and Erk, Katrin and Boleda, Gemma},
  year = {2014},
  pages = {1025--1036},
  note = {1025: Definition Distributional Inclusion Hypothesis: "[The Distributional Inclusion Hypothesis] states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot \textendash{} for instance, rights can be a typical cooccurrence for animal (e.g. ``animal rights''), but not so much for dog (e.g. \#``dog rights'')."}
}

@inproceedings{sachanEffectiveUseBidirectional2018,
  title = {Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition},
  booktitle = {Machine {{Learning}} for {{Healthcare Conference}}},
  author = {Sachan, Devendra Singh and Xie, Pengtao and Sachan, Mrinmaya and Xing, Eric P},
  year = {2018},
  pages = {383--402},
  note = {386: Padding wird oft auf der rechten Seite, d.h. am Ende des Inputs angewendet, und mit Null-Vektoren als Platzhalter: "To have a uniform length, each word is right-padded with a special padding token so that the length of every word is the same as that of the longest word in every mini-batch. The embedding of the padding character is always a zero vector."}
}

@article{schibelZurAneignungLateinischer2013,
  title = {Zur {{Aneignung}} Lateinischer {{Literatur}} Und {{Sprache}}},
  author = {Schibel, Wolfgang},
  year = {2013},
  pages = {113--124},
  journal = {Forum Classicum},
  note = {115: Lekt\"ureschock ist bedingt durch schlechtes Design der Lehrb\"ucher, z.B. zu wenig Progression: "Die Antwort gibt die von Lateinlehrern oft zu h\"orende Klage \"uber einen ,Lekt\"ureschock`, den Sch\"uler beim \"Ubergang vom Lehrbuch zu Originaltexten erleiden. Die wohlmeinende Sorge der Verfasser lateinischer Lehrwerke f\"ur einen allm\"ahlichen, von Lektion zu Lektion sparsam dosierten Anstieg der sprachlichen Anforderungen zeitigt einen langen Aufenthalt der Lerner in einer fabrizierten Sprachwelt, die nicht zur \"Uberwindung sprachlicher H\"urden anspornt."}
}

@inproceedings{seddahCheatingParserDeath2018,
  title = {Cheating a {{Parser}} to {{Death}}: {{Data}}-Driven {{Cross}}-{{Treebank Annotation Transfer}}},
  booktitle = {Eleventh {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Seddah, Djam{\'e} and {de La Clergerie}, {\'E}ric Villemonte and Sagot, Beno{\^i}t and Alonso, H{\'e}ctor Mart{\'i}nez and Candito, Marie},
  year = {2018}
}

@book{shortLatinDictionary1879,
  title = {A {{Latin Dictionary}}},
  author = {Short, Charles and Lewis, Charlton T.},
  year = {1879},
  publisher = {{Clarendon Press}},
  address = {{Oxford}}
}

@phdthesis{souille-rigautSemanticAccountQuasiLexemes2010,
  title = {A {{Semantic Account}} of {{Quasi}}-{{Lexemes}} in {{Modern English}}-{{Processing Semiotic Units}} of {{Greek}} or {{Latin Origin}} into {{Lexical Units}}},
  author = {{Souill{\'e}-Rigaut}, Chris},
  year = {2010},
  file = {/Users/Konstantin/Zotero/storage/WJSA5ZGD/7406.html},
  note = {24-33: semantische Relationen sind aus der Morphologie herleitbar, selbst wenn ein Wort als Ganzes nicht paraphrasiert werden kann: "Because quasi-lexemes are protolexical elements, rather than independent lexemes, I would argue that they are never paraphrasable. I may say that a toothache is when a tooth aches, but I may not say that an odontalgia is when an -odont- -alg-. [...] The head constituent in both compounds is on the right, the modifier constituent on the left, and, although `cardiopathy' is not paraphrasable, we can still deduce from its semiotic units that the compound as a whole is a hyponym of the meaning carried by the semiotic unit -path-."
\par
43: Komposition erm\"oglicht Hyponymie bzw. macht sie morphologisch sichtbar: "`Dontopedalogy is the science of opening your mouth and putting your foot in it, a science which I have practiced for a good many years'. There is evidence that we are dealing with a secondary compound of the type W + X + Y, that is to say, with the semiotic units -dont- + -ped- + -log- being concatenated synchronically. [...] secondary compounds W + XY are always expanded primary compounds in which WXY is a hyponym of XY. The compound XY itself may be a primary or a synthetic compound, but stands necessarily as a nominal constituent."},
  school = {University of Kansas},
  type = {{{PhD Thesis}}}
}

@article{stollNounsVerbsChintang2012,
  title = {Nouns and Verbs in {{Chintang}}: Children's Usage and Surrounding Adult Speech},
  author = {Stoll, Sabine and Bickel, Balthasar and Lieven, Elena and Paudyal, Netra P and Banjade, Goma and Bhatta, Toya N and Gaenszle, Martin and Pettigrew, Judith and Rai, Ichchha Purna and Rai, Manoj},
  year = {2012},
  volume = {39},
  pages = {284--321},
  publisher = {{Cambridge University Press}},
  issn = {1469-7602},
  journal = {Journal of Child Language},
  note = {300: Formel f\"ur die Berechnung des Substantiv-Verb-Verh\"altnisses in einem Korpus: "We measured the noun-to-verb ratio by computing the proportion of nouns over the total of nouns and verbs. This leads to a bounded scale between 0 (verbs only) and 1 (nouns only) and avoids division by zero when no verbs occur [...]."},
  number = {2}
}

@article{taslimExperimentalStudyTeaching2014,
  title = {An {{Experimental Study}} of {{Teaching Vocabulary}} by {{Using Hyponymy Games}} on the {{Seventh Grader F MTs Syech Ibrahim Payakumbuh}}},
  author = {Taslim, Fadilla},
  year = {2014},
  volume = {21},
  pages = {189--197},
  journal = {Al-Ta lim Journal},
  note = {196: hyponymie-basierte Spiele tragen zum Wortschatzerwerb bei: "[...] Hyponymy games are highly effective in developing students' level of vocabulary [...]."},
  number = {3}
}

@article{tengiDesignImplementationWordNet1998,
  title = {Design and Implementation of the {{WordNet}} Lexical Database and Searching Software},
  author = {Tengi, Randee I.},
  editor = {Fellbaum, Christiane},
  year = {1998},
  pages = {105--127},
  file = {/Users/Konstantin/Zotero/storage/YJS4ZNCU/books.html},
  journal = {WordNet: an electronic lexical database},
  note = {109: Erkl\"arung der Zeichenk\"urzel/Abk\"urzungen in WordNet, z.B. f\"ur Synonymie, Antonymie etc.:}
}

@article{uzunerCommunityAnnotationExperiment2010,
  title = {Community Annotation Experiment for Ground Truth Generation for the I2b2 Medication Challenge},
  author = {Uzuner, {\"O}zlem and Solti, Imre and Xia, Fei and Cadag, Eithon},
  year = {2010},
  month = sep,
  volume = {17},
  pages = {519--523},
  publisher = {{Oxford Academic}},
  issn = {1067-5027},
  doi = {10.1136/jamia.2010.004200},
  abstract = {Abstract.  Objective: Within the context of the Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records, the authors (also referred t},
  file = {/Users/Konstantin/Zotero/storage/T994HV9P/831043.html},
  journal = {Journal of the American Medical Informatics Association},
  language = {en},
  note = {519: Definition von Ground Truth / Goldstandard / Expertenannotationen: "Traditionally, ground truth is generated by a team consisting of guideline designers, annotators, and technical support staff. The annotators in this team are either domain experts or are trained on the annotation task for a long period of time; therefore, we refer to their annotations as `expert annotations'."},
  number = {5}
}

@incollection{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5998--6008},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/Konstantin/Zotero/storage/M2CUCG48/7181-attention-is-all-you-need.html},
  note = {9: Definition Transformer Language Model mit Self-Attention: "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention."}
}

@article{weiRoleBalancedTraining2013,
  title = {The {{Role}} of {{Balanced Training}} and {{Testing Data Sets}} for {{Binary Classifiers}} in {{Bioinformatics}}},
  author = {Wei, Qiong and Jr, Roland L. Dunbrack},
  year = {2013},
  month = jul,
  volume = {8},
  pages = {e67863},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0067863},
  abstract = {Training and testing of conventional machine learning models on binary classification problems depend on the proportions of the two outcomes in the relevant data sets. This may be especially important in practical terms when real-world applications of the classifier are either highly imbalanced or occur in unknown proportions. Intuitively, it may seem sensible to train machine learning models on data similar to the target data in terms of proportions of the two binary outcomes. However, we show that this is not the case using the example of prediction of deleterious and neutral phenotypes of human missense mutations in human genome data, for which the proportion of the binary outcome is unknown. Our results indicate that using balanced training data (50\% neutral and 50\% deleterious) results in the highest balanced accuracy (the average of True Positive Rate and True Negative Rate), Matthews correlation coefficient, and area under ROC curves, no matter what the proportions of the two phenotypes are in the testing data. Besides balancing the data by undersampling the majority class, other techniques in machine learning include oversampling the minority class, interpolating minority-class data points and various penalties for misclassifying the minority class. However, these techniques are not commonly used in either the missense phenotype prediction problem or in the prediction of disordered residues in proteins, where the imbalance problem is substantial. The appropriate approach depends on the amount of available data and the specific problem at hand.},
  file = {/Users/Konstantin/Zotero/storage/34MUKKKF/article.html},
  journal = {PLOS ONE},
  keywords = {Deletion mutation,Human genomics,Machine learning,Missense mutation,Mutation,Mutation databases,Primates,Substitution mutation},
  language = {en},
  note = {9: Balancing / Ausgewogenheit der Trainingsdaten ist wichtig f\"ur Maschinelles Lernen bei bin\"arer Klassifikation: "[...] support vector machines trained on balanced data sets rather than imbalanced data sets performed better on each of the measures of accuracy commonly used in binary classification [...]."},
  number = {7}
}

@article{yorkConstructivistApproachGamebased2018,
  title = {A Constructivist Approach to Game-Based Language Learning: {{Student}} Perceptions in a Beginner-Level {{EFL}} Context},
  author = {York, James and {deHaan}, Jonathan William},
  year = {2018},
  volume = {8},
  pages = {19--40},
  publisher = {{IGI Global}},
  journal = {International Journal of Game-Based Learning (IJGBL)},
  note = {25: Interaktion mit anderen Lernenden in einer Gruppe als konstruktivistischer Spracherwerbsprozess: "Sociocultural theory applied in second language acquisition contexts has shown that interaction affords the collaborative construction of group expertise."},
  number = {1}
}

@article{yuanCountingNounsSimple2012,
  title = {Counting the {{Nouns}}: {{Simple Structural Cues}} to {{Verb Meaning}}},
  shorttitle = {Counting the {{Nouns}}},
  author = {Yuan, Sylvia and Fisher, Cynthia and Snedeker, Jesse},
  year = {2012},
  volume = {83},
  pages = {1382--1399},
  issn = {1467-8624},
  doi = {10.1111/j.1467-8624.2012.01783.x},
  abstract = {Two-year-olds use the sentence structures verbs appear in\textemdash subcategorization frames\textemdash to guide verb learning. This is syntactic bootstrapping. This study probed the developmental origins of this ability. The structure-mapping account proposes that children begin with a bias toward one-to-one mapping between nouns in sentences and participant roles in events. This account predicts that subcategorization frames should guide very early verb learning, if the number of nouns in the sentences is informative. In 3 experiments, one hundred and thirty-six 21- and 19-month-olds assigned appropriately different interpretations to novel verbs in transitive (``He's gorping him!'') versus intransitive sentences (``He's gorping!'') differing in their number of nouns. Thus, subcategorization frames guide verb interpretation in very young children. These findings constrain theoretical proposals about mechanisms for syntactic bootstrapping.},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8624.2012.01783.x},
  file = {/Users/Konstantin/Zotero/storage/MZ3QG8NW/j.1467-8624.2012.01783.html},
  journal = {Child Development},
  language = {en},
  note = {1383: Kinder im Alter von 1-2 Jahren lernen die Unterscheidung zwischen transitiven und intransitiven Verben, indem sie auf die Anzahl der involvierten Substantive achten: "Even a young child can infer that a verb combined with two nouns implies two participant roles, whereas a verb combined with one noun implies one participant role. Such inferences yield a probabilistic distinction between the transitive and intransitive subcategorization frames."},
  number = {4}
}

@article{yuanSemisupervisedWordSense2016,
  title = {Semi-Supervised {{Word Sense Disambiguation}} with {{Neural Models}}},
  author = {Yuan, Dayu and Richardson, Julian and Doherty, Ryan and Evans, Colin and Altendorf, Eric},
  year = {2016},
  month = nov,
  abstract = {Determining the intended sense of words in text - word sense disambiguation (WSD) - is a long standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text. In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs.},
  archivePrefix = {arXiv},
  eprint = {1603.07012},
  eprinttype = {arxiv},
  file = {/Users/Konstantin/Zotero/storage/32R2SA68/1603.html},
  journal = {arXiv:1603.07012 [cs]},
  keywords = {Computer Science - Computation and Language},
  note = {6: Nutzung von W\"orterb\"uchern/Lexika als Trainingsgrundlage f\"ur Word Sense Disambiguation: "Many dictionary lemmas and senses have no examples in SemCor or [OMSTI], giving rise to losses in all-words [Word Sense Disambiguation] when these corpora are used as training data. The above SemEval scores do not distinguish errors caused by missing training data for certain labels or inaccurate classifier. To better study the proposed algorithms, we train the classifiers with the New Oxford American Dictionary (NOAD) [...], in which there are example sentences for each word sense."},
  primaryClass = {cs}
}

@book{zipfPsychobiologyLanguageIntroduction1936,
  title = {The Psycho-Biology of Language: {{An}} Introduction to Dynamic Philology},
  shorttitle = {The Psycho-Biology of Language},
  author = {Zipf, George Kingsley},
  year = {1936},
  note = {VI: Definition Zipf'sches Gesetz: "If the number of different words occurring once in a given sample is taken as x, the number of different words occurring twice, three times, four times, n times, in the same sample, is respectively 1/2**2, 1/3**2, 1/4**2,.... 1/n**2 of x, up to, though not including, the few most frequently used words; that is, we find an unmistakable progression according to the inverse square, valid for well over 95\% of all the different words used in the sample."}
}


