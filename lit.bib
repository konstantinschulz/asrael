
@article{gardnerValidatingConstructWord2007,
  title = {Validating the {{Construct}} of {{Word}} in {{Applied Corpus}}-Based {{Vocabulary Research}}: {{A Critical Survey}}},
  volume = {28},
  issn = {0142-6001, 1477-450X},
  shorttitle = {Validating the {{Construct}} of {{Word}} in {{Applied Corpus}}-Based {{Vocabulary Research}}},
  language = {en},
  number = {2},
  journal = {Applied Linguistics},
  doi = {10.1093/applin/amm010},
  author = {Gardner, D.},
  month = apr,
  year = {2007},
  pages = {241-265},
  file = {/Users/Konstantin/Zotero/storage/HYDZU5BG/612101f506d0270b002e8a051d5424233478.pdf},
  annote = {248: Wortfamilien sollten von der Wurzel her gelernt werden: "Of particular concern here is the question of whether learners of various skill levels and language backgrounds are as capable of recognizing and utilizing the common morphemic stem of a Word Family when their initial and extensive exposure to that stem may be through inflected and derived forms, rather than base forms. Extant research in this regard suggests that such a non-linear process may be more difficult for learners. [...] Furthermore, while instruction may tend to favor the presentation of root forms before their affixed relatives (cf. Jiang 2000), there is no way of controlling for such exposure in `authentic' texts and during `natural' reading and conversational experiences. This would only seem possible through materials and communicative contexts that have been linguistically engineered to control for vocabulary presentation."

251: Homonymie und Polysemie werden erst im Kontext wirklich erfassbar und analysierbar: "This potential for meaning variation (both homonymy and polysemy) becomes even more convoluted when the morphological word family is considered. For instance, forms that appear to be related through affixation may actually be homographs in context (e.g. \emph{bear}, the animal, and \emph{bears}/\emph{bearing}, the verb meaning \emph{to carry}) [...]."}
}

@inproceedings{haugCreatingParallelTreebank2008,
  title = {Creating a Parallel Treebank of the Old {{Indo}}-{{European Bible}} Translations [{{PROIEL}}]},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Language Technology}} for {{Cultural Heritage Data}} ({{LaTeCH}} 2008)},
  author = {Haug, Dag TT and J{\o}hndal, Marius},
  year = {2008},
  pages = {27--34},
  file = {/Users/Konstantin/Zotero/storage/G88CBH7B/Haug and Jøhndal - 2008 - Creating a parallel treebank of the old Indo-Europ.pdf;/Users/Konstantin/Zotero/storage/RSZ8U3HE/Haug and Jøhndal - 2008 - Creating a parallel treebank of the old Indo-Europ.pdf},
  annote = {29: Objekt eines Verbs sagt viel {\"u}ber Wortstellung aus: "To study the interaction between syntax/argument structure and pragmatics in determining word order, we need to be able to separate objects (OBJ) from other arguments of the verb (OBL)"}
}

@article{ellisUsagebasedLanguageInvestigating2013,
  title = {Usage-Based Language: {{Investigating}} the Latent Structures That Underpin Acquisition},
  volume = {63},
  shorttitle = {Usage-Based Language},
  journal = {Language Learning},
  author = {Ellis, Nick C. and O'Donnell, Matthew Brook and R{\"o}mer, Ute},
  year = {2013},
  pages = {25--51},
  file = {/Users/Konstantin/Zotero/storage/XJ25GGBE/Ellis et al. - 2013 - Usage-based language Investigating the latent str.pdf},
  annote = {27: Basiskategorien / Prototypen werden fr{\"u}her erworben als {\"u}ber- oder untergeordnete W{\"o}rter: "Basic categories are also those which are the most codable (naming is faster), most coded, and most necessary in language (being highly frequent in usage). Children acquire basic-category terms like dog, bird, hammer, or apple earlier than they do their superordinates animal, tool, fruit, or subordinates poodle, wren, ball-peen hammer, or Granny Smith."}
}

@book{zipfPsychobiologyLanguageIntroduction1936,
  title = {The Psycho-Biology of Language: {{An}} Introduction to Dynamic Philology},
  shorttitle = {The Psycho-Biology of Language},
  author = {Zipf, George Kingsley},
  year = {1936},
  annote = {VI: Definition Zipf'sches Gesetz: "If the number of different words occurring once in a given sample is taken as x, the number of different words occurring twice, three times, four times, n times, in the same sample, is respectively 1/2**2, 1/3**2, 1/4**2,.... 1/n**2 of x, up to, though not including, the few most frequently used words; that is, we find an unmistakable progression according to the inverse square, valid for well over 95\% of all the different words used in the sample."}
}

@inproceedings{bartunovBreakingSticksAmbiguities2016,
  title = {Breaking Sticks and Ambiguities with Adaptive Skip-Gram},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  year = {2016},
  pages = {130--138},
  file = {/Users/Konstantin/Zotero/storage/K7AHQAHW/Bartunov et al. - 2016 - Breaking sticks and ambiguities with adaptive skip.pdf},
  annote = {131: AdaGram erlaubt mehrere Wortbedeutungen f{\"u}r jedes Wort: "It retains all noticeable properties of [Skip-gram] such as fast online learning and high quality of representations while allowing to automatically learn the necessary number of prototypes per word at desired semantic resolution."

137: Testset f{\"u}r Word Sense Induction aus den Disambiguierungsseiten von Wikipedia gewonnen: "Since the problem of learning multi-prototype word representation is closely related to word-sense induction, we evaluated AdaGram on several WSI datasets and contributed a new large one obtained automatically from Wikipedia disambiguation pages."}
}

@incollection{nerlichPolysemyFlexibilityIntroduction2003,
  address = {{Berlin}},
  title = {Polysemy and Flexibility: Introduction and Overview},
  shorttitle = {Polysemy and Flexibility},
  booktitle = {Polysemy. {{Flexible Patterns}} of {{Meaning}} in {{Mind}} and {{Language}}},
  publisher = {{Mouton de Gruyter}},
  author = {Nerlich, Brigitte and Clarke, David D.},
  editor = {Nerlich, Brigitte and Todd, Zazie and Herman, Vimala and Clarke, David D.},
  year = {2003},
  pages = {3--30},
  file = {/Users/Konstantin/Zotero/storage/FU82SJXC/books.html},
  annote = {4: Polysemie vs. Homonymie vs. Ambiguit{\"a}t vs. Vagheit: "To give a relatively clear-cut example, the homonyms (river) bank and (financial) bank would\\
be accommodated in two entries, the meanings of the polysemous word nose ('facial organ', 'sense of smell' and 'attribute of a wine') would be accommodated\\
in one. But not all cases are so clear-cut. Another problem arising from polysemy and homonymy is lexical ambiguity, and the precise relationship between polysemy, homonymy, ambiguity and vagueness is still an unresolved issue in lexical semantics."

5: polysemes Wort als Kategorie mit netzwerkartig verbundenen Mitgliedern: "In cognitive linguistics, the word itself with its network of polysemous senses came to be regarded as a category in which the senses of the word (i.e. the members of the category) are related to each other by means of general cognitive principles such as metaphor, metonymy, generalization, specialization, and image-schema transformations."}
}

@article{hagiwaraSupervisedSynonymAcquisition2009,
  title = {Supervised Synonym Acquisition Using Distributional Features and Syntactic Patterns},
  volume = {4},
  number = {2},
  journal = {Information and Media Technologies},
  author = {Hagiwara, Masato and Ogawa, Yasuhiro and Toyama, Katsuhiko},
  year = {2009},
  pages = {558--582},
  file = {/Users/Konstantin/Zotero/storage/EAC7LMV9/Hagiwara et al. - 2009 - Supervised synonym acquisition using distributiona.pdf;/Users/Konstantin/Zotero/storage/B2DHR6NQ/ja.html},
  annote = {566: Modellierung von Synonymie als Pointwise Mutual Information der beiden Wortkandidaten in Bezug auf den gemeinsamen Kontext: "The value of distributional features f\textsubscript{j}\textsuperscript{D} (x, z) is determined so that it represents the degree of commonality of context c\textsubscript{j} shared by the word pair (x, z). [...] The advantage of this feature construction is that, given the independence assumption between word x and z , the feature value is easily calculated as the simple sum of two corresponding pointwise mutual information weights as: f\textsubscript{j}\textsuperscript{D} (x, z) = PMI(x, c\textsubscript{j}) + PMI(z, c\textsubscript{j}) [...]"}
}

@inproceedings{onoWordEmbeddingbasedAntonym2015,
  title = {Word Embedding-Based Antonym Detection Using Thesauri and Distributional Information},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ono, Masataka and Miwa, Makoto and Sasaki, Yutaka},
  year = {2015},
  pages = {984--989},
  file = {/Users/Konstantin/Zotero/storage/UE83AWER/Ono et al. - 2015 - Word embedding-based antonym detection using thesa.pdf},
  annote = {984-988: Distributionelle Semantik alleine reicht zur Unterscheidung von Synonymie und Antonymie nicht aus, man braucht auch Thesauri / Goldstandard: "The WE-T model receives supervised information from synonym and antonym pairs in thesauri and infers the relations of the other word pairs in the thesauri from the supervised information. The WE-TD model incorporates corpus-based contextual information (distributional information) into the WE-T model, which enables the calculation of the similarities among in-vocabulary and out-of-vocabulary words. [...] Our WE-TD model achieved the highest score among the models that use both thesauri and distributional information."}
}

@article{divjakCorpusbasedCognitiveSemantics2009,
  title = {Corpus-Based Cognitive Semantics: {{A}} Contrastive Study of Phasal Verbs in {{English}} and {{Russian}}},
  shorttitle = {Corpus-Based Cognitive Semantics},
  journal = {Studies in cognitive corpus linguistics},
  author = {Divjak, Dagmar and Gries, Stefan Th},
  year = {2009},
  pages = {273--296},
  file = {/Users/Konstantin/Zotero/storage/574YI3WY/Divjak and Gries - 2009 - Corpus-based cognitive semantics A contrastive st.pdf},
  annote = {274: Probleme bei der Erforschung von Synonymie: "Polysemy requires the researcher to determine whether two usage events are identical or sufficiently similar to be considered a single sense, what the degree of similarity is between different senses, where to connect a sense to others in the network, and which sense(s) to recognize as prototypical one(s). [...] in addition, [linguists] have to decide what the differences are between the near-synonyms as well as what the relation is between semantically similar words in a domain."}
}

@inproceedings{herbelotBuildingSharedWorld2015,
  title = {Building a Shared World: {{Mapping}} Distributional to Model-Theoretic Semantic Spaces},
  shorttitle = {Building a Shared World},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Herbelot, Aur{\'e}lie and Vecchi, Eva Maria},
  year = {2015},
  pages = {22--32},
  file = {/Users/Konstantin/Zotero/storage/I2QCLQF4/Herbelot and Vecchi - 2015 - Building a shared world Mapping distributional to.pdf},
  annote = {23: mengentheoretische Zuordnungen (z.B. "Alle/Einige/Keine X sind auch Y") k{\"o}nnen mittels distributioneller Semantik induziert werden: "[...] human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data."

27: bei ausreichenden, spezialisierten Trainingsdaten (z.B. annotierte Texte {\"u}ber Tiere) k{\"o}nnen modelltheoretische Repr{\"a}sentationen f{\"u}r Konzept-Eigenschaft-Paare (z.B. bear -{$>$} is\_brown) distributionell gelernt werden: "[...] given a reasonable amount of training data for a category, we can proficiently generate modeltheoretic representations for concept-feature pairs from a distributional space."}
}

@inproceedings{herbelotMeasuringSemanticContent2013,
  title = {Measuring Semantic Content in Distributional Vectors},
  volume = {2},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Herbelot, Aur{\'e}lie and Ganesalingam, Mohan},
  year = {2013},
  pages = {440--445},
  file = {/Users/Konstantin/Zotero/storage/X7YJ3IHE/Herbelot and Ganesalingam - 2013 - Measuring semantic content in distributional vecto.pdf},
  annote = {443: Hypernyme sind nicht notwendigerweise allgemeinere W{\"o}rter im distributionellen Sinne, sie k{\"o}nnen trotzdem sehr spezifische Kontexte aufweisen (gro{\ss}e Extension, aber kleine Intension): "Although beverage is an umbrella word for many various types of drinks, speakers of English use it in very particular contexts. So, distributionally, it is not a `general word'."

444: bei semantischen Analysen sollte immer auf verschiedene Wortbedeutungen geachtet werden: "Some of the errors we observe may also be related to word senses. For instance, the word medium, to be found in the pair magazine \textendash{} medium, can be synonymous with middle, clairvoyant or again mode of communication. In the sense of clairvoyant, it is clearly more specific than in the sense intended in the test pair. As distributions do not distinguish between senses, this will have an effect on our results."

444: Informationsma{\ss}e werden durch starke Kollokationen gest{\"o}rt: "[...] strong collocation effects can influence the measurement of information negatively: it is an open question which phrases should be considered `words-with-spaces' when building distributions."}
}

@inproceedings{rollerInclusiveSelectiveSupervised2014,
  title = {Inclusive yet Selective: {{Supervised}} Distributional Hypernymy Detection},
  shorttitle = {Inclusive yet Selective},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Roller, Stephen and Erk, Katrin and Boleda, Gemma},
  year = {2014},
  pages = {1025--1036},
  file = {/Users/Konstantin/Zotero/storage/T8VLJX3A/Roller et al. - 2014 - Inclusive yet selective Supervised distributional.pdf},
  annote = {1025: Definition Distributional Inclusion Hypothesis: "[The Distributional Inclusion Hypothesis] states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot \textendash{} for instance, rights can be a typical cooccurrence for animal (e.g. ``animal rights''), but not so much for dog (e.g. \#``dog rights'')."}
}

@inproceedings{gyllenstenRgramsUnsupervisedLearning2019,
  title = {R-Grams: {{Unsupervised Learning}} of {{Semantic Units}} in {{Natural Language}}},
  shorttitle = {R-Grams},
  language = {en-us},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Computational Semantics}} - {{Student Papers}}},
  author = {Gyllensten, Amaru Cuba and Ekgren, Ariel and Sahlgren, Magnus},
  year = {2019},
  pages = {52-62},
  file = {/Users/Konstantin/Zotero/storage/HMWS97VS/Gyllensten et al. - 2019 - R-grams Unsupervised Learning of Semantic Units i.pdf;/Users/Konstantin/Zotero/storage/Z9EN8SK6/W19-0607.html},
  annote = {56f.: word2vec nutzt preprocessing, um die Wortverteilungen f{\"u}r die Analyse zu gl{\"a}tten: "It is worth noting that the skipgram model uses subsampling of common words, which is an optimization introduced to compensate for the power law distribution in common vocabularies. Also, the skipgram model controls for collocations by dampening the impact of frequent collocations."}
}

@article{fowlerAgileManifesto2001,
  title = {The Agile Manifesto},
  volume = {9},
  number = {8},
  journal = {Software Development},
  author = {Fowler, Martin and Highsmith, Jim},
  year = {2001},
  pages = {28--35},
  file = {/Users/Konstantin/Zotero/storage/NE9U5PM5/Fowler and Highsmith - 2001 - The agile manifesto.pdf},
  annote = {32: st{\"a}ndige/iterative Anpassung der Anforderungen und des Designs: "[...] agile processes assume and encourage the alteration of requirements while the code is being written. As such, design cannot be a purely up-front activity to be completed before construction. Instead, design is a continuous activity that's performed throughout the project."}
}

@article{garcia-penalvoOpenKnowledgeManagement2010,
  title = {Open Knowledge Management in Higher Education},
  volume = {34},
  number = {4},
  journal = {Online Information Review},
  author = {{Garc{\'i}a-Penalvo}, Francisco J. and {Garc{\'i}a de Figuerola}, Carlos and Merlo, Jose A.},
  year = {2010},
  pages = {517-519},
  file = {/Users/Konstantin/Zotero/storage/TSKVH6Y2/García-Penalvo Carlos García de Figuerola Jose A. Merlo - 2010 - Open knowledge management in higher education.pdf;/Users/Konstantin/Zotero/storage/B46WJT8U/oir.2010.26434daa.html},
  annote = {518: Offenes Wissen als gemeinsame Grundlage von offener Software und offener Wissenschaft etc.: "We think that Open Knowledge comprises Open Software, Open Content, Open Science and Open Innovation. [...] Open Software owes its deepest roots to Open Access; Open Contents are related to open access to the educative, cultural or divulgative contents that are published under a non restrictive license that allows copy and distribution, but also the right to modify works. Open Science is devoted to the open access to scientific contents, while Open Innovation transfers the Open Access principles to the enterprise production world, which is actually indispensable for the enhancement of University-Enterprises relationships."}
}

@article{hestnessDeepLearningScaling2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.00409},
  primaryClass = {cs, stat},
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  journal = {arXiv:1712.00409 [cs, stat]},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/Konstantin/Zotero/storage/JLUM4XBH/Hestness et al. - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf;/Users/Konstantin/Zotero/storage/5DGISFJH/1712.html},
  annote = {11: kleine Trainingsdaten verlangen kleinere Batch Size (\& Learning Rate?) "For large models with fixed hyperparameters, increasing the batch sizes and learning rates usually closed a significant portion of the gap to the power-law trend. Analogously, smaller training sets often require smaller batch sizes to ensure models behave well while fitting."

13: mehr Trainingsdaten sind IMMER besser: "We empirically validate that DL model accuracy improves as a power-law as we grow training sets for state-of-the-art (SOTA) model architectures in four machine learning domains: machine translation, language modeling, image processing, and speech recognition. These power-law learning curves [exist] across all tested domains, model architectures, optimizers, and loss functions."

Comment: 19 pages, 11 figures}
}

@article{gevaudanSemantischeRelationenNominalen1999,
  title = {Semantische {{Relationen}} in Nominalen Und Adjektivischen {{Kompositionen}} Und {{Syntagmen}}},
  volume = {9},
  journal = {PhiN. Philologie im Netz},
  author = {G{\'e}vaudan, Paul},
  year = {1999},
  pages = {11--34},
  annote = {16: Definition Hyponymie: "Auf der konzeptuellen Ebene, auf der auch Similarit{\"a}t und Kontiguit{\"a}t angesiedelt sind, beruht Hyponymie auf der~\emph{taxonomischen Inklusion}~von Konzepten. Die taxonomische Inklusion ist nichts anderes als eine Ober-/Unterbegriffsbeziehung [...]. Diese Konzeptpaare sind teilidentisch, weil die Extension (die Menge der mit dem Begriff gemeinten Ph{\"a}nomene) des Unterbegriffs in der des Oberbegriffs enthalten ist und gleichzeitig die Intension (die Menge der dem Begriff zugeordneten Eigenschaften) des Oberbegriffs in der des Unterbegriffs enthalten ist [...]."

17: Zusammenspiel von Hyponymie mit anderen semantischen Ph{\"a}nomenen: "(4) Hyponymische Relationen (taxonomische Inklusion)

a. Polysemie: fr.~\emph{homme}~'Mann' / 'Mensch'

b. Bedeutungswandel: lt.~\emph{passer}~'Sperling' {$>$} sp.~\emph{p{\'a}jaro}~'(kleiner) Vogel'

c. Suffigierung: lt.~\emph{artus}~'Gelenk' {$>~$}\emph{articulus} 'kleines Gelenk'"

18: Hyponymie (taxonomische Inklusion) beruht auf Spezifizierung bzw. Generalisierung:}
}

@incollection{anstattTypenSemantischerRelationen2009,
  title = {Typen Semantischer {{Relationen}}},
  booktitle = {Die Slavischen {{Sprachen}}. {{Ein}} Internationales {{Handbuch}} Zu Ihrer {{Geschichte}}, Ihrer {{Struktur}} Und Ihrer {{Erforschung}}},
  author = {Anstatt, Tanja},
  editor = {Berger, T. and Gutschmidt, K. and Kempgen, S. and Kosta, P.},
  year = {2009},
  pages = {906-915},
  file = {/Users/Konstantin/Zotero/storage/UJJVJSM9/Tanja_Anstatt_2009_Typen_semantischer_Relationen.html},
  annote = {913: Suffixe zur Bildung von Hyponymen in slawischen Sprachen: "[...] zur Bildung von Hyponymen kann z.B. das Suffix -ovye/-evye (seld' ,Hering` \textendash{} sel'devye ,Heringsartige`) verwendet werden (s. Ginzburg 1985, 9)."}
}

@book{coenenAnalogieUndMetapher2013,
  title = {{Analogie und Metapher: Grundlegung einer Theorie der bildlichen Rede}},
  isbn = {978-3-11-089463-9},
  shorttitle = {{Analogie und Metapher}},
  abstract = {Von zahllosen anderen Ver{\"o}ffentlichungen zur bildlichen Rede unterscheidet sich das Studienbuch "Analogie und Metapher" in zweifacher Hinsicht: 1. Es verbindet Anregungen der antiken Rhetorik mit Errungenschaften der modernen Sprachwissenschaft und -philosophie, so da{\ss} die Entw{\"u}rfe der Klassiker in ein tragf{\"a}higeres theoretisches Ger{\"u}st eingef{\"u}gt werden k{\"o}nnen. 2. Es f{\"u}hrt die verschiedenen Erscheinungsformen der bildlichen Rede auf eine gemeinsame Tiefenstruktur zur{\"u}ck: die Analogie, verstanden als Geltung eines gemeinsamen Beschreibungsinhalts f{\"u}r zwei verschiedene Gegenst{\"a}nde. Die Differenzierung des Analogiebegriffs - z.B. in ein- und mehrstellige sowie triviale und nicht-triviale Analogien - erweist sich als Schl{\"u}ssel sowohl zum linguistischen Verst{\"a}ndnis des Ph{\"a}nomens der Bildlichkeit wie auch zur Interpretation einzelner sprachlicher Bilder etwa in der Literatur. Insofern ist dieses Studienbuch sowohl f{\"u}r Linguisten als auch f{\"u}r Literaturwissenschaftler von Interesse.},
  language = {de},
  publisher = {{Walter de Gruyter}},
  author = {Coenen, Hans Georg},
  month = feb,
  year = {2013},
  keywords = {Literary Criticism / General,Language Arts \& Disciplines / Linguistics / Historical \& Comparative,Literary Criticism / European / German,Literary Criticism / Semiotics \& Theory},
  annote = {46: Unterscheidung von theoretischer und empirischer Bedeutung bzw. Anwendungsbereich eines Wortes: "Theoretischer Anwendungsbereich eines Wortes: die Klasse der Gegenst{\"a}nde, auf die ein Wort kraft seiner Bedeutung beschreibend angewandt werden kann (TA). Empirischer Anwendungsbereich eines Wortes: die Menge der Gegenst{\"a}nde, zu deren Beschreibung ein Wort - bis zu einem gewissen Zeitpunkt - in der Erfahrung eines bestimmten SprachteiInehmers bereits angewandt wurde (EA)."

58: Hyponymie l{\"a}sst sich hierarchisch {\"u}bertragen, ist also transitiv: "Die Relationen der Hyper- und Hyponymie sind transitiv. Wenn A Hyponym in Bezug auf B ist, dann auch in Bezug auf die Hyperonyme von B, und wenn B Hyperonym in Bezug auf A ist, dann auch in Bezug auf die Hyponyme von A."}
}

@incollection{ponsborderiaPathsGrammaticalizationSpanish2014,
  title = {Paths of Grammaticalization in {{Spanish}} o Sea},
  booktitle = {Discourse and {{Pragmatic Markers}} from {{Latin}} to the {{Romance Languages}}},
  author = {Pons Border{\'i}a, Salvador},
  editor = {Ghezzi, Chiara and Molinelli, Piera},
  year = {2014},
  pages = {109--136},
  annote = {126: eindeutige Unterscheidung zwischen Hyponymie und Kohyponymie nicht immer m{\"o}glich: "(29) Y en ese convento nos regalaron diversas veces con tortillas de huevos [ ... ]. Un d{\'i}a, o sea una tarde, salimos de dicho convento de San Diego, adonde hab{\'i}amos merendado muy bien de dichas tortillas (1705, Raimundo de Lantery, Memorias)\\
'In that convent we were given egg omelettes several times [ ... ] One day, or be-SBJV one afternoon, we went out of this convent of San Diego, where we\\
had a tasty snack of such omelettes'\\
In (29), \emph{o sea} links two words, which can be interpreted as either hyperonym {$>$} hyponym (day {$>$} afternoon), or exclusive co-hyponyms (morning V afternoon)."}
}

@phdthesis{souille-rigautSemanticAccountQuasiLexemes2010,
  type = {{{PhD Thesis}}},
  title = {A {{Semantic Account}} of {{Quasi}}-{{Lexemes}} in {{Modern English}}-{{Processing Semiotic Units}} of {{Greek}} or {{Latin Origin}} into {{Lexical Units}}},
  school = {University of Kansas},
  author = {{Souill{\'e}-Rigaut}, Chris},
  year = {2010},
  file = {/Users/Konstantin/Zotero/storage/WQSJT2H9/Souillé-Rigaut - 2010 - A Semantic Account of Quasi-Lexemes in Modern Engl.pdf;/Users/Konstantin/Zotero/storage/WJSA5ZGD/7406.html},
  annote = {24-33: semantische Relationen sind aus der Morphologie herleitbar, selbst wenn ein Wort als Ganzes nicht paraphrasiert werden kann: "Because quasi-lexemes are protolexical elements, rather than independent lexemes, I would argue that they are never paraphrasable. I may say that a toothache is when a tooth aches, but I may not say that an odontalgia is when an -odont- -alg-. [...] The head constituent in both compounds is on the right, the modifier constituent on the left, and, although `cardiopathy' is not paraphrasable, we can still deduce from its semiotic units that the compound as a whole is a hyponym of the meaning carried by the semiotic unit -path-."

43: Komposition erm{\"o}glicht Hyponymie bzw. macht sie morphologisch sichtbar: "`Dontopedalogy is the science of opening your mouth and putting your foot in it, a science which I have practiced for a good many years'. There is evidence that we are dealing with a secondary compound of the type W + X + Y, that is to say, with the semiotic units -dont- + -ped- + -log- being concatenated synchronically. [...] secondary compounds W + XY are always expanded primary compounds in which WXY is a hyponym of XY. The compound XY itself may be a primary or a synthetic compound, but stands necessarily as a nominal constituent."}
}

@incollection{minozziLatinWordNetProject2010,
  address = {{Innsbruck}},
  series = {Innsbrucker {{Beitr{\"a}ge}} Zur {{Sprachwissenschaft}}},
  title = {The {{Latin WordNet Project}}},
  volume = {137},
  isbn = {978-3-85124-723-7},
  booktitle = {Latin Linguistics Today: {{Akten}} Des 15. {{Internationalen Kolloquiums}} Zur {{Lateinischen Linguistik}}, {{Innsbruck}}, 4.- 9. {{April}} 2009},
  publisher = {{Institut f{\"u}r Sprachen und Literaturen der Universit{\"a}t Innsbruck, Bereich Sprachwissenschaft}},
  author = {Minozzi, S.},
  editor = {Anreiter, Peter and Kienpointner, M.},
  year = {2010},
  pages = {707-716},
  file = {/Users/Konstantin/Zotero/storage/M7U8QZFR/latin-linguistics-today-akten-des-15-internationalen-kolloquiums-zur-lateinischen-linguistik.html}
}

@incollection{pinkalSemantik1993,
  edition = {1},
  title = {Semantik},
  booktitle = {Einf{\"u}hrung in Die K{\"u}nstliche {{Intelligenz}}},
  author = {Pinkal, Manfred},
  editor = {G{\"o}rz, G{\"u}nther},
  year = {1993},
  pages = {425-498},
  annote = {427f.: semantische Konstruktion vs. Resolution vs. Auswertung einer Aussage:

"[...] die Semantikkonstruktion [...], die das semantische Potential auf der Grundlage der lexikalischen und syntaktischen Information ermittelt, die in der Eingabekette enthalten ist; [...] die semantische Resolution, die den aktuellen semantischen Wert bestimmt, unter anderem durch die Aufl{\"o}sung von Mehrdeutigkeiten (Disambiguierung) und [...] die semantische Auswertung, die durch die Anwendung von Deduktions- und Inferenzmechanismen auf den semantischen Wert einer {\"A}u{\ss}erung die relevante {\"A}u{\ss}erungsinformation extrahiert und dabei unter anderem Weltwissen (episodisches Wissen und Regelwissen) einbezieht."}
}

@article{faruquiProblemsEvaluationWord2016,
  title = {Problems with Evaluation of Word Embeddings Using Word Similarity Tasks},
  journal = {arXiv preprint arXiv:1605.02276},
  author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
  year = {2016},
  file = {/Users/Konstantin/Zotero/storage/KY2HU9J4/Faruqui et al. - 2016 - Problems with evaluation of word embeddings using .pdf;/Users/Konstantin/Zotero/storage/LNR5YHXT/1605.html},
  annote = {4: Evaluation von Embeddings beruht oft auf Reduzierung der Wortbedeutungen auf eine einzige zentrale Bedeutung pro Wort: "However in [the dataset] WS-353, bank is given a similarity score of 8.5/10 to money, signifying that bank is a financial institution. Such an assumption of one sense per word is prevalent in many of the existing word similarity tasks, and it can incorrectly penalize a word vector model for capturing a specific sense of the word absent in the word similarity task."}
}

@article{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  journal = {arXiv preprint arXiv:1802.05365},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  file = {/Users/Konstantin/Zotero/storage/AL3ZNP2P/Peters et al. - 2018 - Deep contextualized word representations.pdf;/Users/Konstantin/Zotero/storage/UL78NLC2/1802.html},
  annote = {2f.: ELMo ber{\"u}cksichtigt Wortkontext f{\"u}r die Berechnung der Vektoren, indem es die Token-Abfolge vorw{\"a}rts und r{\"u}ckw{\"a}rts modelliert: "Given a sequence of N tokens, (t1, t2, ..., tN), a forward language model computes the probability of the sequence by modeling the probability of token tk given the history (t1, ..., tk-1) [...]. A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context [...]. A biLM combines both a forward and backward LM. Our formulation jointly maximizes the log likelihood of the forward and backward directions [...]."}
}

@article{devlinBertPretrainingDeep2018,
  title = {Bert: {{Pre}}-Training of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  journal = {arXiv preprint arXiv:1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  file = {/Users/Konstantin/Zotero/storage/QIH93SR6/Devlin et al. - 2018 - Bert Pre-training of deep bidirectional transform.pdf;/Users/Konstantin/Zotero/storage/Q5R6ARG5/1810.html},
  annote = {8: Gr{\"u}nde f{\"u}r die {\"U}berlegenheit von tief bidirektionalen Modellen wie BERT gegen{\"u}ber einfachen LSTMs oder losen Verkettungen zweier LSTMs im Sinne eines oberfl{\"a}chlich bidirektionalen Modells (z.B: ELMo): "(b) [Concatenating separate Left-To-Right and Right-To-Left models] is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) [this] is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer."}
}

@article{roelliCorpusCorporumNew2014,
  title = {The {{Corpus Corporum}}, a New Open {{Latin}} Text Repository and Tool},
  journal = {Archivum Latinitatis Medii Aevi-Bulletin du Cange (ALMA)},
  author = {Roelli, Philipp},
  year = {2014},
  file = {/Users/Konstantin/Zotero/storage/8QP8LVWQ/Roelli - 2014 - The Corpus Corporum, a new open Latin text reposit.pdf;/Users/Konstantin/Zotero/storage/TMFM3VHH/69525.html}
}

@article{munznerNeueWegeIm2013,
  title = {Neue {{Wege}} Im {{Lateinunterricht}}? {{Zur}} Praktischen {{Umsetzung}} Der {{Kompetenzorientierung}} Am {{Beispiel}} Der {{Metamorphosen Ovids}}},
  shorttitle = {Neue {{Wege}} Im {{Lateinunterricht}}?},
  journal = {Pegasus-Onlinezeitschrift},
  author = {M{\"u}nzner, Iris},
  year = {2013},
  pages = {218--273},
  file = {/Users/Konstantin/Zotero/storage/R6V4G366/Münzner - 2013 - Neue Wege im Lateinunterricht Zur praktischen Ums.pdf},
  annote = {243 Anm. 89: Diskussion {\"u}ber lautes Vorlesen im Lateinunterricht: "Gl{\"u}cklich / von Albrecht erg{\"a}nzen jedoch auch, dass das Lesen eigentlich erst nach dem {\"U}bersetzen und Interpretieren des lateinischen Textes sinnvoll sei, da erst dann Rhythmus, Musikalit{\"a}t und die inhaltliche Sinnhaftigkeit des Textes ber{\"u}cksichtigt werden k{\"o}nnten [...]. Diese Warnung wird nicht v{\"o}llig zu Unrecht ausgesprochen. Allerdings kann die Skandierung der Metrik auch als sinnvolle Textvorerschlie{\ss}ung gew{\"a}hlt werden und auf bestimmte Ph{\"a}nomene des Textes vor der {\"U}bersetzung hinweisen."}
}

@article{taslimExperimentalStudyTeaching2014,
  title = {An {{Experimental Study}} of {{Teaching Vocabulary}} by {{Using Hyponymy Games}} on the {{Seventh Grader F MTs Syech Ibrahim Payakumbuh}}},
  volume = {21},
  number = {3},
  journal = {Al-Ta lim Journal},
  author = {Taslim, Fadilla},
  year = {2014},
  pages = {189--197},
  file = {/Users/Konstantin/Zotero/storage/SHBTLPL7/Taslim - 2014 - An Experimental Study of Teaching Vocabulary by Us.pdf},
  annote = {196: hyponymie-basierte Spiele tragen zum Wortschatzerwerb bei: "[...] Hyponymy games are highly effective in developing students' level of vocabulary [...]."}
}

@inproceedings{ayseExtractionSemanticWord2011,
  address = {{Portland, Oregon, USA}},
  title = {Extraction of {{Semantic Word Relations}} in {{Turkish}} from {{Dictionary Definitions}}},
  booktitle = {Proceedings of the {{ACL}} 2011 {{Workshop}} on {{Relational Models}} of {{Semantics}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Ay{\c s}e, {\c S}erbet{\c c}i and Zeynep, Orhan and {\.I}lknur, Pehlivan},
  month = jun,
  year = {2011},
  pages = {11--18},
  file = {/Users/Konstantin/Zotero/storage/HKR3KEGH/Ayşe et al. - 2011 - Extraction of Semantic Word Relations in Turkish f.pdf},
  annote = {12: semantische Relationen m{\"u}ssen immer je nach Wortbedeutung differenziert werden: "For more accurate semantic analysis, the connection between words should be established between appropriate senses of the words. To be more concrete, an example can be given on the semantically ambiguous word as; y{\"u}z `face' or `hundred'. When a has-a relation is detected between the words v{\"u}cut `body' and y{\"u}z, the appropriate sense for y{\"u}z should be selected as `face', instead of `hundred'."}
}

@article{griesBehavioralProfilesCorpusbased2009,
  title = {Behavioral Profiles: A Corpus-Based Approach to Cognitive Semantic Analysis},
  shorttitle = {Behavioral Profiles},
  journal = {New directions in cognitive linguistics},
  author = {Gries, Stefan Th and Divjak, Dagmar},
  year = {2009},
  pages = {57--75},
  file = {/Users/Konstantin/Zotero/storage/JBN4ZAXB/books.html},
  annote = {59: {\"a}hnliche Wortverteilung weist auf {\"a}hnliche Semantik hin: "[...] distributional similarity reflects, or is indicative of, functional similarity, the understanding of functional similarity being rather broad, i.e. encompassing semantic, discourse-pragmatic, and other functions a particular expression can take on."

60: Frequenz, Authentizit{\"a}t, Variation und systematische Induktion als Vorteile von Korpora gegen{\"u}ber Introspektion/Intuition: "[In contrast to introspection,] corpora a. provide many instances rather than a few isolated judgments; b. provide data from natural settings rather than 'armchair' judgments or responses that potentially reflect experimentally-induced biases; c. provide co-occurrence data of many different kinds, i.e. not just those a particular researcher may consider important; d. and thus, allow for bottom-up identification of relevant distinctions as well as for a more comprehensive description than is typically provided."}
}

@inproceedings{carlsonArchitectureNeverendingLanguage2010,
  title = {Toward an Architecture for Never-Ending Language Learning},
  booktitle = {Twenty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R. and Mitchell, Tom M.},
  year = {2010},
  pages = {1306-1313},
  file = {/Users/Konstantin/Zotero/storage/CMRWR4KL/Carlson et al. - 2010 - Toward an architecture for never-ending language l.pdf;/Users/Konstantin/Zotero/storage/PXP2U9WG/1879.html},
  annote = {1306: semantische Relationen sind auch im Bereich der k{\"u}nstlichen Intelligenz f{\"u}r den Spracherwerb unerl{\"a}sslich: "At present, [Never-Ending Language Learner] acquires two types of knowledge: (1) knowledge about which noun phrases refer to which specified semantic categories, such as cities, companies, and sports teams, and (2) knowledge about which pairs of noun phrases satisfy which specified semantic relations, such as hasOfficesIn(organization, location)."}
}

@article{punuruLearningNontaxonomicalSemantic2012,
  title = {Learning Non-Taxonomical Semantic Relations from Domain Texts},
  volume = {38},
  issn = {1573-7675},
  abstract = {Ontology of a domain mainly consists of concepts, taxonomical (hierarchical) relations and non-taxonomical relations. Automatic ontology construction requires methods for extracting both taxonomical and non-taxonomical relations. Compared to extensive works on concept extraction and taxonomical relation learning, little attention has been given on identification and labeling of non-taxonomical relations in text mining. In this paper, we propose an unsupervised technique for extracting non-taxonomical relations from domain texts. We propose the VF*ICF metric for measuring the importance of a verb as a representative relation label, in much the same spirit as the TF*IDF measure in information retrieval. Domain-relevant concepts (nouns) are extracted using techniques developed earlier. Candidate non-taxonomical relations are generated as (SVO) triples of the form (subject, verb, object) from domain texts. A statistical method with log-likelihood ratios is used to estimate the significance of relationships between concepts and to select suitable relation labels. Texts from two domains, the Electronic Voting (EV) domain texts and the Tenders and Mergers (TNM) domain texts are used to compare our method with one of the existing approaches. Experiments showed that our method achieved better performance in both domains.},
  language = {en},
  number = {1},
  journal = {Journal of Intelligent Information Systems},
  doi = {10.1007/s10844-011-0149-4},
  author = {Punuru, Janardhana and Chen, Jianhua},
  month = feb,
  year = {2012},
  keywords = {Ontology learning,Semantical relation,Text mining},
  pages = {191-207},
  file = {/Users/Konstantin/Zotero/storage/RUML34M4/Punuru and Chen - 2012 - Learning non-taxonomical semantic relations from d.pdf},
  annote = {192f.: automatischer Aufbau von Ontologien f{\"u}r das Semantic Web hilft bei der Erschlie{\ss}ung dom{\"a}nenspezifischer Texte und erh{\"o}ht die Qualit{\"a}t maschineller Suchverfahren: "Semantic Web aims at representing the textual content of Web pages by ontologies, which allow machines to comprehend the semantics of documents and return more accurate answers to user queries. As of now, ontologies for domains of interests are developed manually in spite of the wide spread use of ontologies. This situation clearly indicates the urgent need for techniques that can automatically learn (construct) ontology from domain texts."}
}

@article{crossleyDevelopmentSemanticRelations2010,
  title = {The Development of Semantic Relations in Second Language Speakers: {{A}} Case for Latent Semantic Analysis.},
  volume = {7},
  shorttitle = {The Development of Semantic Relations in Second Language Speakers},
  journal = {Vigo International Journal of Applied Linguistics},
  author = {Crossley, Scott A. and Salsbury, Tom and McNamara, Danielle S.},
  year = {2010},
  pages = {55-74},
  file = {/Users/Konstantin/Zotero/storage/37ZW5ST9/Crossley et al. - 2010 - The development of semantic relations in second la.pdf},
  annote = {56: Auswendiglernen hilft nur bei hochfrequenten Vokabeln im anf{\"a}nglichen Spracherwerb, f{\"u}r Fortgeschrittene sind semantische Relationen wichtiger: "While it may be true that explicit vocabulary instruction concentrating on the first 2,000 to 3,000 words is valuable for the beginning learner [...], it is generally agreed that subsequent vocabulary acquisition results from inference strategies and the development of word connections [...]."

70: Fortgeschrittene Sprachlernende entwickeln immer engere semantische Verkn{\"u}pfungen zwischen einzelnen Sprachsegmenten: "[...] L2 learners begin to develop closer semantic similarities between speech segments as they progress in acquiring a second language."

71: Wortschatzerwerb ist zu komplex, um ohne maschinelle Hilfe erforscht zu werden: "[...] since lexical acquisition is a phenomenon that is generally too complex to be analyzed based on human intuition, computational approaches are beneficial."}
}

@inproceedings{schibelZurAneignungLateinischer2013,
  title = {Zur {{Aneignung}} Lateinischer {{Literatur}} Und {{Sprache}}},
  booktitle = {Forum {{Classicum}}},
  author = {Schibel, Wolfgang},
  year = {2013},
  pages = {113--124},
  file = {/Users/Konstantin/Zotero/storage/C4K8VU6U/Schibel - 2013 - Zur Aneignung lateinischer Literatur und Sprache.pdf},
  annote = {115: Lekt{\"u}reschock ist bedingt durch schlechtes Design der Lehrb{\"u}cher, z.B. zu wenig Progression: "Die Antwort gibt die von Lateinlehrern oft zu h{\"o}rende Klage {\"u}ber einen ,Lekt{\"u}reschock`, den Sch{\"u}ler beim {\"U}bergang vom Lehrbuch zu Originaltexten erleiden. Die wohlmeinende Sorge der Verfasser lateinischer Lehrwerke f{\"u}r einen allm{\"a}hlichen, von Lektion zu Lektion sparsam dosierten Anstieg der sprachlichen Anforderungen zeitigt einen langen Aufenthalt der Lerner in einer fabrizierten Sprachwelt, die nicht zur {\"U}berwindung sprachlicher H{\"u}rden anspornt."}
}

@inproceedings{boledaIntensionalityWasOnly2013,
  title = {Intensionality Was Only Alleged: {{On}} Adjective-Noun Composition in Distributional Semantics},
  shorttitle = {Intensionality Was Only Alleged},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Computational Semantics}} ({{IWCS}} 2013): Long Papers; 2013 {{Mar}} 20-22; {{Postdam}}, {{Germany}}. {{Stroudsburg}} ({{USA}}): {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  publisher = {{ACL (Association for Computational Linguistics)}},
  author = {Boleda, Gemma and Baroni, Marco and McNally, Louise},
  year = {2013},
  pages = {35-46},
  file = {/Users/Konstantin/Zotero/storage/TZL9TG8J/Boleda et al. - 2013 - Intensionality was only alleged On adjective-noun.pdf;/Users/Konstantin/Zotero/storage/NX339DFP/37126.html},
  annote = {42: Modifizierende Adjektive k{\"o}nnen die Polysemie von Substantiven deutlich eingrenzen und dadurch zur Disambiguierung der Wortbedeutung wesentlich beitragen: "We find that, the more polysemous a noun is, the less similar its vector is to the corresponding phrase vector. It is plausible that modifying a noun has a larger impact when the noun is polysemous, as the adjective narrows down the meaning of the noun; indeed, adjectives have been independently shown to be powerful word sense disambiguators of nouns [...]. In distributional terms, the adjective notably ``shifts'' the vector of polysemous nouns, but for monosemous nouns there is just not much shifting room."}
}

@article{hamiltonDiachronicWordEmbeddings2016,
  title = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  journal = {arXiv preprint arXiv:1605.09096},
  author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  year = {2016},
  file = {/Users/Konstantin/Zotero/storage/HKULL62R/Hamilton et al. - 2016 - Diachronic word embeddings reveal statistical laws.pdf;/Users/Konstantin/Zotero/storage/Z5L9AGG8/1605.html},
  annote = {8: Kollokationsnetzwerke als n{\"a}herungsweise Quantifizierung von Polysemie: "We construct empirical co-occurrence networks for the top-10,000 non-stop words of each language using the PPMI measure [...]. In these networks words are connected to each other if they co-occur more than one would expect by chance (after smoothing). The polysemy of a word is then measured as its local clustering coefficient within this network [...]."}
}

@article{karanDistributionalSemanticsApproach2012,
  title = {Distributional Semantics Approach to Detecting Synonyms in {{Croatian}} Language},
  journal = {Information Society},
  author = {Karan, Mladen and {\v S}najder, Jan and Ba{\v s}ic, Bojana Dalbelo},
  year = {2012},
  pages = {111--116},
  file = {/Users/Konstantin/Zotero/storage/FIGU2ATQ/Karan et al. - 2012 - Distributional semantics approach to detecting syn.pdf},
  annote = {114: einheitliche Vektoren pro Lemma sind bei starker Polysemie problematisch: "Distributional representations of each sense of a polysemous word get merged into a single distributional representation \textendash{} a mixture of distributions. For questions with high polysemy level, the corresponding distributional vectors are blurred and the similarity comparisons between such vectors are less meaningful."

115: Unterscheidung zwischen Synonymie und semantischer Verwandtschaft ist f{\"u}r distributionelle Semantik problematisch: "However, manual inspection revealed that most false synonyms are syntagmatically related to the target word. To avoid this kind of error, we would need a method to distinguish between synonymy and general semantic relatedness."}
}

@article{fellbaumChallengesMultilingualWordnet2012,
  title = {Challenges for a Multilingual Wordnet},
  volume = {46},
  issn = {1574-0218},
  abstract = {Wordnets have been created in many languages, revealing both their lexical commonalities and diversity. The next challenge is to make multilingual wordnets fully interoperable. The EuroWordNet experience revealed the shortcomings of an interlingua based on a natural language. Instead, we propose a model based on the division of the lexicon and a language-independent, formal ontology that serves as the hub interlinking the language-specific lexicons. The ontology avoids the idiosyncracies of the lexicon and furthermore allows formal reasoning about the concepts it contains. We address the division of labor between ontology and lexicon. Finally, we illustrate our model in the context of a domain-specific multilingual information system based on a central ontology and interconnected wordnets in seven languages.},
  language = {en},
  number = {2},
  journal = {Language Resources and Evaluation},
  doi = {10.1007/s10579-012-9186-z},
  author = {Fellbaum, Christiane and Vossen, Piek},
  month = jun,
  year = {2012},
  keywords = {Formal ontology,Information system,Multilingual wordnets},
  pages = {313-326},
  file = {/Users/Konstantin/Zotero/storage/59NZN9HK/Fellbaum and Vossen - 2012 - Challenges for a multilingual wordnet.pdf},
  annote = {315: WordNet als Modell des menschlichen Ged{\"a}chtnisses, v.a. in Bezug auf Semantik: "WordNet's original motivation was to test the feasibility of a model of human semantic memory that sought to explain principles of storage and retrieval of words and concepts."}
}


