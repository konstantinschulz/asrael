@article{adiFinegrainedAnalysisSentence2016,
  title = {Fine-Grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks},
  author = {Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
  year = {2016},
  journal = {arXiv preprint arXiv:1608.04207},
  eprint = {1608.04207},
  archiveprefix = {arxiv}
}

@inproceedings{akbikContextualStringEmbeddings2018,
  title = {Contextual String Embeddings for Sequence Labeling},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  year = {2018},
  pages = {1638--1649}
}

@article{amramiBetterSubstitutionbasedWord2019,
  title = {Towards Better Substitution-Based Word Sense Induction},
  author = {Amrami, Asaf and Goldberg, Yoav},
  year = {2019},
  month = may,
  journal = {arXiv:1905.12598 [cs]},
  eprint = {1905.12598},
  primaryclass = {cs},
  urldate = {2020-08-28},
  abstract = {Word sense induction (WSI) is the task of unsupervised clustering of word usages within a sentence to distinguish senses. Recent work obtain strong results by clustering lexical substitutes derived from pre-trained RNN language models (ELMo). Adapting the method to BERT improves the scores even further. We extend the previous method to support a dynamic rather than a fixed number of clusters as supported by other prominent methods, and propose a method for interpreting the resulting clusters by associating them with their most informative substitutes. We then perform extensive error analysis revealing the remaining sources of errors in the WSI task. Our code is available at https://github.com/asafamr/bertwsi.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/konstantin/Zotero/storage/VJPA6TZF/1905.html}
}

@incollection{anstattTypenSemantischerRelationen2009,
  title = {Typen Semantischer {{Relationen}}},
  booktitle = {Die Slavischen {{Sprachen}}. {{Ein}} Internationales {{Handbuch}} Zu Ihrer {{Geschichte}}, Ihrer {{Struktur}} Und Ihrer {{Erforschung}}},
  author = {Anstatt, Tanja},
  editor = {Berger, T. and Gutschmidt, K. and Kempgen, S. and Kosta, P.},
  year = {2009},
  pages = {906--915},
  urldate = {2019-07-31},
  file = {/home/konstantin/Zotero/storage/UJJVJSM9/Tanja_Anstatt_2009_Typen_semantischer_Relationen.html}
}

@inproceedings{ayseExtractionSemanticWord2011,
  title = {Extraction of {{Semantic Word Relations}} in {{Turkish}} from {{Dictionary Definitions}}},
  booktitle = {Proceedings of the {{ACL}} 2011 {{Workshop}} on {{Relational Models}} of {{Semantics}}},
  author = {Ay{\c s}e, {\c S}erbet{\c c}i and Zeynep, Orhan and {\.I}lknur, Pehlivan},
  year = {2011},
  month = jun,
  pages = {11--18},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}},
  urldate = {2019-10-21}
}

@article{baidakKeyDataTeaching2017,
  title = {Key {{Data}} on {{Teaching Languages}} at {{School}} in {{Europe}}. 2017 {{Edition}}. {{Eurydice Report}}.},
  author = {Ba{\"i}dak, Nathalie and Balcon, Marie-Pascale and Motiejunaite, Akvile},
  year = {2017},
  journal = {Education, Audiovisual and Culture Executive Agency, European Commission},
  publisher = {{ERIC}},
  issn = {9294924823}
}

@inproceedings{bakerWordNetFrameNetComplementary2009,
  title = {{{WordNet}} and {{FrameNet}} as {{Complementary Resources}} for {{Annotation}}},
  booktitle = {Proceedings of the {{Third Linguistic Annotation Workshop}} ({{LAW III}})},
  author = {Baker, Collin F. and Fellbaum, Christiane},
  year = {2009},
  month = aug,
  pages = {125--129},
  publisher = {{Association for Computational Linguistics}},
  address = {{Suntec, Singapore}},
  urldate = {2020-08-25}
}

@inproceedings{bartunovBreakingSticksAmbiguities2016,
  title = {Breaking Sticks and Ambiguities with Adaptive Skip-Gram},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  year = {2016},
  pages = {130--138},
  keywords = {reference}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  journal = {Journal of machine learning research},
  volume = {3},
  number = {Feb},
  pages = {1137--1155}
}

@inproceedings{boledaIntensionalityWasOnly2013,
  title = {Intensionality Was Only Alleged: {{On}} Adjective-Noun Composition in Distributional Semantics},
  shorttitle = {Intensionality Was Only Alleged},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Computational Semantics}} ({{IWCS}} 2013): Long Papers; 2013 {{Mar}} 20-22; {{Postdam}}, {{Germany}}. {{Stroudsburg}} ({{USA}}): {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Boleda, Gemma and Baroni, Marco and McNally, Louise},
  year = {2013},
  pages = {35--46},
  publisher = {{ACL (Association for Computational Linguistics)}},
  file = {/home/konstantin/Zotero/storage/NX339DFP/37126.html}
}

@article{boneApplyingMachineLearning2015,
  title = {Applying Machine Learning to Facilitate Autism Diagnostics: Pitfalls and Promises},
  author = {Bone, Daniel and Goodwin, Matthew S and Black, Matthew P and Lee, Chi-Chun and Audhkhasi, Kartik and Narayanan, Shrikanth},
  year = {2015},
  journal = {Journal of autism and developmental disorders},
  volume = {45},
  number = {5},
  pages = {1121--1136},
  publisher = {{Springer}},
  issn = {0162-3257}
}

@inproceedings{borgesvolkerHDTUDVeryLarge2019,
  title = {{{HDT-UD}}: {{A}} Very Large {{Universal Dependencies Treebank}} for {{German}}},
  shorttitle = {{{HDT-UD}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Universal Dependencies}} ({{UDW}}, {{SyntaxFest}} 2019)},
  author = {Borges V{\"o}lker, Emanuel and Wendt, Maximilian and Hennig, Felix and K{\"o}hn, Arne},
  year = {2019},
  month = aug,
  pages = {46--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Paris, France}},
  doi = {10.18653/v1/W19-8006},
  urldate = {2020-09-07}
}

@article{brownAlgorithmsChemoinformatics2011,
  title = {Algorithms for Chemoinformatics},
  author = {Brown, Nathan},
  year = {2011},
  journal = {WIREs Computational Molecular Science},
  volume = {1},
  number = {5},
  pages = {716--726},
  issn = {1759-0884},
  doi = {10.1002/wcms.42},
  urldate = {2020-12-11},
  abstract = {The field of chemoinformatics makes use of a great number of different computational algorithms from mathematics and computer science as well as algorithms that are defined explicitly for challenges specific to chemoinformatics. This article provides an overview of algorithms and algorithmic complexity together with a review of common algorithmic design paradigms. These approaches are then given context with a set of relevant examples from chemoinformatics with definitions of the algorithms and, where relevant, graphical explanations. \textcopyright{} 2011 John Wiley \& Sons, Ltd. WIREs Comput Mol Sci 2011 1 716-726 DOI: 10.1002/wcms.42 This article is categorized under: Computer and Information Science {$>$} Chemoinformatics},
  langid = {english},
  file = {/home/konstantin/Zotero/storage/2JG2HPZ4/wcms.html}
}

@inproceedings{carlsonArchitectureNeverendingLanguage2010,
  title = {Toward an Architecture for Never-Ending Language Learning},
  booktitle = {Twenty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R. and Mitchell, Tom M.},
  year = {2010},
  pages = {1306--1313},
  file = {/home/konstantin/Zotero/storage/PXP2U9WG/1879.html}
}

@book{coenenAnalogieUndMetapher2013,
  title = {{Analogie und Metapher: Grundlegung einer Theorie der bildlichen Rede}},
  shorttitle = {{Analogie und Metapher}},
  author = {Coenen, Hans Georg},
  year = {2013},
  month = feb,
  publisher = {{Walter de Gruyter}},
  abstract = {Von zahllosen anderen Ver\"offentlichungen zur bildlichen Rede unterscheidet sich das Studienbuch "Analogie und Metapher" in zweifacher Hinsicht: 1. Es verbindet Anregungen der antiken Rhetorik mit Errungenschaften der modernen Sprachwissenschaft und -philosophie, so da\ss{} die Entw\"urfe der Klassiker in ein tragf\"ahigeres theoretisches Ger\"ust eingef\"ugt werden k\"onnen. 2. Es f\"uhrt die verschiedenen Erscheinungsformen der bildlichen Rede auf eine gemeinsame Tiefenstruktur zur\"uck: die Analogie, verstanden als Geltung eines gemeinsamen Beschreibungsinhalts f\"ur zwei verschiedene Gegenst\"ande. Die Differenzierung des Analogiebegriffs - z.B. in ein- und mehrstellige sowie triviale und nicht-triviale Analogien - erweist sich als Schl\"ussel sowohl zum linguistischen Verst\"andnis des Ph\"anomens der Bildlichkeit wie auch zur Interpretation einzelner sprachlicher Bilder etwa in der Literatur. Insofern ist dieses Studienbuch sowohl f\"ur Linguisten als auch f\"ur Literaturwissenschaftler von Interesse.},
  googlebooks = {HfUgAAAAQBAJ},
  isbn = {978-3-11-089463-9},
  langid = {ngerman},
  keywords = {Language Arts \& Disciplines / Linguistics / Historical \& Comparative,Literary Criticism / European / German,Literary Criticism / General,Literary Criticism / Semiotics \& Theory}
}

@article{conneauCrosslingualLanguageModel2019,
  title = {Cross-Lingual Language Model Pretraining},
  author = {Conneau, Alexis and Lample, Guillaume},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  pages = {7059--7069},
  file = {/home/konstantin/Zotero/storage/3JC5XWV3/Lample and Conneau - 2019 - Cross-lingual Language Model Pretraining.pdf}
}

@article{crossleyDevelopmentPolysemyFrequency2010,
  title = {The {{Development}} of {{Polysemy}} and {{Frequency Use}} in {{English Second Language Speakers}}},
  author = {Crossley, Scott and Salsbury, Tom and McNamara, Danielle},
  year = {2010},
  journal = {Language Learning},
  volume = {60},
  number = {3},
  pages = {573--605},
  issn = {1467-9922},
  doi = {10.1111/j.1467-9922.2010.00568.x},
  urldate = {2020-11-19},
  abstract = {Spoken language data were collected from six adult second language (L2) English learners over a year-long period in order to explore the development of word polysemy and frequency use. The data were analyzed both quantitatively and qualitatively. In the first analysis, the growth of WordNet polysemy values and CELEX word frequency values were examined. For both indexes, significant growth was demonstrated from the 2nd to the 16th week of observation, after which values remained stable. Growth in word polysemy values also correlated with changes in word frequency, supporting the notion that frequency and polysemy effects in word use are related. A second analysis used the WordNet dictionary to explore qualitative changes in word sense use concerning six frequent lexical items in the learner corpus (think, know, place, work, play, and name). A qualitative analysis compared normalized frequencies for each word sense in the first trimester of the study to the later trimesters. Differences in the number of word senses used across trimesters were found for all six words. Analyses 1 and 2, taken together, support the notion that L2 learners begin to use words that have the potential for more senses during the first 4 months; learners then begin to extend the core meanings of these polysemous words. These findings provide further insights into the development of lexical proficiency in L2 learners and the growth of lexical networks.},
  langid = {english},
  keywords = {computational linguistics,corpus linguistics,lexical networks,lexical proficiency,polysemy,word frequency},
  file = {/home/konstantin/Zotero/storage/Y8V9P6G7/j.1467-9922.2010.00568.html}
}

@article{crossleyDevelopmentSemanticRelations2010,
  title = {The Development of Semantic Relations in Second Language Speakers: {{A}} Case for Latent Semantic Analysis},
  shorttitle = {The Development of Semantic Relations in Second Language Speakers},
  author = {Crossley, Scott A. and Salsbury, Tom and McNamara, Danielle S.},
  year = {2010},
  journal = {Vigo International Journal of Applied Linguistics},
  volume = {7},
  pages = {55--74}
}

@article{daiDocumentEmbeddingParagraph2015,
  title = {Document {{Embedding}} with {{Paragraph Vectors}}},
  author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
  year = {2015},
  month = jul,
  journal = {arXiv:1507.07998 [cs]},
  eprint = {1507.07998},
  primaryclass = {cs},
  urldate = {2020-06-18},
  abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/konstantin/Zotero/storage/F2THWCYK/1507.html}
}

@article{debowskiNaturalLanguagePerigraphic2018,
  title = {Is Natural Language a Perigraphic Process? {{The}} Theorem about Facts and Words Revisited},
  author = {D{\k{e}}bowski, {\L}ukasz},
  year = {2018},
  journal = {Entropy},
  volume = {20},
  number = {2},
  pages = {85},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e20020085}
}

@book{devineLatinWordOrder2006,
  title = {Latin Word Order: {{Structured}} Meaning and Information},
  author = {Devine, Andrew M. and Stephens, Laurence D.},
  year = {2006},
  publisher = {{Oxford University Press}},
  isbn = {0-19-972050-9}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  urldate = {2020-11-25},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{divjakCorpusbasedCognitiveSemantics2009,
  title = {Corpus-Based Cognitive Semantics: {{A}} Contrastive Study of Phasal Verbs in {{English}} and {{Russian}}},
  shorttitle = {Corpus-Based Cognitive Semantics},
  author = {Divjak, Dagmar and Gries, Stefan Th},
  year = {2009},
  journal = {Studies in cognitive corpus linguistics},
  pages = {273--296}
}

@article{ellisUsagebasedLanguageInvestigating2013,
  title = {Usage-Based Language: {{Investigating}} the Latent Structures That Underpin Acquisition},
  shorttitle = {Usage-Based Language},
  author = {Ellis, Nick C. and O'Donnell, Matthew Brook and R{\"o}mer, Ute},
  year = {2013},
  journal = {Language Learning},
  volume = {63},
  pages = {25--51}
}

@article{faruquiProblemsEvaluationWord2016,
  title = {Problems with Evaluation of Word Embeddings Using Word Similarity Tasks},
  author = {Faruqui, Manaal and Tsvetkov, Yulia and Rastogi, Pushpendre and Dyer, Chris},
  year = {2016},
  journal = {arXiv preprint arXiv:1605.02276},
  eprint = {1605.02276},
  archiveprefix = {arxiv},
  keywords = {reference},
  file = {/home/konstantin/Zotero/storage/LNR5YHXT/1605.html}
}

@article{fellbaumChallengesMultilingualWordnet2012,
  title = {Challenges for a Multilingual Wordnet},
  author = {Fellbaum, Christiane and Vossen, Piek},
  year = {2012},
  month = jun,
  journal = {Language Resources and Evaluation},
  volume = {46},
  number = {2},
  pages = {313--326},
  issn = {1574-0218},
  doi = {10.1007/s10579-012-9186-z},
  urldate = {2019-10-23},
  abstract = {Wordnets have been created in many languages, revealing both their lexical commonalities and diversity. The next challenge is to make multilingual wordnets fully interoperable. The EuroWordNet experience revealed the shortcomings of an interlingua based on a natural language. Instead, we propose a model based on the division of the lexicon and a language-independent, formal ontology that serves as the hub interlinking the language-specific lexicons. The ontology avoids the idiosyncracies of the lexicon and furthermore allows formal reasoning about the concepts it contains. We address the division of labor between ontology and lexicon. Finally, we illustrate our model in the context of a domain-specific multilingual information system based on a central ontology and interconnected wordnets in seven languages.},
  langid = {english},
  keywords = {Formal ontology,Information system,Multilingual wordnets}
}

@incollection{firthSynopsisLinguisticTheory1957,
  title = {A Synopsis of Linguistic Theory 1930-55},
  booktitle = {Studies in {{Linguistic Analysis}}},
  author = {Firth, J. R.},
  editor = {Firth, J. R.},
  year = {1957},
  pages = {1--32},
  publisher = {{Blackwell}},
  address = {{Oxford}},
  abstract = {Reprinted in: Palmer, F. R. (ed.) (1968). Selected Papers of J. R. Firth 1952-59, pages 168-205. Longmans, London.},
  keywords = {classic linguistics meanign relatedness semantic,reference}
}

@inproceedings{fischerGroundTruthCreation2010,
  title = {Ground Truth Creation for Handwriting Recognition in Historical Documents},
  booktitle = {Proceedings of the 9th {{IAPR International Workshop}} on {{Document Analysis Systems}}},
  author = {Fischer, Andreas and Inderm{\"u}hle, Emanuel and Bunke, Horst and Viehhauser, Gabriel and Stolz, Michael},
  year = {2010},
  pages = {3--10},
  doi = {https://dl.acm.org/doi/pdf/10.1145/1815330.1815331}
}

@article{fowlerAgileManifesto2001,
  title = {The Agile Manifesto},
  author = {Fowler, Martin and Highsmith, Jim},
  year = {2001},
  journal = {Software Development},
  volume = {9},
  number = {8},
  pages = {28--35}
}

@inproceedings{franziniNuncEstAestimandum2019,
  title = {Nunc {{Est Aestimandum}}: {{Towards}} an {{Evaluation}} of the {{Latin WordNet}}},
  booktitle = {{{CLiC-it}}},
  author = {Franzini, Greta and Peverelli, Andrea and Ruffolo, Paolo and Passarotti, Marco and Sanna, Helena and Signoroni, Edoardo and Ventura, Viviana and Zampedri, Federica},
  year = {2019}
}

@article{garcia-gamezLearningNounsVerbs2019,
  title = {Learning Nouns and Verbs in a Foreign Language: {{The}} Role of Gestures},
  author = {{Garcia-Gamez}, Ana B and Macizo, Pedro},
  year = {2019},
  journal = {Applied Psycholinguistics},
  volume = {40},
  number = {2},
  pages = {473--507},
  publisher = {{Cambridge University Press}},
  issn = {0142-7164}
}

@article{garcia-penalvoOpenKnowledgeManagement2010,
  title = {Open Knowledge Management in Higher Education},
  author = {{Garc{\'i}a-Penalvo}, Francisco J. and {Garc{\'i}a de Figuerola}, Carlos and Merlo, Jose A.},
  year = {2010},
  journal = {Online Information Review},
  volume = {34},
  number = {4},
  pages = {517--519},
  file = {/home/konstantin/Zotero/storage/B46WJT8U/oir.2010.26434daa.html}
}

@article{gardenforsSemanticTransformations2015,
  title = {Semantic Transformations},
  author = {G{\"a}rdenfors, Peter},
  year = {2015},
  journal = {Language and Semiotic Studies},
  volume = {1},
  number = {1},
  pages = {41--51},
  publisher = {{Cambridge Scholars Publishing}}
}

@article{gardnerValidatingConstructWord2007,
  title = {Validating the {{Construct}} of {{Word}} in {{Applied Corpus-based Vocabulary Research}}: {{A Critical Survey}}},
  shorttitle = {Validating the {{Construct}} of {{Word}} in {{Applied Corpus-based Vocabulary Research}}},
  author = {Gardner, D.},
  year = {2007},
  month = apr,
  journal = {Applied Linguistics},
  volume = {28},
  number = {2},
  pages = {241--265},
  issn = {0142-6001, 1477-450X},
  doi = {10.1093/applin/amm010},
  urldate = {2017-10-30},
  langid = {english}
}

@inproceedings{geffetDistributionalInclusionHypotheses2005,
  title = {The Distributional Inclusion Hypotheses and Lexical Entailment},
  booktitle = {Proceedings of the 43rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{ACL}}'05)},
  author = {Geffet, Maayan and Dagan, Ido},
  year = {2005},
  pages = {107--114}
}

@book{georgesAusfuhrlichesLateinischDeutschesHandworterbuch1913,
  title = {Ausf\"uhrliches {{Lateinisch-Deutsches Handw\"orterbuch}}},
  author = {Georges, Karl Ernst},
  year = {1913}
}

@inproceedings{gladkovaAnalogybasedDetectionMorphological2016,
  title = {Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings: What Works and What Doesn't},
  shorttitle = {Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings},
  booktitle = {Proceedings of the {{NAACL Student Research Workshop}}},
  author = {Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi},
  year = {2016},
  pages = {8--15},
  keywords = {reference}
}

@inproceedings{golcherStylometryInterplayTopic2011,
  title = {Stylometry and the Interplay of Topic and {{L1}} in the Different Annotation Layers in the {{FALKO}} Corpus},
  booktitle = {Proceedings of {{Quantitative Investigations}} in {{Theoretical Linguistics}} 4},
  author = {Golcher, Felix and Reznicek, Marc},
  editor = {Zeldes, Amir and L{\"u}deling, Anke},
  year = {2011},
  pages = {29--34},
  publisher = {{Humboldt-Universit\"at zu Berlin}}
}

@article{grefenstetteExperimentalSupportCategorical2011,
  title = {Experimental Support for a Categorical Compositional Distributional Model of Meaning},
  author = {Grefenstette, Edward and Sadrzadeh, Mehrnoosh},
  year = {2011},
  journal = {arXiv preprint arXiv:1106.4058},
  eprint = {1106.4058},
  archiveprefix = {arxiv}
}

@article{griesBehavioralProfilesCorpusbased2009,
  title = {Behavioral Profiles: A Corpus-Based Approach to Cognitive Semantic Analysis},
  shorttitle = {Behavioral Profiles},
  author = {Gries, Stefan Th and Divjak, Dagmar},
  year = {2009},
  journal = {New directions in cognitive linguistics},
  pages = {57--75},
  keywords = {reference},
  file = {/home/konstantin/Zotero/storage/JBN4ZAXB/books.html}
}

@inproceedings{gyllenstenRgramsUnsupervisedLearning2019,
  title = {R-Grams: {{Unsupervised Learning}} of {{Semantic Units}} in {{Natural Language}}},
  shorttitle = {R-Grams},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{Computational Semantics}} - {{Student Papers}}},
  author = {Gyllensten, Amaru Cuba and Ekgren, Ariel and Sahlgren, Magnus},
  year = {2019},
  pages = {52--62},
  urldate = {2019-07-03},
  langid = {american},
  file = {/home/konstantin/Zotero/storage/Z9EN8SK6/W19-0607.html}
}

@article{hagiwaraSupervisedSynonymAcquisition2009,
  title = {Supervised Synonym Acquisition Using Distributional Features and Syntactic Patterns},
  author = {Hagiwara, Masato and Ogawa, Yasuhiro and Toyama, Katsuhiko},
  year = {2009},
  journal = {Information and Media Technologies},
  volume = {4},
  number = {2},
  pages = {558--582},
  file = {/home/konstantin/Zotero/storage/B2DHR6NQ/ja.html}
}

@article{hamiltonDiachronicWordEmbeddings2016,
  title = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
  author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
  year = {2016},
  journal = {arXiv preprint arXiv:1605.09096},
  eprint = {1605.09096},
  archiveprefix = {arxiv},
  keywords = {reference},
  file = {/home/konstantin/Zotero/storage/Z5L9AGG8/1605.html}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  year = {1954},
  month = aug,
  journal = {Word},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  urldate = {2020-03-17},
  langid = {english},
  keywords = {reference}
}

@inproceedings{haugCreatingParallelTreebank2008,
  title = {Creating a Parallel Treebank of the Old {{Indo-European Bible}} Translations},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Language Technology}} for {{Cultural Heritage Data}} ({{LaTeCH}} 2008)},
  author = {Haug, Dag TT and Johndal, Marius},
  year = {2008},
  pages = {27--34},
  annotation = {PROIEL}
}

@inproceedings{herbelotBuildingSharedWorld2015,
  title = {Building a Shared World: {{Mapping}} Distributional to Model-Theoretic Semantic Spaces},
  shorttitle = {Building a Shared World},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Herbelot, Aur{\'e}lie and Vecchi, Eva Maria},
  year = {2015},
  pages = {22--32}
}

@inproceedings{herbelotMeasuringSemanticContent2013,
  title = {Measuring Semantic Content in Distributional Vectors},
  booktitle = {Proceedings of the 51st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Herbelot, Aur{\'e}lie and Ganesalingam, Mohan},
  year = {2013},
  volume = {2},
  pages = {440--445}
}

@misc{hestnessDeepLearningScaling2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  urldate = {2019-07-29},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/konstantin/Zotero/storage/5DGISFJH/1712.html}
}

@article{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  year = {2012},
  journal = {arXiv preprint arXiv:1207.0580},
  eprint = {1207.0580},
  archiveprefix = {arxiv}
}

@inproceedings{huangImprovingWordRepresentations2012,
  title = {Improving {{Word Representations}} via {{Global Context}} and {{Multiple Word Prototypes}}},
  booktitle = {Proceedings of the 50th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Eric and Socher, Richard and Manning, Christopher and Ng, Andrew},
  year = {2012},
  month = jul,
  pages = {873--882},
  publisher = {{Association for Computational Linguistics}},
  address = {{Jeju Island, Korea}},
  urldate = {2020-09-04}
}

@article{janaNetworkFeaturesBased2018,
  title = {Network Features Based Co-Hyponymy Detection},
  author = {Jana, Abhik and Goyal, Pawan},
  year = {2018},
  journal = {arXiv preprint arXiv:1802.04609},
  eprint = {1802.04609},
  archiveprefix = {arxiv}
}

@article{janaUsingDistributionalThesaurus2020,
  title = {Using {{Distributional Thesaurus Embedding}} for {{Co-hyponymy Detection}}},
  author = {Jana, Abhik and Varimalla, Nikhil Reddy and Goyal, Pawan},
  year = {2020},
  journal = {arXiv preprint arXiv:2002.11506},
  eprint = {2002.11506},
  archiveprefix = {arxiv}
}

@article{jimeno-yepesExploitingMeSHIndexing2011,
  title = {Exploiting {{MeSH}} Indexing in {{MEDLINE}} to Generate a Data Set for Word Sense Disambiguation},
  author = {{Jimeno-Yepes}, Antonio J. and McInnes, Bridget T. and Aronson, Alan R.},
  year = {2011},
  month = jun,
  journal = {BMC Bioinformatics},
  volume = {12},
  number = {223},
  pages = {1--14},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-12-223},
  urldate = {2020-08-25},
  abstract = {Evaluation of Word Sense Disambiguation (WSD) methods in the biomedical domain is difficult because the available resources are either too small or too focused on specific types of entities (e.g. diseases or genes). We present a method that can be used to automatically develop a WSD test collection using the Unified Medical Language System (UMLS) Metathesaurus and the manual MeSH indexing of MEDLINE. We demonstrate the use of this method by developing such a data set, called MSH WSD.},
  file = {/home/konstantin/Zotero/storage/THBLQSX8/1471-2105-12-223.html}
}

@article{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08361 [cs, stat]},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  urldate = {2020-06-17},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,reference,Statistics - Machine Learning},
  file = {/home/konstantin/Zotero/storage/ISNHPEHH/2001.html}
}

@article{karakantaNeuralMachineTranslation2018,
  title = {Neural Machine Translation for Low-Resource Languages without Parallel Corpora},
  author = {Karakanta, Alina and Dehdari, Jon and {van Genabith}, Josef},
  year = {2018},
  journal = {Machine Translation},
  volume = {32},
  pages = {167--189}
}

@article{karanDistributionalSemanticsApproach2012,
  title = {Distributional Semantics Approach to Detecting Synonyms in {{Croatian}} Language},
  author = {Karan, Mladen and {\v S}najder, Jan and Ba{\v s}ic, Bojana Dalbelo},
  year = {2012},
  journal = {Information Society},
  pages = {111--116}
}

@article{kettunenCanTypetokenRatio2014,
  title = {Can Type-Token Ratio Be Used to Show Morphological Complexity of Languages?},
  author = {Kettunen, Kimmo},
  year = {2014},
  journal = {Journal of Quantitative Linguistics},
  volume = {21},
  number = {3},
  pages = {223--245},
  publisher = {{Taylor \& Francis}},
  issn = {0929-6174},
  doi = {https://www.tandfonline.com/doi/pdf/10.1080/09296174.2014.911506}
}

@article{koizumiRelationshipsTextLength2012,
  title = {Relationships between Text Length and Lexical Diversity Measures: {{Can}} We Use Short Texts of Less than 100 Tokens},
  author = {Koizumi, Rie},
  year = {2012},
  journal = {Vocabulary Learning and Instruction},
  volume = {1},
  number = {1},
  pages = {60--69}
}

@book{kuhnerAusfuhrlicheGrammatikLateinischen1914,
  title = {Ausf\"uhrliche {{Grammatik}} Der Lateinischen {{Sprache}}, 2. {{Teil}}: {{Satzlehre}}},
  author = {K{\"u}hner, Raphael and Stegmann, Carl},
  year = {1914},
  volume = {1},
  publisher = {{Hanovre}}
}

@inproceedings{leeGeneratingGroundTruth2012,
  title = {Generating Ground Truth for Music Mood Classification Using Mechanical Turk},
  booktitle = {Proceedings of the 12th {{ACM}}/{{IEEE-CS}} Joint Conference on {{Digital Libraries}}},
  author = {Lee, Jin Ha and Hu, Xiao},
  year = {2012},
  pages = {129--138},
  doi = {https://dl.acm.org/doi/pdf/10.1145/2232817.2232842}
}

@inproceedings{lewisCompositionalHyponymyPositive2019,
  title = {Compositional {{Hyponymy}} with {{Positive Operators}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Recent Advances}} in {{Natural Language Processing}} ({{RANLP}} 2019)},
  author = {Lewis, Martha},
  year = {2019},
  month = sep,
  pages = {638--647},
  publisher = {{INCOMA Ltd.}},
  address = {{Varna, Bulgaria}},
  doi = {10.26615/978-954-452-056-4_075},
  urldate = {2020-09-07},
  abstract = {Language is used to describe concepts, and many of these concepts are hierarchical. Moreover, this hierarchy should be compatible with forming phrases and sentences. We use linear-algebraic methods that allow us to encode words as collections of vectors. The representations we use have an ordering, related to subspace inclusion, which we interpret as modelling hierarchical information. The word representations built can be understood within a compositional distributional semantic framework, providing methods for composing words to form phrase and sentence level representations. We show that the resulting representations give competitive results on both word-level hyponymy and sentence-level entailment datasets.}
}

@article{liuCombiningIntegratedSampling2011,
  title = {Combining Integrated Sampling with {{SVM}} Ensembles for Learning from Imbalanced Datasets},
  author = {Liu, Yang and Yu, Xiaohui and Huang, Jimmy Xiangji and An, Aijun},
  year = {2011},
  journal = {Information Processing \& Management},
  volume = {47},
  number = {4},
  pages = {617--631},
  publisher = {{Elsevier}},
  issn = {0306-4573}
}

@article{luoCosineNormalizationUsing2017,
  title = {Cosine {{Normalization}}: {{Using Cosine Similarity Instead}} of {{Dot Product}} in {{Neural Networks}}},
  shorttitle = {Cosine {{Normalization}}},
  author = {Luo, Chunjie and Zhan, Jianfeng and Wang, Lei and Yang, Qiang},
  year = {2017},
  month = oct,
  journal = {arXiv:1702.05870 [cs, stat]},
  eprint = {1702.05870},
  primaryclass = {cs, stat},
  urldate = {2020-09-14},
  abstract = {Traditionally, multi-layer neural networks use dot product between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded, thus increases the risk of large variance. Large variance of neuron makes the model sensitive to the change of input distribution, thus results in poor generalization, and aggravates the internal covariate shift which slows down the training. To bound dot product and decrease the variance, we propose to use cosine similarity or centered cosine similarity (Pearson Correlation Coefficient) instead of dot product in neural networks, which we call cosine normalization. We compare cosine normalization with batch, weight and layer normalization in fully-connected neural networks as well as convolutional networks on the data sets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that cosine normalization achieves better performance than other normalization techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/konstantin/Zotero/storage/8XEEV8AH/1702.html}
}

@article{luoIncorporatingGlossesNeural2018,
  title = {Incorporating {{Glosses}} into {{Neural Word Sense Disambiguation}}},
  author = {Luo, Fuli and Liu, Tianyu and Xia, Qiaolin and Chang, Baobao and Sui, Zhifang},
  year = {2018},
  month = jul,
  journal = {arXiv:1805.08028 [cs]},
  eprint = {1805.08028},
  primaryclass = {cs},
  urldate = {2020-06-29},
  abstract = {Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/konstantin/Zotero/storage/DYM287G2/Luo et al. - 2018 - Incorporating Glosses into Neural Word Sense Disam.pdf;/home/konstantin/Zotero/storage/Y6DIAFR4/1805.html}
}

@article{mcdevitteCaesarGallicWar1869,
  title = {Caesar's {{Gallic War}}, Translated from {{De Bello Gallico}}, {{Julius Caesar}}},
  author = {McDevitte, {\relax WA} and Bohn, {\relax WS}},
  year = {1869},
  publisher = {{New York: Harper \& Brothers. Provided by The Internet Classics Archive at~\ldots}}
}

@book{mcgillivrayMethodsLatinComputational2013,
  title = {Methods in {{Latin}} Computational Linguistics},
  author = {McGillivray, Barbara},
  year = {2013},
  publisher = {{Brill}},
  address = {{Leiden; Boston}},
  keywords = {reference},
  file = {/home/konstantin/Zotero/storage/97SCDDJN/books.html}
}

@book{mengeLehrbuchLateinischenSyntax2009,
  title = {Lehrbuch Der Lateinischen {{Syntax}} Und {{Semantik}}},
  author = {Menge, Hermann and Burkard, Thorsten and Schauer, Markus},
  year = {2009},
  edition = {Fourth},
  publisher = {{Wissenschaftliche Buchgesellschaft}},
  collaborator = {Maier, Friedrich},
  isbn = {3-534-13661-6}
}

@misc{mihalceaSemCorCorpus2008,
  title = {{{SemCor Corpus}}},
  author = {Mihalcea, Rada},
  year = {2008},
  month = jun,
  urldate = {2020-08-25},
  abstract = {Sense-tagged Semantic Corpus 3.0},
  file = {/home/konstantin/Zotero/storage/V7SKIEMC/metadata.html}
}

@inproceedings{mihalceaSenseval3EnglishLexical2004,
  title = {The {{Senseval-3 English}} Lexical Sample Task},
  booktitle = {Proceedings of {{SENSEVAL-3}}, the {{Third International Workshop}} on the {{Evaluation}} of {{Systems}} for the {{Semantic Analysis}} of {{Text}}},
  author = {Mihalcea, Rada and Chklovski, Timothy and Kilgarriff, Adam},
  year = {2004},
  month = jul,
  pages = {25--28},
  publisher = {{Association for Computational Linguistics}},
  address = {{Barcelona, Spain}},
  urldate = {2020-08-25}
}

@inproceedings{mikolovDistributedRepresentationsWords2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  year = {2013},
  pages = {3111--3119},
  file = {/home/konstantin/Zotero/storage/2F4JX28B/5021-distributed-representations-of-words-andphrases.html}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  shorttitle = {{{Word2Vec}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  journal = {arXiv preprint arXiv:1301.3781},
  eprint = {1301.3781},
  pages = {1--12},
  archiveprefix = {arxiv},
  keywords = {reference},
  file = {/home/konstantin/Zotero/storage/VK2LNR37/1301.html}
}

@incollection{minozziLatinWordNetProject2010,
  title = {The {{Latin WordNet Project}}},
  booktitle = {Latin Linguistics Today: {{Akten}} Des 15. {{Internationalen Kolloquiums}} Zur {{Lateinischen Linguistik}}, {{Innsbruck}}, 4.- 9. {{April}} 2009},
  author = {Minozzi, S.},
  editor = {Anreiter, Peter and Kienpointner, M.},
  year = {2010},
  series = {Innsbrucker {{Beitr\"age}} Zur {{Sprachwissenschaft}}},
  volume = {137},
  pages = {707--716},
  publisher = {{Institut f\"ur Sprachen und Literaturen der Universit\"at Innsbruck, Bereich Sprachwissenschaft}},
  address = {{Innsbruck}},
  isbn = {978-3-85124-723-7},
  file = {/home/konstantin/Zotero/storage/M7U8QZFR/latin-linguistics-today-akten-des-15-internationalen-kolloquiums-zur-lateinischen-linguistik.html}
}

@inproceedings{moroAnnotatingMASCCorpus2014,
  title = {Annotating the {{MASC Corpus}} with {{BabelNet}}},
  booktitle = {{{LREC}}},
  author = {Moro, Andrea and Navigli, Roberto and Tucci, Francesco Maria and Passonneau, Rebecca J},
  year = {2014},
  pages = {4214--4219}
}

@inproceedings{navigliBabelNetBuildingVery2010,
  title = {{{BabelNet}}: {{Building}} a {{Very Large Multilingual Semantic Network}}},
  shorttitle = {{{BabelNet}}},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Navigli, Roberto and Ponzetto, Simone Paolo},
  year = {2010},
  month = jul,
  pages = {216--225},
  publisher = {{Association for Computational Linguistics}},
  address = {{Uppsala, Sweden}},
  urldate = {2020-08-25}
}

@misc{niederauNavigiumLateinDeutschWorterbuch2012,
  title = {Navigium {{Latein-Deutsch-W\"orterbuch}}},
  author = {Niederau, Philipp},
  year = {2012}
}

@incollection{niemeyerZurStellungAttributiven1997,
  title = {Zur {{Stellung}} Des Attributiven {{Adjektivs}} Im {{Lateinischen}} Und in Den Romanischen {{Sprachen}}. {{Syntaktische Gemeinsamkeiten}} Und {{Unterschiede}}},
  booktitle = {Semiotische {{Prozesse}} Und Nat\"urliche {{Sprache}}: {{Festschrift}} F\"ur {{Udo L}}. {{Figge}} Zum 60. {{Geburtstag}}},
  author = {Niemeyer, J{\"u}rgen and Krenn, Herwig},
  editor = {Gather, Andreas and Werner, Heinz},
  year = {1997},
  pages = {411--423},
  publisher = {{Franz Steiner Verlag}},
  address = {{Stuttgart}}
}

@inproceedings{nivreUniversalDependenciesV12016,
  title = {Universal Dependencies v1: {{A}} Multilingual Treebank Collection},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Nivre, Joakim and De Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajic, Jan and Manning, Christopher D and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia},
  year = {2016},
  pages = {1659--1666}
}

@inproceedings{ochabStylometryLiteraryPapyri2019,
  title = {Stylometry of Literary Papyri},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Digital Access}} to {{Textual Cultural Heritage}}},
  author = {Ochab, Jeremi K. and Essler, Holger},
  year = {2019},
  pages = {139--142},
  file = {/home/konstantin/Zotero/storage/WQRFYIPA/3322905.html}
}

@inproceedings{onoWordEmbeddingbasedAntonym2015,
  title = {Word Embedding-Based Antonym Detection Using Thesauri and Distributional Information},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ono, Masataka and Miwa, Makoto and Sasaki, Yutaka},
  year = {2015},
  pages = {984--989}
}

@article{petersDeepContextualizedWord2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  journal = {arXiv preprint arXiv:1802.05365},
  eprint = {1802.05365},
  archiveprefix = {arxiv},
  file = {/home/konstantin/Zotero/storage/UL78NLC2/1802.html}
}

@article{petersSoftClusteringFuzzy2013,
  title = {Soft Clustering\textendash Fuzzy and Rough Approaches and Their Extensions and Derivatives},
  author = {Peters, Georg and Crespo, Fernando and Lingras, Pawan and Weber, Richard},
  year = {2013},
  journal = {International Journal of Approximate Reasoning},
  volume = {54},
  number = {2},
  pages = {307--322},
  publisher = {{Elsevier}},
  issn = {0888-613X}
}

@incollection{pierrehumbertBurstinessVerbsDerived2012,
  title = {Burstiness of Verbs and Derived Nouns},
  booktitle = {Shall {{We Play}} the {{Festschrift Game}}? {{Essays}} on the {{Occasion}} of {{Lauri Carlson}}'s 60th {{Birthday}}},
  author = {Pierrehumbert, Janet B},
  editor = {Santos, Diana and Linden, Krister and Ng'ang'a, Wanjiju},
  year = {2012},
  pages = {99--115},
  publisher = {{Springer}}
}

@incollection{pinkalSemantik1993,
  title = {Semantik},
  booktitle = {Einf\"uhrung in Die K\"unstliche {{Intelligenz}}},
  author = {Pinkal, Manfred},
  editor = {G{\"o}rz, G{\"u}nther},
  year = {1993},
  edition = {First},
  pages = {425--498},
  publisher = {{Addison-Wesley}}
}

@incollection{ponsborderiaPathsGrammaticalizationSpanish2014,
  title = {Paths of Grammaticalization in {{Spanish}} o Sea},
  booktitle = {Discourse and {{Pragmatic Markers}} from {{Latin}} to the {{Romance Languages}}},
  author = {Pons Border{\'i}a, Salvador},
  editor = {Ghezzi, Chiara and Molinelli, Piera},
  year = {2014},
  pages = {109--136}
}

@article{punuruLearningNontaxonomicalSemantic2012,
  title = {Learning Non-Taxonomical Semantic Relations from Domain Texts},
  author = {Punuru, Janardhana and Chen, Jianhua},
  year = {2012},
  month = feb,
  journal = {Journal of Intelligent Information Systems},
  volume = {38},
  number = {1},
  pages = {191--207},
  issn = {1573-7675},
  doi = {10.1007/s10844-011-0149-4},
  urldate = {2019-10-22},
  abstract = {Ontology of a domain mainly consists of concepts, taxonomical (hierarchical) relations and non-taxonomical relations. Automatic ontology construction requires methods for extracting both taxonomical and non-taxonomical relations. Compared to extensive works on concept extraction and taxonomical relation learning, little attention has been given on identification and labeling of non-taxonomical relations in text mining. In this paper, we propose an unsupervised technique for extracting non-taxonomical relations from domain texts. We propose the VF*ICF metric for measuring the importance of a verb as a representative relation label, in much the same spirit as the TF*IDF measure in information retrieval. Domain-relevant concepts (nouns) are extracted using techniques developed earlier. Candidate non-taxonomical relations are generated as (SVO) triples of the form (subject, verb, object) from domain texts. A statistical method with log-likelihood ratios is used to estimate the significance of relationships between concepts and to select suitable relation labels. Texts from two domains, the Electronic Voting (EV) domain texts and the Tenders and Mergers (TNM) domain texts are used to compare our method with one of the existing approaches. Experiments showed that our method achieved better performance in both domains.},
  langid = {english},
  keywords = {Ontology learning,Semantical relation,Text mining}
}

@article{robillardMonolingualBilingualChildren2014,
  title = {Monolingual and Bilingual Children with and without Primary Language Impairment: Core Vocabulary Comparison},
  shorttitle = {Monolingual and Bilingual Children with and without Primary Language Impairment},
  author = {Robillard, Manon and {Mayer-Crittenden}, Chantal and {Minor-Corriveau}, Mich{\`e}le and B{\'e}langer, Roxanne},
  year = {2014},
  journal = {Augmentative and alternative communication},
  volume = {30},
  number = {3},
  pages = {267--278},
  file = {/home/konstantin/Zotero/storage/QP773HXB/07434618.2014.html}
}

@article{roelliCorpusCorporumNew2014,
  title = {The {{Corpus Corporum}}, a New Open {{Latin}} Text Repository and Tool},
  author = {Roelli, Philipp},
  year = {2014},
  journal = {Archivum Latinitatis Medii Aevi-Bulletin du Cange (ALMA)},
  file = {/home/konstantin/Zotero/storage/TMFM3VHH/69525.html}
}

@inproceedings{rogersTooManyProblems2017,
  title = {The (Too Many) Problems of Analogical Reasoning with Word Vectors},
  booktitle = {Proceedings of the 6th {{Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (* {{SEM}} 2017)},
  author = {Rogers, Anna and Drozd, Aleksandr and Li, Bofang},
  year = {2017},
  pages = {135--148},
  keywords = {reference},
  file = {/home/konstantin/Zotero/storage/K5QC7J34/S17-1017.html}
}

@inproceedings{rollerInclusiveSelectiveSupervised2014,
  title = {Inclusive yet Selective: {{Supervised}} Distributional Hypernymy Detection},
  shorttitle = {Inclusive yet Selective},
  booktitle = {Proceedings of {{COLING}} 2014, the 25th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Roller, Stephen and Erk, Katrin and Boleda, Gemma},
  year = {2014},
  pages = {1025--1036}
}

@inproceedings{sachanEffectiveUseBidirectional2018,
  title = {Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition},
  booktitle = {Machine {{Learning}} for {{Healthcare Conference}}},
  author = {Sachan, Devendra Singh and Xie, Pengtao and Sachan, Mrinmaya and Xing, Eric P},
  year = {2018},
  pages = {383--402}
}

@article{schibelZurAneignungLateinischer2013,
  title = {Zur {{Aneignung}} Lateinischer {{Literatur}} Und {{Sprache}}},
  author = {Schibel, Wolfgang},
  year = {2013},
  journal = {Forum Classicum},
  pages = {113--124}
}

@inproceedings{seddahCheatingParserDeath2018,
  title = {Cheating a {{Parser}} to {{Death}}: {{Data-driven Cross-Treebank Annotation Transfer}}},
  booktitle = {Eleventh {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Seddah, Djam{\'e} and {de La Clergerie}, {\'E}ric Villemonte and Sagot, Beno{\^i}t and Alonso, H{\'e}ctor Mart{\'i}nez and Candito, Marie},
  year = {2018}
}

@article{shadrovaMeasuringCoselectionalConstraint2020,
  title = {Measuring Coselectional Constraint in Learner Corpora: {{A}} Graph-Based Approach},
  shorttitle = {Measuring Coselectional Constraint in Learner Corpora},
  author = {Shadrova, Anna Valer'evna},
  year = {2020},
  month = jul,
  publisher = {{Humboldt-Universit\"at zu Berlin}},
  doi = {10.18452/21606},
  urldate = {2020-10-19},
  abstract = {Die korpuslinguistische Arbeit untersucht den Erwerb von Koselektionsbeschr\"ankungen bei Lerner*innen des Deutschen als Fremdsprache in einem quasi-longitudinalen Forschungsdesign anhand des Kobalt-Korpus. Neben einigen statistischen Analysen wird vordergr\"undig eine graphbasierte Analyse entwickelt, die auf der Graphmetrik Louvain-Modularit\"at aufbaut. Diese wird f\"ur diverse Subkorpora nach verschiedenen Kriterien berechnet und mit Hilfe verschiedener Samplingtechniken umfassend intern validiert. Im Ergebnis zeigen sich eine Abh\"angigkeit der gemessenen Modularit\"atswerte vom Sprachstand der Teilnehmer*innen, eine h\"ohere Modularit\"at bei Muttersprachler*innen, niedrigere Modularit\"atswerte bei wei\ss russischen vs. chinesischen Lerner*innen sowie ein U-Kurven-f\"ormiger Erwerbsverlauf bei wei\ss russischen, nicht aber chinesischen Lerner*innen. Unterschiede zwischen den Gruppen werden aus typologischer, kognitiver, diskursiv-kultureller und Registerperspektive diskutiert. Abschlie\ss end werden Vorschl\"age f\"ur den Einsatz von graphbasierten Modellierungen in kernlinguistischen Fragestellungen entwickelt. Zus\"atzlich werden theoretische L\"ucken in der gebrauchsbasierten Beschreibung von Koselektionsph\"anomenen (Phraseologie, Idiomatizit\"at, Kollokation) aufgezeigt und ein multidimensionales funktionales Modell als Alternative vorgeschlagen.},
  copyright = {(CC BY-NC 4.0) Attribution-NonCommercial 4.0 International},
  langid = {english},
  annotation = {Accepted: 2020-07-24T10:43:05Z},
  file = {/home/konstantin/Zotero/storage/GU5ZZKYA/22356.html;/home/konstantin/Zotero/storage/Z38NMF7F/22356.html}
}

@book{shortLatinDictionary1879,
  title = {A {{Latin Dictionary}}},
  author = {Short, Charles and Lewis, Charlton T.},
  year = {1879},
  publisher = {{Clarendon Press}},
  address = {{Oxford}}
}

@article{shwartz-zivOpeningBlackBox2017,
  title = {Opening the {{Black Box}} of {{Deep Neural Networks}} via {{Information}}},
  author = {{Shwartz-Ziv}, Ravid and Tishby, Naftali},
  year = {2017},
  month = apr,
  journal = {arXiv:1703.00810 [cs]},
  eprint = {1703.00810},
  primaryclass = {cs},
  pages = {1--19},
  urldate = {2020-12-08},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textbackslash textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{\textbackslash emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/konstantin/Zotero/storage/6BKCVKBR/1703.html}
}

@phdthesis{souille-rigautSemanticAccountQuasiLexemes2010,
  title = {A {{Semantic Account}} of {{Quasi-Lexemes}} in {{Modern English-Processing Semiotic Units}} of {{Greek}} or {{Latin Origin}} into {{Lexical Units}}},
  author = {{Souill{\'e}-Rigaut}, Chris},
  year = {2010},
  school = {University of Kansas},
  file = {/home/konstantin/Zotero/storage/WJSA5ZGD/7406.html}
}

@article{stollNounsVerbsChintang2012,
  title = {Nouns and Verbs in {{Chintang}}: Children's Usage and Surrounding Adult Speech},
  author = {Stoll, Sabine and Bickel, Balthasar and Lieven, Elena and Paudyal, Netra P and Banjade, Goma and Bhatta, Toya N and Gaenszle, Martin and Pettigrew, Judith and Rai, Ichchha Purna and Rai, Manoj},
  year = {2012},
  journal = {Journal of Child Language},
  volume = {39},
  number = {2},
  pages = {284--321},
  publisher = {{Cambridge University Press}},
  issn = {1469-7602}
}

@article{taslimExperimentalStudyTeaching2014,
  title = {An {{Experimental Study}} of {{Teaching Vocabulary}} by {{Using Hyponymy Games}} on the {{Seventh Grader F MTs Syech Ibrahim Payakumbuh}}},
  author = {Taslim, Fadilla},
  year = {2014},
  journal = {Al-Ta lim Journal},
  volume = {21},
  number = {3},
  pages = {189--197}
}

@article{tengiDesignImplementationWordNet1998,
  title = {Design and Implementation of the {{WordNet}} Lexical Database and Searching Software},
  author = {Tengi, Randee I.},
  editor = {Fellbaum, Christiane},
  year = {1998},
  journal = {WordNet: an electronic lexical database},
  pages = {105--127},
  file = {/home/konstantin/Zotero/storage/YJS4ZNCU/books.html}
}

@article{uzunerCommunityAnnotationExperiment2010,
  title = {Community Annotation Experiment for Ground Truth Generation for the I2b2 Medication Challenge},
  author = {Uzuner, {\"O}zlem and Solti, Imre and Xia, Fei and Cadag, Eithon},
  year = {2010},
  month = sep,
  journal = {Journal of the American Medical Informatics Association},
  volume = {17},
  number = {5},
  pages = {519--523},
  publisher = {{Oxford Academic}},
  issn = {1067-5027},
  doi = {10.1136/jamia.2010.004200},
  urldate = {2020-08-24},
  abstract = {Abstract.  Objective: Within the context of the Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records, the authors (also referred t},
  langid = {english},
  file = {/home/konstantin/Zotero/storage/T994HV9P/831043.html}
}

@incollection{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5998--6008},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-09-08},
  file = {/home/konstantin/Zotero/storage/M2CUCG48/7181-attention-is-all-you-need.html}
}

@article{weiRoleBalancedTraining2013,
  title = {The {{Role}} of {{Balanced Training}} and {{Testing Data Sets}} for {{Binary Classifiers}} in {{Bioinformatics}}},
  author = {Wei, Qiong and Dunbrack Jr, Roland L.},
  year = {2013},
  month = jul,
  journal = {PLOS ONE},
  volume = {8},
  number = {7},
  pages = {e67863},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0067863},
  urldate = {2020-09-07},
  abstract = {Training and testing of conventional machine learning models on binary classification problems depend on the proportions of the two outcomes in the relevant data sets. This may be especially important in practical terms when real-world applications of the classifier are either highly imbalanced or occur in unknown proportions. Intuitively, it may seem sensible to train machine learning models on data similar to the target data in terms of proportions of the two binary outcomes. However, we show that this is not the case using the example of prediction of deleterious and neutral phenotypes of human missense mutations in human genome data, for which the proportion of the binary outcome is unknown. Our results indicate that using balanced training data (50\% neutral and 50\% deleterious) results in the highest balanced accuracy (the average of True Positive Rate and True Negative Rate), Matthews correlation coefficient, and area under ROC curves, no matter what the proportions of the two phenotypes are in the testing data. Besides balancing the data by undersampling the majority class, other techniques in machine learning include oversampling the minority class, interpolating minority-class data points and various penalties for misclassifying the minority class. However, these techniques are not commonly used in either the missense phenotype prediction problem or in the prediction of disordered residues in proteins, where the imbalance problem is substantial. The appropriate approach depends on the amount of available data and the specific problem at hand.},
  langid = {english},
  keywords = {Deletion mutation,Human genomics,Machine learning,Missense mutation,Mutation,Mutation databases,Primates,Substitution mutation},
  file = {/home/konstantin/Zotero/storage/34MUKKKF/article.html}
}

@article{yorkConstructivistApproachGamebased2018,
  title = {A Constructivist Approach to Game-Based Language Learning: {{Student}} Perceptions in a Beginner-Level {{EFL}} Context},
  author = {York, James and {deHaan}, Jonathan William},
  year = {2018},
  journal = {International Journal of Game-Based Learning (IJGBL)},
  volume = {8},
  number = {1},
  pages = {19--40},
  publisher = {{IGI Global}}
}

@article{yuanCountingNounsSimple2012,
  title = {Counting the {{Nouns}}: {{Simple Structural Cues}} to {{Verb Meaning}}},
  shorttitle = {Counting the {{Nouns}}},
  author = {Yuan, Sylvia and Fisher, Cynthia and Snedeker, Jesse},
  year = {2012},
  journal = {Child Development},
  volume = {83},
  number = {4},
  pages = {1382--1399},
  issn = {1467-8624},
  doi = {10.1111/j.1467-8624.2012.01783.x},
  urldate = {2020-09-08},
  abstract = {Two-year-olds use the sentence structures verbs appear in\textemdash subcategorization frames\textemdash to guide verb learning. This is syntactic bootstrapping. This study probed the developmental origins of this ability. The structure-mapping account proposes that children begin with a bias toward one-to-one mapping between nouns in sentences and participant roles in events. This account predicts that subcategorization frames should guide very early verb learning, if the number of nouns in the sentences is informative. In 3 experiments, one hundred and thirty-six 21- and 19-month-olds assigned appropriately different interpretations to novel verbs in transitive (``He's gorping him!'') versus intransitive sentences (``He's gorping!'') differing in their number of nouns. Thus, subcategorization frames guide verb interpretation in very young children. These findings constrain theoretical proposals about mechanisms for syntactic bootstrapping.},
  langid = {english},
  file = {/home/konstantin/Zotero/storage/MZ3QG8NW/j.1467-8624.2012.01783.html}
}

@article{yuanSemisupervisedWordSense2016,
  title = {Semi-Supervised {{Word Sense Disambiguation}} with {{Neural Models}}},
  author = {Yuan, Dayu and Richardson, Julian and Doherty, Ryan and Evans, Colin and Altendorf, Eric},
  year = {2016},
  month = nov,
  journal = {arXiv:1603.07012 [cs]},
  eprint = {1603.07012},
  primaryclass = {cs},
  urldate = {2020-08-25},
  abstract = {Determining the intended sense of words in text - word sense disambiguation (WSD) - is a long standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text. In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/konstantin/Zotero/storage/32R2SA68/1603.html}
}

@book{zipfPsychobiologyLanguageIntroduction1936,
  title = {The Psycho-Biology of Language: {{An}} Introduction to Dynamic Philology},
  shorttitle = {The Psycho-Biology of Language},
  author = {Zipf, George Kingsley},
  year = {1936}
}
