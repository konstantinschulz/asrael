\documentclass[jou]{apa6} %apacite

\usepackage[style=authoryear,sorting=nyt,backend=bibtex]{biblatex}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{soul}
\usepackage{graphicx}
\graphicspath{ {figures/} }

\addbibresource{lit.bib}

\title{ASRAEL - Acquisition of Semantic RelAtions bEtween Latin nouns}
\shorttitle{APA style}

\author{Konstantin Schulz}
\affiliation{Humboldt-Universität zu Berlin}

\abstract{XXXabstractXXX}

\rightheader{ASRAEL}
\leftheader{Konstantin Schulz}

\begin{document}
\maketitle    
\tableofcontents
\listoffigures
                        
\section{Introduction}

This dissertation deals with different ways of detecting semantic relations in text corpora. Its focus is more on methodology than on analytical results. Some important restrictions are necessitated by the precise research question: In a corpus of Latin texts, how can we automatically extract semantic relations that are relevant for language learning? I will concentrate only on a subset of the existing phenomena, namely hypernymy and hyponymy. This selection is motivated by the objective of utilizing the obtained insights as the basis for exercises in the process of learning historical languages. Thus, the detection of said relations is not an end in itself, but is to be viewed in the context of how humans learn languages. 
Specifically, I hope that an improved integration of hyponymy into the learning process can lead to a higher level of language skills: 
 \blockquote[{\cite[p.~5]{nerlichPolysemyFlexibilityIntroduction2003}}]{In cognitive linguistics, the word itself with its network of polysemous senses came to be regarded as a category in which the senses of the word (i.e. the members of the category) are related to each other by means of general cognitive principles such as metaphor, metonymy, generalization, specialization, and image-schema transformations.}
If there is a parallel between semantic relations and the way that humans think, it seems reasonable to exploit that connection for learning purposes. Unfortunately, this still leaves us with the lack of a clearly defined research object:
\blockquote[{\cite[p.~4]{nerlichPolysemyFlexibilityIntroduction2003}}]{Another problem arising from polysemy and homonymy is lexical ambiguity, and the precise relationship between polysemy, homonymy, ambiguity and vagueness is still an unresolved issue in lexical semantics.}
It is therefore important to define the various semantic relations and distinguish between them as clearly as possible. In the case of, e.g., synonymy, a good operationalization is needed to retrieve the relevant instances from a corpus of text data:
\blockquote[{\cite[p.~274]{divjakCorpusbasedCognitiveSemantics2009}}]{Polysemy requires the researcher to determine whether two usage events are identical or sufficiently similar to be considered a single sense, what the degree of similarity is between different senses, where to connect a sense to others in the network, and which sense(s) to recognize as prototypical one(s). [...] in addition, [linguists] have to decide what the differences are between the near-synonyms as well as what the relation is between semantically similar words in a domain.}
In my study, I will follow the approach of distributional semantics, describing specific semantic relations between words in terms of their co-occurrences and typical contexts in a corpus. Previous research has been carried out using, e.g., association measures to detect synonymy:
\blockquote[{\cite[p.~566]{hagiwaraSupervisedSynonymAcquisition2009}}]{The value of distributional features \( f_j^D (x, z) \) is determined so that it represents the degree of commonality of context \( c_j \) shared by the word pair (x, z). [...] The advantage of this feature construction is that, given the independence assumption between word x and z , the feature value is easily calculated as the simple sum of two corresponding pointwise mutual information weights as: \( f_j^D (x, z) = PMI(x, c_j) + PMI(z, c_j) \) [...].}
One of the critical parts in that view is the term "word pair", which introduces the linguistically ill-defined concept of word into an otherwise seemingly precise mathematical formula. Thus, the usage of bare token-based PMI (Pointwise Mutual Information) for distributional questions may lead to severely skewed results:
\blockquote[{\cite[p.~444]{herbelotMeasuringSemanticContent2013}}]{[...] strong collocation effects can influence the measurement of information negatively: it is an open question which phrases should be considered ‘words-with-spaces’ when building distributions.}
Therefore, corpora may have to be tokenized and segmented using more complex separator rules than "whitespace and punctuation" before they can serve as a basis for distributional analyses. Besides, polysemy itself may also become a problem for resolving other issues like synonymy, e.g. if two words are synonymous only in one of their possibly numerous senses:
\blockquote[{\cite[p.~444]{herbelotMeasuringSemanticContent2013}}]{Some of the errors we observe may also be related to word senses. For instance, the word medium, to be found in the pair magazine – medium, can be synonymous with middle, clairvoyant or again mode of communication. In the sense of clairvoyant, it is clearly more specific than in the sense intended in the test pair. As distributions do not distinguish between senses, this will have an effect on our results.}
Since the solution to this problem may be to disambiguate word senses for every single token in a corpus, polysemy will be treated in this study not just as one of the analytical targets, but also as a methodological aspect that needs to be addressed before other kinds of analyses can be performed. 

\subsection{Hyponymy}

After dealing with various issues of polysemy, some of the second-tier relations may have to be redefined when looked at from a distributional perspective, e.g. hyponymy:
\blockquote[{\cite[p.~443]{herbelotMeasuringSemanticContent2013}}]{Although beverage is an umbrella word for many various types of drinks, speakers of English use it in very particular contexts. So, distributionally, it is not a ‘general word’.}
In this view, other existing taxonomies that may have acted as models of hyponymy need to be replaced by a definition that reflects how actual usage contexts can limit our selection of possible candidates for hyponymy:
\blockquote[{\cite[p.~443]{rollerInclusiveSelectiveSupervised2014}}]{[The Distributional Inclusion Hypothesis] states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. \#“dog rights”).}
Unfortunately, this view may clash with the observations of \cite{herbelotMeasuringSemanticContent2013}:
\begin{itemize}
      \item Hypernyms can have very specific usage contexts.
      \item Collocations may lead to unexpected results in the distribution of words.
\end{itemize}
Similar adjustments have to be made in treating other kinds of semantic relations. Therefore, many researchers apply preprocessing to their corpora before performing the actual analysis on them:
\blockquote[{\cite[p.~56f.]{gyllenstenRgramsUnsupervisedLearning2019}}]{It is worth noting that the skipgram model uses subsampling of common words, which is an optimization introduced to compensate for the power law distribution in common vocabularies. Also, the skipgram model controls for collocations by dampening the impact of frequent collocations.}
However, this increases the danger of circularity, revealing only those results that we were already expecting in the first place. As an alternative, we should look for a model that is robust enough to detect semantic relations despite the confounding factors. The objective should be to fit the model to the base text, not the other way round. For creating word embeddings using the skipgram model, subsampling and the restriction of strong collocations may be justified from a computational perspective because they enable a faster training of models while more or less preserving the resulting performance in the prediction of word contexts. This very tradeoff, though, may not work out as well for the detection of semantic relations, e.g. if we give a higher priority to the frequency of certain contexts. Such a change of algorithm may particularly be motivated by the distinction of theoretical and empirical scopes of words:
\blockquote[{\cite[p.~46]{coenenAnalogieUndMetapher2013}}]{Theoretischer Anwendungsbereich eines Wortes: die Klasse der Gegenstände, auf die ein Wort kraft seiner Bedeutung beschreibend angewandt werden kann (TA). Empirischer Anwendungsbereich eines Wortes: die Menge der Gegenstände, zu deren Beschreibung ein Wort - bis zu einem gewissen Zeitpunkt - in der Erfahrung eines bestimmten SprachteiInehmers bereits angewandt wurde (EA).}
This may be transferred to the domains of corpus linguistics and language learning: Given a text corpus that includes a learner's desired knowledge (e.g. all the literature one wants to read), the theoretical scope of a certain word would be equivalent to all the distinct relevant contexts in the corpus in which that word occurs. In contrast, the empirical scope of that same word would be equal to the number of distinct relevant contexts in the corpus in which it occurs \hl{and} which are known to the learner (at any given point in time of his learning process). Thus, the theoretical scope will remain static as long as we do not change the corpus, while the empirical one will always be dynamic in any case. From this point of view, we can also restrict our notion of semantic relations: Hyponymy, for instance, will be tied to those word networks that are present in a specific corpus and, for a learner, specific instances of hyponomy relations will not exist unless they have already been part of a learning situation. It is therefore our duty to a) detect such instances in the corpus and b) use them to design learning situations that help language learners to build lexical knowledge. In order to accomplish that first part, we need to have clearly defined requirements to qualify a relation as one of hyponymy:
\blockquote[{\cite[p.~16]{gevaudanSemantischeRelationenNominalen1999}}]{Auf der konzeptuellen Ebene, auf der auch Similarität und Kontiguität angesiedelt sind, beruht Hyponymie auf der taxonomischen Inklusion von Konzepten. Die taxonomische Inklusion ist nichts anderes als eine Ober-/Unterbegriffsbeziehung [...]. Diese Konzeptpaare sind teilidentisch, weil die Extension (die Menge der mit dem Begriff gemeinten Phänomene) des Unterbegriffs in der des Oberbegriffs enthalten ist und gleichzeitig die Intension (die Menge der dem Begriff zugeordneten Eigenschaften) des Oberbegriffs in der des Unterbegriffs enthalten ist [...].}
This notion of semantic super- and subordination between words closely resembles the principles in cognitive linguistics cited above \cite[p.~5]{nerlichPolysemyFlexibilityIntroduction2003}:
\begin{figure}[h]
\includegraphics[width=\linewidth]{hyponymy_generalization_specification}
\caption{Hyponymy as generalisation/specification (\cite[p.~18]{gevaudanSemantischeRelationenNominalen1999})}
\end{figure}
If hyponymy refers to some underlying taxonomy in which a hyponym is subordinate to its hypernym, our most important task will be to establish that taxonomy in a corpus-based manner. Therefore, lexical resources like the Latin WordNet (\cite{minozziLatinWordNetProject2010}) may contribute a reference model for such a purpose because, apart from its own invention of synsets, it includes a built-in notion of hyponymy. Unfortunately, the WordNet defines such relations based on intuition, not on actual language use. It is therefore static and non-empirical. However, since it was supposedly constructed by experts, we may use their assessment for evaluating the corpus-based extraction of hyponymy. This kind of approach is supported by recent studies:
\blockquote[{\cite[p.~984-988]{onoWordEmbeddingbasedAntonym2015}}]{The WE-T model receives supervised information from synonym and antonym pairs in thesauri and infers the relations of the other word pairs in the thesauri from the supervised information. The WE-TD model incorporates corpus-based contextual information (distributional information) into the WE-T model, which enables the calculation of the similarities among in-vocabulary and out-of-vocabulary words. [...] Our WE-TD model achieved the highest score among the models that use both thesauri and distributional information.}
According to \cite{onoWordEmbeddingbasedAntonym2015}, it is not sufficient to use thesauri like the WordNet for detecting semantic relations; rather, we should use that as a basis and provide additional information from actual corpora. In both cases, we can increase the amount of provided information by relying on associated properties of hyponymy relations, e.g. transitivity:
\blockquote[{\cite[p.~58]{coenenAnalogieUndMetapher2013}}]{Die Relationen der Hyper- und Hyponymie sind transitiv. Wenn A Hyponym in Bezug auf B ist, dann auch in Bezug auf die Hyperonyme von B, und wenn B Hyperonym in Bezug auf A ist, dann auch in Bezug auf die Hyponyme von A.}
Applied to the Latin WordNet, this may look as follows:
\begin{enumerate}
         \item \textit{gladius}
         \item a cutting or thrusting weapon with a long blade
         \item weaponry used in fighting or hunting
         \item weapons considered collectively
         \item an artifact (or system of artifacts) that is instrumental in accomplishing some end
         \item a man-made object
         \item a physical (tangible and visible) entity
\end{enumerate}
This information flow can be read from both sides. Beginning at \textit{gladius} (engl. sword), it can be used to simulate the process of reading a text: We encounter a word in the text (e.g. the plural \textit{gladii}) and determine its lemma (\textit{gladius}). From there, we can try to find a corresponding meaning for the surface form, e.g. "a cutting or thrusting weapon with a long blade". This meaning, in the WordNet taxonomy, can be hyponym to other kinds of meaning, e.g. "weaponry used in fighting or hunting". Now, if we know that
\begin{itemize}
         \item \textit{gladius} is a weapon with a blade and
         \item weapons with a blade can be used for hunting,
\end{itemize}
then we are safe to assume that a \textit{gladius} can be used for hunting. The further we go down the list, the more abstract do the meanings get. Nonetheless (or because of that), every layer can remind us of new specific features that we may want to associate with \textit{gladius}: specific motion (2.), specific purpose (3.), hypernym (4.), general purpose (5.), origin and production (6.), material (7.). In some cases, we may use morphological hints to get faster access to information about hyponymy:
\blockquote[{\cite[p.~913]{anstattTypenSemantischerRelationen2009a}}]{[...] zur Bildung von Hyponymen kann z.B. das Suffix -ovye/-evye (seld’ ‚Hering‘ – sel’devye ‚Heringsartige‘) verwendet werden (s. Ginzburg 1985, 9).}
This is especially true for Latin with its relatively rich morphology: the meaning of \textit{artifex} (engl. artist) may be associated directly to \textit{ars} (engl. art) and \textit{facere} (engl. to make), without necessarily traversing a hierarchical chain of increasing generalisation. Interestingly, this is possible even in cases where the compound is formed from foreign language material and thus cannot be easily paraphrased in the native language:
\blockquote[{\cite[p.~33]{souille-rigautSemanticAccountQuasiLexemes2010}}]{The head constituent in both compounds is on the right, the modifier constituent on the left, and, although ‘cardiopathy’ is not paraphrasable, we can still deduce from its semiotic units that the compound as a whole is a hyponym of the meaning carried by the semiotic unit -path-.}
This works similarly for Greek loanwords in Latin, e.g. \textit{xylophytum} (a certain kind of plant) cannot be described in Latin words as a \textit{*phytum} that is made of \textit{*xylum}. However, Romans may have deduced that the meaning of \textit{xylophytum} is related to ($\rightarrow$ hyponymy) \textit{-phyt-}, thereby inferring previous knowledge about plants. Besides, such a model is able to explain why we can immediately see semantic structure in newly coined words:
\blockquote[{\cite[p.~43]{souille-rigautSemanticAccountQuasiLexemes2010}}]{‘Dontopedalogy is the science of opening your mouth and putting your foot in it, a science which I have practiced for a good many years’. There is evidence that we are dealing with a secondary compound of the type W + X + Y, that is to say, with the semiotic units -dont- + -ped- + -log- being concatenated synchronically. [...] secondary compounds W + XY are always expanded primary compounds in which WXY is a hyponym of XY.}
In this respect, knowledge about specific hyponyms or hyponymy in general can facilitate the language learning process by providing a rather intuitive access to the meaning of single words or phrases. Unfortunately, this intuition may not correspond to the desired understanding of a word in its context:
% TODO
\section{Corpus data}

General notes on the agility and openness of the methods that will be applied in this study can be found in the \hyperref[sec:appendixA]{Appendix A}. Now, it is time to have a closer look at the involved resources, most notably the corpus. 

\section{Appendix}

\subsection{Appendix A: Agile and Open Methodology}
\label{sec:appendixA}

This study will follow an approach which is borrowed from computer science, i.e. the agile methodology for software development:
\blockquote[{\cite[p.~32]{fowlerAgileManifesto2001}}]{[...] agile processes assume and encourage the alteration of requirements while the code is being written. As such, design cannot be a purely up-front activity to be completed before construction. Instead, design is a continuous activity that's performed throughout the project.}
Coding will be used to establish a proof of concept for this study by implementing the discussed models and testing them on real-world corpus data. Furthermore, the basic notion of incremental and iterative progress also applies to the way the models themselves are being created: The dissertation will see a range of approaches and considerations being applied for various purposes, which reflects my own thoughts about the detection of semantic relations growing from initial, very basic foundations (e.g. pure contingency tables) to more complex environments of semantic study. The application of agile methodology to linguistic research is motivated not just by the related coding activities, but also by the similarities between dissertations and software products: In this view, a dissertation can be seen as a product to be delivered to customers, i.e. the research community. The analogy is enhanced further by the overlapping quality factor of openness: Dissertations are necessarily products of open science in much the same way as open source software is the product of other communities sharing the desire for unrestricted public access to creative commons.
\blockquote[{\cite[p.~518]{garcia-penalvoOpenKnowledgeManagement2010}}]{We think that Open Knowledge comprises Open Software, Open Content, Open Science and Open Innovation. [...] Open Software owes its deepest roots to Open Access; Open Contents are related to open access to the educative, cultural or divulgative contents that are published under a non restrictive license that allows copy and distribution, but also the right to modify works. Open Science is devoted to the open access to scientific contents, while Open Innovation transfers the Open Access principles to the enterprise production world, which is actually indispensable for the enhancement of University-Enterprises relationships.}
To demonstrate the viability of this transfer, the dissertation is split into various modules, separating, e.g., the bibliography from the rest of the documents and employing cross-references to put it all together for presentational purposes. These modules are, to some extent, directly visible in the sense of multiple source files that may be compiled to form a more complex document. Also, they are all publicly accessible at \href{https://github.com/konstantinschulz/asrael} and subject to version control, thus enabling readers to trace the resulting thoughts back to their very roots. Eventually, the reason to make all these considerations explicit is not so much to suggest a high quality of the work process, but rather to emphasize their possibly tremendous influence on the results and how they are gathered.


\printbibliography

\end{document}
























