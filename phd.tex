% !TEX TS–program = pdflatexmk
% keep this before the document class so it works with the LLNCS template
\RequirePackage{amsmath}
\documentclass[runningheads]{llncs}

\usepackage[backend=biber,maxcitenames=1,sorting=nyt,style=authoryear]{biblatex} % bibtex
\usepackage{caption} 
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{soul}
% this should be the last imported package
\usepackage{glossaries}
\graphicspath{ {figures/} }
% more space between tables and captions
\captionsetup[table]{skip=10pt}
% ignore notes in the bibliography
\AtEveryBibitem{
  \clearfield{note}
}
\addbibresource{lit.bib}
% show sections and subsections in the table of contents
\setcounter{tocdepth}{2}
\makeglossaries

\newglossaryentry{CVS}{name=CVS, description={continuous vector space}}
\newglossaryentry{DSM}{name=DSM, description={distributional semantic model}}
\newglossaryentry{PMI}{name=PMI, description={Pointwise Mutual Information}}
\newglossaryentry{POS}{name=POS, description={part of speech}}
\newglossaryentry{WSD}{name=WSD, description={word sense disambiguation}}
\newglossaryentry{WSI}{name=WSI, description={word sense induction}}

\begin{document}
\title{ASRAEL - Acquisition of Semantic RelAtions bEtween Latin nouns}
\author{Konstantin Schulz}
\institute{Humboldt-Universität zu Berlin}
\maketitle 

\tableofcontents
\listoffigures
\listoftables
\printglossaries

\begin{abstract}
	XXXabstractXXX
\end{abstract}
                        
\section{Introduction}
This dissertation deals with different ways of detecting semantic relations in text corpora. Its focus is more on methodology than on analytical results. Some important restrictions are necessitated by the precise research question: In a corpus of Latin texts, how can we automatically extract semantic relations that are relevant for language learning? I will concentrate only on a subset of the existing phenomena, namely hypernymy and hyponymy. This selection is motivated by the objective of utilizing the obtained insights as the basis for exercises in the process of learning historical languages. Thus, the detection of said relations is not an end in itself, but is to be viewed in the context of how humans learn languages. 

\subsection{Semantic relations in language learning}
Specifically, recent studies suggest that an improved integration of hyponymy into the learning process can lead to a higher level of language skills \parencite[196]{taslimExperimentalStudyTeaching2014}. Roughly the same procedure is applicable also in the context of artificial intelligence, which highlights the general importance of semantic relations in any kind of language learning effort \parencite[1306]{carlsonArchitectureNeverendingLanguage2010}. The automated extraction of semantic relations from texts can be used for constructing ontologies in specific domains, which is important for language learning because many textbooks are heavily structured according to a range different topics \parencite[192f.]{punuruLearningNontaxonomicalSemantic2012}. 

Since there is a parallel between semantic relations (or ontologies in general) and the way that humans think, it seems reasonable to exploit that connection for learning purposes. This is particularly important for advanced learners' lexical acquisition \parencite[56]{crossleyDevelopmentSemanticRelations2010}. Now, one might be tempted to conclude that predefined lists of basic vocabulary and more advanced semantic relations would be a sufficient help to language teachers. However, static materials und human supervision will probably fail to account for the entire processual complexity of lexical acquisition \parencite[71]{crossleyDevelopmentSemanticRelations2010}. The involvement of artificial intelligence in the modeling of human knowledge about vocabulary faces various problems, including the progression of learning over time \parencite[70]{crossleyDevelopmentSemanticRelations2010}. 

In language learning contexts, this problem extends also to the morphological domain, where it can prove difficult for students to exploit their knowledge about affixation for the acquisition of semantic relations \parencite[248]{gardnerValidatingConstructWord2007}. However, "inflected and derived forms" are exactly what we encounter in the curricular targets for Latin courses, i.e. literary texts. Most textbooks take care of the problem by using hand-crafted Latin texts instead of authentic ones. This leads to a highly problematic transition from textbook to literature, which is already famous among German teachers of Latin \parencite[115]{schibelZurAneignungLateinischer2013}. The challenge, then, is to not only extract semantic relations from a given corpus, but also to group them according to general principles of progression that are tightly coupled with a domain-specific theory of learning.

\section{Polysemy}
\subsection{Word Sense Induction}
\label{polysemyProblemForSemanticRelations}
In order to reach that point, we first need to define the basic object to be acquired, i.e. semantic relations. Unfortunately, this is not a precisely defined research object per se because it strongly depends on our previous knowledge about the possible word senses \parencite[12]{ayseExtractionSemanticWord2011}. Consequently, the sense of a word has to be defined before we can start to learn more abstract representations like hyponymy \parencite[137]{bartunovBreakingSticksAmbiguities2016}. Further obstacles may occur in the case of specific relations like synonymy, where a good operationalization is needed to retrieve the relevant instances from a corpus of text data \parencite[274]{divjakCorpusbasedCognitiveSemantics2009}. In my study, I will focus heavily on the approach of distributional semantics, describing specific semantic relations between words in terms of their co-occurrences, i.e. typical contexts in a corpus \parencites[162]{harrisDistributionalStructure1954}[30]{firthSynopsisLinguisticTheory1957}. This approach has traditionally suffered from several limitations. In some cases (e.g. synonymy), building unified representations for every type in a corpus can prove highly problematic \parencite[114]{karanDistributionalSemanticsApproach2012}. However, this is not universally true anymore as there are \gls{DSM}s that consistently distinguish between various contexts (\hyperlink{ELMO}{see below}). Other problems of distributional semantics are tied to fine-grained distinctions between specific semantic relations \parencite[115]{karanDistributionalSemanticsApproach2012}.

Co-occurrence counts can be a good basis for the detection of semantic relations in general, but they are rarely sufficient for a thorough classification into several subcategories. Nonetheless, previous research has been carried out (more or less successfully) using, e.g., association measures to detect synonymy \parencite[566]{hagiwaraSupervisedSynonymAcquisition2009}. One of the critical parts in that regard is the term `word pair', which introduces the linguistically ill-defined concept of `word' into an otherwise seemingly precise mathematical formula. In the following, we will start by using the term `word' simply as `token in a corpus' and later proceed to a more advanced definition. This progression seems reasonable given the fact that the usage of token-based \gls{PMI} for the research purposes of distributional semantics often leads to severely skewed results \parencite[444]{herbelotMeasuringSemanticContent2013}. Therefore, corpora may have to be tokenized and segmented using more complex separator rules than `whitespace and punctuation' before they can serve as a basis for distributional analyses \parencite[6]{mikolovDistributedRepresentationsWords2013}. Besides, polysemy may also become a problem for resolving other issues like synonymy, e.g. if two words are synonymous only in one of their possibly numerous senses \parencite[444]{herbelotMeasuringSemanticContent2013}. 

In addition to polysemy itself, related concepts like homonymy can also become obstacles with regard to the aim of defining semantic relations precisely \parencite[251]{gardnerValidatingConstructWord2007}. Distributional semantics provides a valuable framework to break down the multiple aspects of meaning for homonymous or polysemous words into smaller subsets, based on the combination with, e.g., adjectival modifiers \parencite[42]{boledaIntensionalityWasOnly2013}. Therefore, the integration of specific contextual elements into the analysis of hyponymy between Latin nouns seems to be a reasonable basis for distinguishing subtle semantic nuances more consistently. As a consequence, polysemy will be treated in this study not just as one of the analytical targets, but also as a methodological aspect that needs to be addressed before other kinds of analyses can be performed. A useful operationalization of polysemy in a distributional context is to look at the collocational diversity \parencite[8]{hamiltonDiachronicWordEmbeddings2016}. This approach brings along the problem of sampling, which is a general one for the whole of corpus linguistics: If we want to model a specific language like Latin as a whole but only use a tiny subset of the available language data for our corpus, how can we expect our results to be applicable to other Latin texts? The key point here is the purpose: For this study, our ultimate goal is not to model Latin nouns per se, but only with regard to the rather well-defined application context of language learning. Therefore, the explanatory power of our linguistic model does not need to be as comprehensive, and it will be restricted to predefined parameters. These parameters may in turn be described as, e.g., the objective of maximizing a student's learning success by providing structured information about semantic relations.

\subsection{Distributional Semantic Models for Word Senses}
Historically, linguistic features of words have often been represented in \gls{CVS}s \parencites{mikolovEfficientEstimationWord2013}{daiDocumentEmbeddingParagraph2015}{akbikContextualStringEmbeddings2018} that can be seen as statistical models of word distributions in natural language \parencite[1140]{bengioNeuralProbabilisticLanguage2003}. Such models map single elements of an utterance (e.g. characters, tokens, types, phrases etc.) to vectors in an n-dimensional \gls{CVS}. Depending on the dimensionality, various kinds of information will be integrated into the model. For instance, lexically related words (e.g. \textit{amor} `love' in PROIEL 86055.22\footnote{For details on the PROIEL corpus, see \textcite{haugCreatingParallelTreebank2008}. Note that, in this study, we only consider the Latin part of the corpus. Dot-separated unique identifiers will be used to refer to segments and tokens in the PROIEL corpus. This is often more precise than the conventional citation model for ancient works (in this case: Cic. off. 1.12) and does not require knowledge about abbreviations of author names or work titles. On the downside, it is corpus-specific and does not offer immediate information about author, work and structural position of the text passage.} and \textit{amare} `to love' in PROIEL 86334.11) usually correspond to vectors whose distance in the \gls{CVS} is rather small. Since most of these models try to predict words in a corpus given their surrounding context \parencite[5]{mikolovEfficientEstimationWord2013}, those words that co-occur frequently will also be mapped to similar vectors in the \gls{CVS}. This basic interaction helps us to distinguish multiple senses of a polysemous word empirically: If each word sense is associated with different contexts and, thus, different co-occurring words, its calculated vector will be different from those of other word senses. Therefore, a very simple evaluation for \gls{DSM}s with regard to \gls{WSI} looks like this:
\begin{itemize}
	\item Given a pair of contexts that
	\item both contain the same word,
	\item classify whether that target word is used in the same sense or not.
\end{itemize}
Formalizing this into a dataset structure, we may provide a plain text file where each each line contains one example for classification. Since Latin is a highly inflectional language, morphological features of the targeted word can differ a lot. Therefore, the actual word forms have to be indicated separately for each context in the example. If they occur multiple times in a context, the dataset should specify which occurrence of the word is to be evaluated. Finally, a binary value (such as `0 / 1' or `true / false') indicates the ground truth for this classification example, i.e. whether the word senses are the same or not. This is the most critical part: `Ground truth' is a dangerous assumption for linguistic phenomena because it suggests a consistent, unchanging goal to be achieved \parencite[129]{leeGeneratingGroundTruth2012}. This desire stems from the formalization that is deemed necessary to systematically evaluate computational language models \parencite[4]{fischerGroundTruthCreation2010}. Usually, such gold standards depend on human expertise, i.e. people that are highly acquainted with the target domain \parencite[519]{uzunerCommunityAnnotationExperiment2010}. In the case of historical languages like Latin, where specialized software and datasets are scarce \parencites[168]{karakantaNeuralMachineTranslation2018}[3]{mcgillivrayMethodsLatinComputational2013}, researchers often have to resort to digitizing analog resources or creating entirely new ones. This kind of curation process can and should rely on successful patterns from datasets for modern languages. Thus, it is reasonable to have a short look at existing comparable resources for evaluation.

The SemCor \parencite{mihalceaSemCorCorpus2008} and MASC-News \parencite[4218]{moroAnnotatingMASCCorpus2014} datasets are \gls{POS}-tagged English corpora where word senses have been annotated using WordNet \parencite{tengiDesignImplementationWordNet1998}. In that sense, such data is both empirical (because it contains authentic historical utterances) and intuition-based (because it uses an external knowledge base for defining the word senses, \hyperref[WordNet]{see below}). There have been attempts to unify \parencite[216]{navigliBabelNetBuildingVery2010} the various knowledge bases that researchers employed to annotate word senses. Promising results were achieved by discussing discrepancies in the face of text corpora, i.e. actual text passages from authentic sources \parencite[127]{bakerWordNetFrameNetComplementary2009}. This development highlights the importance of usage-based linguistics and points towards the need for a more rigorous, bottom-up formalization of word senses. Further approaches to creating datasets for \gls{WSD} include the usage of dictionaries (other than WordNet) as lexical resources \parencites[25]{mihalceaSenseval3EnglishLexical2004}[2]{jimeno-yepesExploitingMeSHIndexing2011}[6]{yuanSemisupervisedWordSense2016} and the usage of glosses as representations of word senses \parencite[8]{luoIncorporatingGlossesNeural2018}. In contrast, \gls{WSI} usually does not rely on external sense inventories, but tries to infer word senses in an unsupervised, empirical manner only from the text itself. State-of-the-art models cluster distributions of vector representations for possible context-dependent substitutes of a word and, thus, are able to detect distinctions between word senses that had not previously been considered in intuition-based sense inventories \parencite[1-4]{amramiBetterSubstitutionbasedWord2019}. Predecessors to this approach directly used a word's surrounding context to construct sense representations for clustering and identified substantial quality criteria for the evaluation of \gls{WSI} models \parencite[876-878]{huangImprovingWordRepresentations2012}:
\begin{itemize}
	\item human judgment on word sense similarity
	\item variation in patterns of meaning (homonymy, polysemy etc.)
	\item variation in parts of speech (esp. nouns, verbs, adjectives)
\end{itemize}
We can certainly add that, in addition to variation, balancing is a decisive factor for the training data \parencites[620]{liuCombiningIntegratedSampling2011}[9]{weiRoleBalancedTraining2013}[1127]{boneApplyingMachineLearning2015}: If we are interested in rare linguistic phenomena, such as subtle semantic differences between almost synonymous words, we need to make sure that they are represented frequently in our training data. The third criterion is most controversial: On the one hand, powerful \gls{WSI} models need to be general enough to provide sufficient results under different circumstances, e.g. in varying domains and registers. On the other hand, modeling word senses for different parts of speech needs to take into account the diverse influences of lexis and syntax: In some settings, nouns may be represented as sets of their hyponyms \parencite[640]{lewisCompositionalHyponymyPositive2019}, while transitive verbs can be modeled as triples together with their arguments, e.g. subjects and objects \parencite[9]{grefenstetteExperimentalSupportCategorical2011}. Such different approaches uncover the intricate nature of formalizing fine-grained semantic distinctions from a distributional perspective. As a consequence, our goal of detecting hyponyms for language learning seems to warrant a modular operationalization with regard to parts of speech. 

\section{Nouns}
Our choice of nouns as the target for hyponymy detection is motivated by evidence from language learning. It suggests that nouns are usually easier to acquire, especially in most modern languages, where this class of words is particularly important in everyday language use \parencite[9]{garcia-gamezLearningNounsVerbs2019}. This is relevant for our research because, nowadays, many learners of Latin have acquired as L1 one of the `noun-dependent' languages, e.g. German or French \parencite[51-52]{baidakKeyDataTeaching2017}. Therefore, it makes sense to build on their individual preconditions and focus, at least initially, on those word classes that are also important in their L1. However, Latin has often been described as verb-heavy language \parencite[41]{mengeLehrbuchLateinischenSyntax2009}. This claim is supported by comparing the noun-to-verb ratio \parencite[300]{stollNounsVerbsChintang2012} of the PROIEL corpus to the Hamburg Dependency Treebank \parencite{borgesvolkerHDTUDVeryLarge2019} and the French Treebank (\cite{seddahCheatingParserDeath2018}; see Table \ref{tableNounToVerbRatio}).
\begin{table}[ht]
	\begin{tabular}{ c | c | c | c }
  		 & PROIEL & French Treebank & Hamburg Dependency Treebank \\ \hline
  		Noun-to-Verb Ratio & 0.50 & 0.71 & 0.73 \\
	\end{tabular}
	\caption{Noun-to-verb ratio for different treebanks in the Universal Dependencies corpus (Latin, French, German; \cite{nivreUniversalDependenciesV12016}).}
	\label{tableNounToVerbRatio}
\end{table}

The lower ratio indicates that Latin authors use fewer nouns, relative to their usage of verbs and compared to authors of modern European languages. This typological difference seems to demand a stronger focus on verbs instead of nouns, at least for Latin. Nonetheless, it has been shown that nouns play a substantial role in the early stages of language acquisition, especially in the understanding of subcategorization frames for verbs \parencite[1383]{yuanCountingNounsSimple2012}. If this is applicable to historical languages as well, we should rather rely on finding suitable combinations, provided that there is a useful formalization of hyponymy for the different parts of speech. The focus on a single word class (in our case: nouns) is one initial step in that direction.

\section{Hyponymy}
After dealing with various issues of polysemy, some of the second-tier semantic relations may have to be redefined when looked at from a distributional perspective, e.g. hyponymy \parencite[443]{herbelotMeasuringSemanticContent2013}. In this view, other existing taxonomies that may have acted as models of hyponymy need to be replaced by a definition that reflects how actual usage contexts can limit our selection of possible candidates for hyponymy \parencite[443]{rollerInclusiveSelectiveSupervised2014}. Unfortunately, this view may clash with the observations of \textcite{herbelotMeasuringSemanticContent2013}:
\begin{itemize}
      \item Hypernyms can have very specific usage contexts.
      \item Collocations may lead to unexpected results in the distribution of words.
\end{itemize}
Similar adjustments have to be made in treating other kinds of semantic relations. Therefore, many researchers apply preprocessing to their corpora before performing the actual analysis on them \parencite[56f.]{gyllenstenRgramsUnsupervisedLearning2019}. However, this increases the danger of circularity, revealing only those results that we were already expecting in the first place. As an alternative, we should look for a model that is robust enough to detect semantic relations despite the confounding factors. The objective should be to fit the model to the base text, not the other way round. For vector space models like Skip-gram \parencite[5]{mikolovEfficientEstimationWord2013}, subsampling and the restriction of strong collocations may be justified from a computational perspective because they enable a faster training of models while more or less preserving the resulting performance in the prediction of word contexts. This very tradeoff, though, may not work out as well for the detection of semantic relations, e.g. if we give a higher priority to the frequency of certain contexts. Such a change of algorithm may particularly be motivated by the distinction of theoretical and empirical scopes of words \parencite[46]{coenenAnalogieUndMetapher2013}. 

This also applies to the domains of corpus linguistics and language learning: Given a text corpus that helps learners to acquire a certain knowledge (e.g. all the ancient literature mentioned in an educational curriculum for the Classics), the theoretical scope of a certain word would be equivalent to the set of all distinct contexts in the corpus in which that word occurs. In contrast, the empirical scope of that same word would be equal to a subset of that theoretical scope, namely those contexts which are already known to the learner (at any given point in time of the learning process). Thus, the theoretical scope will remain static as long as we do not change the corpus, while the empirical one will be dynamic as long as we keep learning. From this point of view, we can also restrict our notion of semantic relations: Hyponymy, for instance, will be tied to those lexical networks that are present in a specific corpus and, for a specific learner, specific instances of hyponomy relations will not exist unless they have already been encountered in the corpus and lead to a relevant learning process. It is therefore our duty to a) detect such instances in the corpus and b) use them to design learning situations that help language learners to construct lexical and semantic knowledge \parencite[25]{yorkConstructivistApproachGamebased2018}. In order to accomplish that first part, we need to have clearly defined requirements to qualify a relation as one of hyponymy, e.g. according to a version of the Distributional Inclusion Hypothesis \parencite[443]{rollerInclusiveSelectiveSupervised2014} that also accounts for idiomaticity, burstiness and other kinds of lexical interferences.

\subsection{WordNet}
\label{WordNet}
Lexical resources like the Latin WordNet \parencite{minozziLatinWordNetProject2010} may contribute a reference model for such a purpose because it includes a built-in notion of hyponymy. Unfortunately, the WordNet defines such relations based on intuition, not on actual language use \parencite[315]{fellbaumChallengesMultilingualWordnet2012}. WordNet is therefore static and non-empirical from the perspective of usage-based fields like corpus linguistics. However, it was constructed by alleged experts and we may use their assessment for reflecting our own approach. This kind of triangulation is supported by recent studies, which often explicitly combine knowledge bases and corpus data \parencite[984-988]{onoWordEmbeddingbasedAntonym2015}, thereby integrating both the introspective and the usage-based approaches to semantics. The former can be helpful in two ways: a) by defining a horizon of expectations in language learning contexts, i.e. which information learners are supposed to infer from their previous knowledge in order to understand a linguistic expression in a specific text passage; and b) by modeling semantic knowledge in a machine-readable, consistent (albeit simplistic) manner. The integration of distributional semantics, on the other hand, is particularly motivated by the hypothesis that semantic properties of words are reflected in their contextual surroundings \parencite[59]{griesBehavioralProfilesCorpusbased2009}. 

By using distributional data from corpora, we also gain the advantages of rigorously empirical research (as opposed to bare intuition and anecdotal evidence): frequency, authenticity, variation and systematic induction \parencite[60]{griesBehavioralProfilesCorpusbased2009}. Still, knowledge databases may be useful for languages like Latin where previous research is scarce and, more often than not, heavily intuition-based or anecdotal. In both the introspective and the distributional case, we can increase the amount of provided information by relying on associated properties of hyponymy relations, e.g. transitivity \parencite[58]{coenenAnalogieUndMetapher2013}. Applied to the Latin WordNet, this may look as follows:
\begin{enumerate}
         \item \textit{gladius}
         \item a cutting or thrusting weapon with a long blade
         \item weaponry used in fighting or hunting
         \item weapons considered collectively
         \item an artifact (or system of artifacts) that is instrumental in accomplishing some end
         \item a man-made object
         \item a physical (tangible and visible) entity
\end{enumerate}
This information flow can be read from both sides. Beginning at \textit{gladius} (sword), it can be used to simulate the process of reading a text: We encounter a word in the text (e.g. the plural \textit{gladii}) and determine its lemma (\textit{gladius}). From there, we can try to find a corresponding meaning for the surface form, e.g. "a cutting or thrusting weapon with a long blade". This meaning, in the WordNet taxonomy, can be hyponym to other kinds of meaning, e.g. "weaponry used in fighting or hunting". Now, if we know that
\begin{itemize}
         \item \textit{gladius} is a weapon with a blade and
         \item weapons with a blade can be used for hunting,
\end{itemize}
then we are safe to assume that a \textit{gladius} can be used for hunting. The further we go down the list, the more abstract do the meanings get. Nonetheless (or because of that), every layer can remind us of new specific features that we may want to associate with \textit{gladius}: specific motion (2.), specific purpose (3.), hypernym (4.), general purpose (5.), origin and production (6.), material (7.). In some cases, we may even use morphological hints to get faster access to information about hyponymy \parencite[913]{anstattTypenSemantischerRelationen2009}. This is especially true for Latin with its relatively rich morphology: the meaning of \textit{artifex} (artist) may be associated directly to \textit{ars} (art) and \textit{facere} (to make), without necessarily traversing a hierarchical tree-like representation of increasing semantic generalization. Interestingly, this is possible even in cases where the compound is formed from foreign language material and thus cannot be easily paraphrased in the native language \parencite[33]{souille-rigautSemanticAccountQuasiLexemes2010}. 

How is that important for Latin hyponymy? Consider Greek loanwords: \textit{xylophytum} (a certain kind of plant) cannot be reasonably described in Latin words as a \textit{*phytum} that is made of \textit{*xylum}. However, Romans may have deduced that the meaning of \textit{xylophytum} is related to ($\rightarrow$ hyponymy) \textit{-phyt-}, thereby inferring previous knowledge about plants. Therefore, we have to allow for the possibility that some instances of hyponymy may not be extractable solely on a distributional basis, but rather on a morphological one. Besides, a morphologically informed model is able to explain why we can immediately see semantic structure in newly coined words \parencite[43]{souille-rigautSemanticAccountQuasiLexemes2010}. In this respect, knowledge about specific hyponyms or hyponymy in general can facilitate the language learning process by providing a rather intuitive access to the meaning of single words or phrases. Unfortunately, this intuition sometimes competes with other interpretations of a word's sense which may be equally intuitive and valid \parencite[126]{ponsborderiaPathsGrammaticalizationSpanish2014}. 

Such examples underline the importance of disambiguation (\hyperref[polysemyProblemForSemanticRelations]{see above}), which may not always be possible. Furthermore, additional steps may be necessary even if a specific word sense was already determined, e.g. the inference of common knowledge or common sense \parencite[427f.]{pinkalSemantik1993}. In this view, disambiguation should be preceded by a thorough linguistic analysis of the context, which, in terms of distributional semantics, is usually addressed by extracting cooccurrence frequencies from sliding windows over a textual input, be it plain text or a linguistically annotated treebank. However, the integration of external information into this process is not as straightforward and therefore often avoided. In the end, this might not be the worst decision, depending on our definition of semantics: Some models of distributional semantics reduce the spectrum of polysemy to one main aspect of meaning per word.
\parencite[4]{faruquiProblemsEvaluationWord2016}.

\hypertarget{ELMO}{Approaches} of this kind level nuances that may actually be important for our understanding of a single word in its context. As an improvement, other models regard entire sentences for the representation of a word's context-specific meaning, thereby differentiating between various common usages \parencite[2f.]{petersDeepContextualizedWord2018}. Modeling the process of language production both in a forward and backward manner at the same time seems reasonable, especially for the Latin language, because the antecedent of, e.g., a relative pronoun can be part of the relative clause, which makes it difficult to understand the structure of such a construction by reading it in just one direction, without referring to the past and future context at the same time:
\blockquote{\textit{Nam et frumentum ex agris cotidie in castra conferebat et quae gravissime adflictae erant naves earum materia atque aere ad reliquas reficiendas utebatur [...].} (PROIEL 53467)\\
for he daily conveyed corn from the country parts into the camp, used the timber and brass of such ships as were most seriously damaged for repairing the rest [...] \parencite{mcdevitteCaesarGallicWar1869}.}

In this example, \textit{naves} is the antecedent to the relative pronoun \textit{quae}. Its integration into the relative clause is recognizable by its morphological congruency (case, number, gender) with the relative pronoun. In a textbook setting, the sentence would rather look like this: \textit{[...] conferebat et earum navium, quae gravissime adflictae erant, materia atque [...]}. Such syntactic variations are probably related to the relatively free word order in the Latin language. Therefore, a language model for Latin should be flexible enough to analyze a given textual sequence by looking at either the left or right context, according to the circumstances of the case \parencite[8]{devlinBertPretrainingDeep2018}. While this strictly bidirectional, attention-based approach \parencite{vaswaniAttentionAllYou2017} enhances the ability to explain word meanings from their larger surrounding contexts, it does not address the problem of inference, e.g. relevant external text passages. To analyze this problem empirically, we will set up a toy language model in order to inspect difficult cases. 

\section{Method}
General notes on the agility and openness of the methods applied in this study can be found in the \hyperref[sec:appendixA]{Appendix A}. Now, it is time to have a closer look at the separate steps in the workflow:
\begin{enumerate}
         \item Create a dataset.
         \begin{enumerate}
		\item Specify the text corpus to be used.
		 \begin{enumerate}
			\item Segment the corpus into smaller pieces, e.g. sentences.
			\item Tokenize every segment.
		\end{enumerate}
		\item Specify a gold standard for hyponymy.
		\item Define hyponymy pairs for every segment in the corpus.
	\end{enumerate}
         \item Split the dataset into subsets for training, validation and evaluation.
         \item Implement a transformer language model \parencite{vaswaniAttentionAllYou2017} for hyponymy extraction.
         \begin{enumerate}
		\item Train the model on the training subset.
		\item Use the validation subset to make sure that the model inferred general rules instead of memorizing input patterns.
	\end{enumerate}
         \item Evaluate the model using the evaluation dataset.
\end{enumerate}
Since machine learning is the major approach to be considered here, the size of the dataset (especially the training subset) is crucial \parencite[13]{hestnessDeepLearningScaling2017}. The rule of maximizing training data applies not only to machine learning, but also to other methods of distributional semantics, e.g. in the case of semantic relations \parencite[27]{herbelotBuildingSharedWorld2015}. Therefore, it seems reasonable to prefer the Corpus Corporum (about 190 million tokens of plain text, cf. \textcite{roelliCorpusCorporumNew2014}) over PROIEL (about 220.000 tokens of annotated text). Unfortunately, this limits the usage of linguistic research results (i.e. annotations) as input for the language models. On the other side, the practical application of the model for language learning purposes becomes easier and more flexible, in the sense that, theoretically, end users can supply any plain Latin text of their choice, without the need to provide additional annotations.

Before we can start to detect hyponyms, we need to induce word senses, especially for polysemous words. To make sure that fine-grained distinctions between different usages of the same word are recognized by the model, it should be evaluated on a test dataset that has the following structure:
\begin{itemize}
	\item For each segment in a pair of segments (e.g. sentences),
	\item look at the target word(s) and
	\item predict whether it was used in the same sense as in the other segment.
\end{itemize}
The second step can be formalized by providing character indices referring to the position of the target word in a string representation of the segment. This works even for cases where a target word form occurs multiple times in the segment and we want to induce the word sense for just a single specific instance. The third step, i.e. prediction of semantic similarity, can be binary in this very basic operationalization: The two target words are either used in the same sense or not. This dichotomy consciously abstracts from more complicated evidence, e.g. where two instances are used in a very similar, but not quite the same sense. In a subsequent effort, we may try to associate the current instance with other segments where the word usage is similar, thus clustering occurrences of the same word sense together. 

Since there are no semantically annotated authentic text corpora for Latin, the next best thing to use are grammar books and dictionaries: They often distinguish between various word senses and offer exemplary contexts for each one \parencites{georgesAusfuhrlichesLateinischDeutschesHandworterbuch1913}{niederauNavigiumLateinDeutschWorterbuch2012}. Some of them cite entire text passages \parencites{shortLatinDictionary1879}{kuhnerAusfuhrlicheGrammatikLateinischen1914}{mengeLehrbuchLateinischenSyntax2009} to support their classifications. Such references can be seen as annotations: If a dictionary lists various text passages as containing a specific word in the same sense, this is roughly equivalent to semantically annotated datasets where sense identifiers from static sense inventories are assigned to tokens in a text corpus. Now, we could proceed by looking at the largest entries in the dictionary, assuming that they contain the largest number of different word senses. This, however, would mean to accept the editor's intuition, which is not a usage-based approach. Instead, we will find the most polysemous words in a corpus inductively, by looking at their usage contexts. In this view, the most polysemous word will be that which is used in the most diverse contexts.

A naive formalization of diverse contexts is to count the number of types that occur in the same sentence as a target type in the corpus. This disregards the absolute frequencies of the types in a corpus and will lead to a Zipfian distribution \parencite[VI]{zipfPsychobiologyLanguageIntroduction1936} of target types. Thus, function words like \textit{et, in, ut, est, non, cum} `and, in, as, is, not, with' will dominate the ranking just because of their immense absolute frequency in the corpus. We can limit their impact by randomly down-sampling their occurrences using an arbitrary upper bound of 100. Since our goal is to produce exercises for language learning, the potential hyponyms should be rather frequent in a given corpus \parencites[27]{ellisUsagebasedLanguageInvestigating2013}[2]{robillardMonolingualBilingualChildren2014}. It is therefore reasonable to establish a minimum threshold for word frequency. This criterion interacts with Latin being a highly inflectional language, thus producing many morphologically different forms for the same lexeme (see Table \ref{tableTypesPerLemma}).
\begin{table}[ht]
	\begin{tabular}{ c | c | c | c }
  		 & PROIEL & French Treebank & Hamburg Dependency Treebank \\ \hline
  		Types per Lemma & 3.64 & 1.21 & 2.09 \\
	\end{tabular}
	\caption{Average number of types per lemma for Latin, French and German.}
	\label{tableTypesPerLemma}
\end{table}

%TODO: did we really use lemmatized texts?
Thus, we will perform this initial experiment on professionally lemmatized texts, which restricts us to the PROIEL treebank. Using lemmata instead of types has the advantage of increased frequency counts per item (esp. in small corpora) and allows us to easily group contexts by lexeme. The downside here is that the diversity of contexts cannot be distinguished anymore for various word forms of the same lexeme, but we can reintroduce that distinction in a later step. The comparison of two contexts for a given target word will be made using representations of sentences in a \gls{CVS}. This mapping from textual to numerical entities (i.e. vectors) is not straightforward, because it is unclear how exactly we should model the difference between short and long sentences that have roughly the same content, but different wording. 

If we pad all shorter sentences to an arbitrary upper bound (e.g. the length of the longest sentence in the corpus) using placeholder values, the model will be able to calculate the cosine similarity \parencite[3]{luoCosineNormalizationUsing2017} between two sentence vectors as input. However, naively using a fixed value as the placeholder in right-padded sequences \parencite[386]{sachanEffectiveUseBidirectional2018} is equivalent to assuming that, compared to very long sentences, every short sentence tends to repeat the same word over and over again after a certain time. From a linguistic perspective, this formalization seems rather unintuitive because it does not take into account the effects of text length on lexis \parencites[31]{golcherStylometryInterplayTopic2011}[141]{ochabStylometryLiteraryPapyri2019}, which may lead to a clustering of input sentences roughly by (original) sequence length.

Against this backdrop, normalization of text length is probably just as problematic as the disruptive influence of text length in the first place. Possible alternatives consist in manual ad hoc annotation or trying to circumvent the problem. We will proceed by testing a model of semantic diversity that includes lexical and syntactic cues while also ignoring text length. By training a transformer language model \parencite[9]{vaswaniAttentionAllYou2017} on the raw text of the PROIEL corpus, we build a \gls{CVS} into which single tokens can be embedded. Our implementation uses Positional Encoding \parencite[4]{lampleCrosslingualLanguageModel2019} to explicitly analyze every word's position in a given sequence, thereby including basic syntactic notions in the model. Besides, the aforementioned distributional approach makes the representations lexically predictable, i.e. components of collocations will be represented next to each other in the \gls{CVS}. To corroborate these hypotheses, we evaluate our model on a few arbitrarily selected example sentences from PROIEL that are lexically identical, but differ in their word order (see Table \ref{tableCosSimWordOrder}). As a cross check, we have calculated the cosine similarity between entirely identical sentences and verified that it equals 1, which is the maximum value.
\begin{table}[ht]
	\begin{tabular}{ c | c | c | c }
  		Sentence ID & Text & Target & Cosine \\ 
		 & & Token & Similarity \\ \hline
  		12966 & si fuerit oculus tuus ... & fuerit & \\
		17274 & si oculus tuus fuerit ... & fuerit & 0.81 \\ \hline
		13425 & qui autem non habet ... & qui & \\
		17880 & ab eo autem qui non habet ... & qui & 0.76 \\ \hline
		13883 & erat enim habens multas possessiones & multas & \\
		11010 & erat enim habens possessiones multas & multas & 0.37 \\ \hline
		14258 & ... praeteribit haec generatio donec omnia haec fiant & haec & \\
		18059 & ... praeteribit generatio haec donec omnia fiant & haec & 0.83 \\ \hline
		14376 & opus bonum operata est in me & bonum & \\
		11310 & bonum opus operata est in me & bonum & 0.75 \\ \hline
		11311 & semper enim pauperes habetis vobis cum & pauperes & \\
		19419 & pauperes enim semper habetis vobis cum & pauperes & 0.81 \\ \hline
	\end{tabular}
	\caption{Cosine similarity for tokens in almost identical sentences from PROIEL.}
	\label{tableCosSimWordOrder}
\end{table}

The outlier value for the sentences 13883 (\textit{erat enim habens multas possessiones} `For he had many possessions.') and 11010 indicates a strong change in the token's representation depending on its position in this context. Morphologically, there is an unambiguous agreement between the noun \textit{possessiones} `possessions' and its modifying adjective \textit{multas} `many' in case, number and gender. Thus, further research is to be done here to explain this outcome by testing for other possible interferences, e.g. sentence length, part of speech or lexeme-specific syntactic preferences. It seems that, at least for the PROIEL corpus, the relatively free word order does not automatically imply a semantic equivalence of various syntactic configurations \parencite[452]{devineLatinWordOrder2006}, even though some of the previous scholarly literature suggested this to be the case \parencite[421]{niemeyerZurStellungAttributiven1997}. 

Lexis, on the other hand, is a much more obvious indicator of semantic differences, especially from a distributional perspective. To compare sentences with identical syntax, we arbitrarily pick examples from PROIEL that are longer than 3 tokens and differ in exactly 1 word, which was replaced by either an entirely different word or by a morphological variant of the same lexeme. Such syntactically almost identical sentences can be compared by computing the cosine similarity of their vector representations. These vectors are aggregated by averaging the vectors all tokens in the sentence, which is a common strategy \parencite[1]{adiFinegrainedAnalysisSentence2016}. Again, as a cross check, we make sure that the cosine similarity for completely identical input sentences is almost at the maximum value. Then, we proceed to compare each sentence to its syntactical twin where 1 token was replaced (see Table \ref{tableCosSimWordSubstitution}).
\begin{table}[ht]
	\begin{tabular}{ c | c | c | c }
  		Sentence ID & Text & Differing & Cosine \\ 
		 & & Token & Similarity \\ \hline
  		34155 & ego sum Alpha et Omega initium et finis & initium & \\
		33474 & ego sum Alpha et Omega principium et finis & principium & 0.90 \\ \hline
		33726 & et secundus angelus tuba cecinit & secundus & \\
		33823 & et septimus angelus tuba cecinit & septimus & 0.88 \\ \hline
		33661 & et ecce equus pallidus & pallidus & \\
		33655 & et ecce equus niger & niger & 0.85 \\ \hline
		14139 & diliges proximum tuum sicut te ipsum & sicut & \\
		11217 & diliges proximum tuum tamquam te ipsum & tamquam & 0.92 \\ \hline
		20141 & si sic eum volo manere donec veniam quid ad te & veniam & \\
		63139 & si sic eum volo manere donec venio quid ad te & venio & 0.47 \\ \hline
		19306 & quaerebant ergo eum prendere & prendere & \\
		18951 & quaerebant ergo eum adprehendere & adprehendere & 0.81 \\ \hline
		18848 & qui credit in me habet vitam aeternam & me & \\
		48253 & qui credit in Filium habet vitam aeternam & Filium & 0.95 \\ \hline
	\end{tabular}
	\caption{Cosine similarity for sentences from PROIEL in which exactly one token was replaced.}
	\label{tableCosSimWordSubstitution}
\end{table}

A major problem with the underlying \gls{CVS} is that its dimensions are not interpretable. In language modeling with neural networks, it is generally assumed that the vector representations entail all kinds of linguistic information, e.g. on morphology, syntax or semantics \parencites[8]{gladkovaAnalogybasedDetectionMorphological2016}[143]{rogersTooManyProblems2017}. However, in each of the linguistic domains, fine-grained nuances may not have been captured adequately, such as the distinction of synonyms from other semantically related words \parencite[115]{karanDistributionalSemanticsApproach2012} or semantic analogies beyond single cases and categories \parencite[142f.]{rogersTooManyProblems2017}. Therefore, it remains unclear why some substitutions invoke much greater changes in similarity than others, e.g. the change of mood (sentences 20141 and 63139) vs. the change of speaker and perspective (sentences 18848 and 48253). Further research in this direction would need to consider alternative sentence representations, inspection of the different attention head layers for the replaced tokens, and systematic trials for specific replacement strategies (inflectional variants, synonyms etc.). For our purposes, it is enough to have a very basic empirical indication that the language model is sensitive to changes in syntax and lexis, which is an important prerequisite for our evaluation of polysemy. 
%TODO:

%As for the gold standard on hyponymy, the Latin part of the WordNet was used. In a very basic approach, we will extract from it the explicit hyponym and hypernym pairs only for nouns, converting both to the notation of "hyponym,hypernym". This ignores additional properties like transitivity in order to create an explicit empirical baseline for later comparison. Besides, since there is no previous empirical knowledge from applying machine learning to hyponymy detection in Latin, we shall transform the complex problem to a much simpler one and then, gradually, reintroduce the complexity: At the very beginning, the machine's task will be to detect whether there are instances of hyponymy in a given sentence or not, which is called binary classification because there are only two options (either there are some instances or there are none). Eventually, the most complex question may look something like this: Given a Latin text passage, which words in it are hyponyms of which other ones and which word sense do they express in that specific context? Notably, the main dimensions of complexity are the following:
%\begin{itemize}
   %      \item The input can be a sentence (easy) or some longer kind of segment, e.g. a paragraph (difficult).
      %   \item The detection can be binary (easy) or token-based (difficult).
         %\item Polysemy can be leveled (easy) or explicitly disambiguated (difficult).
%\end{itemize}
%Furthermore, there are various ways of balancing to consider. By comparing plain text to the WordNet lemmata in a superficial manner, we can only detect hyponymy for those sentences in which the hyponyms appear in their base form. For languages with little inflectional variety (like English), this may be an acceptable choice. For Latin, however, with its rich morphology, such a simplification has far-reaching consequences: About 3\% of the sentences were associated with hyponymy (see Table ~\ref{table:hyponymyPercentageShallow}).

%\begin{table}[ht]
%	\begin{tabular}{ c | c | c }
  %		 & All Sentences & With Hyponyms \\ \hline
  %		Absolute & 9 992 000 & 304 677 \\
  %		Percentage & 100\% & 3.05\%  \\
	%\end{tabular}
	%\caption{Percentage of sentences with hyponymy, extracted by shallow comparison to WordNet}
	%\label{table:hyponymyPercentageShallow}
%\end{table}

%Therefore, machines will achieve high accuracy rates (>95\%) using rather simple rules, e.g. by never predicting a hyponym. Alternatively, they may produce similar results by memorizing all of the (most frequent) hyponyms: There are 6781 noun lemmata in WordNet that also appear as surface forms in the Corpus Corporum. As with many linguistic frequency distributions, their occurrence counts follow Zipf's law \parencite[VI]{zipfPsychobiologyLanguageIntroduction1936}. In other words, a few "super-hypernyms" like \textit{causa} (cause, reason), \textit{genus} (race, kind) or \textit{homo} (human being, man) dominate the overall picture, leaving rarely occurring nouns such as  \textit{concussio} (shaking, concussion),  \textit{audientia} (hearing, listening) or  \textit{congelatio} (freezing, congealing) far behind. This information will be helpful for language learning purposes because, in some cases, rather superordinate terms may be acquired later than their prototypical hyponyms \parencite[27]{ellisUsagebasedLanguageInvestigating2013}. This is true for the acquisition of both L1 and L2. In Latin, however, we are not concerned with acquisition, but with learning: Students usually do not speak Latin in their everyday life or during  classes, except when occasionally reading aloud a text. Reciting Latin texts does not (yet) seem to be a well-defined concept in terms of language learning \parencite[243, fn. 89]{munznerNeueWegeIm2013}.

%(see Table ~\ref{table:modelAccuracyShallow})
%\begin{table}[ht]
%	\begin{tabular}{ c | c | c | c }
  %		 & Precision & Recall & F1 \\ \hline
  %		Score & .73 & .41 & .53 \\
	%\end{tabular}
	%\caption{Evaluation of the simplest model, trained with data from a shallow comparison to WordNet}
	%\label{table:modelAccuracyShallow}
%\end{table}
%\begin{figure}[ht]
%\includegraphics[width=\linewidth]{freq_dist_hyponyms_cc_plain}
%\caption{Frequency distribution of hyponym lemmata in the Corpus Corporum (plain text)}
%\label{figure:frequencyDistributionHyponymsCCplain}
%\end{figure}

\section{Appendix}
\subsection{Appendix A: Agile and Open Methodology}
\label{sec:appendixA}
This study will follow an approach which is borrowed from computer science, namely the agile methodology for software development \parencite[32]{fowlerAgileManifesto2001}. Coding will be used to establish a proof of concept for this study by implementing the discussed models and testing them on real-world corpus data. Furthermore, the basic notion of incremental and iterative progress also applies to the way the models themselves are being created: The dissertation will see a range of approaches and considerations being applied for various purposes, which reflects my own thoughts about the detection of semantic relations growing from initial, very basic foundations (e.g. plain contingency tables) to more complex environments of semantic study. The application of agile methodology to linguistic research is motivated not just by the related coding activities, but also by the similarities between dissertations and software products: In this view, a dissertation can be seen as a product to be delivered to customers, i.e. the research community. The analogy is enhanced further by the overlapping quality factor of openness: Dissertations are necessarily products of open science in much the same way as open source software is the product of other communities sharing the desire for unrestricted public access to creative commons \parencite[518]{garcia-penalvoOpenKnowledgeManagement2010}. 

To demonstrate the viability of this transfer, the dissertation is split into various modules, separating, e.g., the bibliography from the rest of the documents and employing cross-references to put it all together for presentational purposes. These modules are, to some extent, directly visible in the sense of multiple source files that may be compiled to form a more complex document. Also, they are all publicly accessible at \href{https://github.com/konstantinschulz/asrael} and subject to version control, thus enabling readers to trace the resulting thoughts back to their very roots. Eventually, the reason to make all these considerations explicit is not so much to suggest a high quality of the work process, but rather to emphasize their possibly tremendous influence on the results and how they are gathered.

\printbibliography[heading=bibintoc]

\end{document}
























